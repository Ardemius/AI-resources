= Ressources sur les IA g√©n√©ratives et les LLMs (OpenAI ChatGPT et autres)
Thomas SCHWENDER <icon:github[] https://github.com/Ardemius/[GitHub] / icon:twitter[role="aqua"] https://twitter.com/thomasschwender[@thomasschwender]>
// Handling GitHub admonition blocks icons
ifndef::env-github[:icons: font]
ifdef::env-github[]
:status:
:outfilesuffix: .adoc
:caution-caption: :fire:
:important-caption: :exclamation:
:note-caption: :paperclip:
:tip-caption: :bulb:
:warning-caption: :warning:
endif::[]
:imagesdir: ./images
:resourcesdir: ./resources
:source-highlighter: highlightjs
:highlightjs-languages: asciidoc
// We must enable experimental attribute to display Keyboard, button, and menu macros
:experimental:
// Next 2 ones are to handle line breaks in some particular elements (list, footnotes, etc.)
:lb: pass:[<br> +]
:sb: pass:[<br>]
// check https://github.com/Ardemius/personal-wiki/wiki/AsciiDoctor-tips for tips on table of content in GitHub
:toc: macro
:toclevels: 4
// To number the sections of the table of contents
//:sectnums:
// Add an anchor with hyperlink before the section title
:sectanchors:
// To turn off figure caption labels and numbers
:figure-caption!:
// Same for examples
//:example-caption!:
// To turn off ALL captions
// :caption:

toc::[]

Notes au sujet des IA g√©n√©ratives, ChatGPT, Dall-E et leurs concurrents et alternatives.

== FONCTIONNEMENT DES MODELES DE LANGAGES

* Fonctionnement de ChatGPT
	** https://www.assemblyai.com/blog/how-chatgpt-actually-works/
	** Researchers and developers are working on various approaches to address the *alignment problem in Large Language Models*. +
	ChatGPT is based on the original GPT-3 model, but has been further trained by *using human feedback to guide the learning process* with the specific goal of mitigating the model's misalignment issues. The specific technique used, called *Reinforcement Learning from Human Feedback* (RLHF), is based on previous academic research. +
	ChatGPT represents the first case of use of this technique for a model put into production.

* https://platform.openai.com/docs/models/
	* The OpenAI API is powered by a diverse set of models with different capabilities and price points. You can also make limited customizations to our original base models for your specific use case with fine-tuning.
	* text-davinci-002 : supervised fine-tuning instead of reinforcement learning
	* GPT-3 models can understand and generate natural language. These models were superceded by the more powerful GPT-3.5 generation models. +
	However, the *original GPT-3 base models* (davinci, curie, ada, and babbage) are current the *only models that are available to fine-tune*.
		** Une description des mod√®les GPT 3 est disponible juste apr√®s ce paragraphe

* https://gpttools.com/comparisontool : GPT comparison tool *ON GPT 3 base models* (cf l'explication pr√©c√©dente)
	** This tool lets you try out different settings (engines, *temp*, *top p*, etc.) and compare the results.

* https://platform.openai.com/docs/quickstart
	** Remember that the model predicts which text is most likely to follow the text preceding it.

* https://platform.openai.com/docs/api-reference/chat/create
	** on y trouve une liste de tous les param√®tres que l'on peut personnaliser lors de l'appel de l'API d'un mod√®le de langage.

		*** ATTENTION ! Cf https://platform.openai.com/docs/models/model-endpoint-compatibility, on voit que le endpoint "/v1/chat/completions" concerne les mod√®les : gpt-4, gpt-4-0314, gpt-4-32k, gpt-4-32k-0314, gpt-3.5-turbo, gpt-3.5-turbo-0301 
		*** PAR CONTRE, le mod√®le "/v1/fine-tunes" est uniquement associ√© aux mod√®les "davinci, curie, babbage, ada" qui doivent √™tre les mod√®les GPT 3 cf les remarques pr√©c√©dentes.
		*** QUESTION : si on diff√©rencie les 2 endpoints pr√©c√©dents, quels sont donc les param√®tres associ√©s au "fine-tuning" ? Car il est √©galement possible de personnaliser des param√®tres pour le endpoint "/v1/chat/completions"

* Pour un papier de recherche expliquant ce qu'est et ce que n'est pas un mod√®le de langage (LLM) (TRES BONNE RESSOURCE) : +
Talking about Large Language Models (LLM) : https://arxiv.org/pdf/2212.03551.pdf

	** Une TRES BONNE RESSOURCE expliquant de fa√ßon rationnelle et scientifique pourquoi on ne peut pas dire qu'un LLM "pense", "croit" ou "raisonne".

	** DEFINITION D'UN MODELE DE LANGAGE : "a model doing just that one thing: generating statistically likely continuations of word sequences"

	** "But an *LLM* ‚Äî the bare-bones model ‚Äî is *not in the business of making judgement*s. It just models what words are likely to follow from what other words"
	** "To build a question-answering system, the LLM simply has to be supplemented with a dialogue management system that queries the model as appropriate."
	** "If we prompt an LLM with ‚ÄúAll humans are mortal and Socrates is human therefore‚Äù, we are not instructing it to carry out deductive inference. Rather, we are asking it the following question. Given the statistical distribution of words in the public corpus, what words are likely to follow the sequence ‚ÄòAll humans are mortal and Socrates is human therefore‚Äù. A good answer to this would be ‚ÄúSocrates is mortal‚Äù."
	** "This ability to learn a general pattern from a few examples in a prompt prefix, and to complete sequences in a way that conforms to that pattern, is sometimes called *in-context learning* or *few-shot prompting*."
	** "As usual, though, it's a good idea to remind ourselves that the question really being posed to the model is of the form ‚ÄúGiven the statistical distribution of words in the public corpus, what words are likely to follow the sequence S‚Äù, where in this case the sequence S is the chainof-thought prompt prefix plus the user's query. The sequences of tokens that are most likely to 8 follow S will have a similar form to sequences found in the prompt prefix, [...]"
	** "The noteworthy thing is that next token prediction is sufficient for solving previously unseen reasoning problems, even if unreliably. How is this possible? Certainly it would not be possible if the LLM were doing nothing more than cutting-andpasting fragments of text from its training set and assembling them into a response. But this is not what an LLM does. Rather, *an LLM models a distribution that is unimaginably complex, and allows users and applications to sample from that distribution*."
	** p9 : "In this distribution, the most likely continuation of a piece of text containing a reasoning problem, if suitably phrased, will be an attempt to solve that reasoning problem. It will take this form, this overall shape, because that is the form that a generic human response would take. Moreover, because the vast corpus of published human text contains numerous examples of reasoning problems accompanied by correct answers, the most likely continuation will sometimes be the correct answer. When this occurs, it is *not because the correct answer is a likely individual human response*, but *because it is a likely collective human response*."
	** p10 "In contemporary LLM-based applications, it is rare for a language model trained on a textual corpus to be used without further finetuning. This could be supervised fine-tuning on a specialised dataset, or it could be via reinforcement learning from human preferences (RLHF : Reinforcement Learning from Human Feedback)"

* Large Language Model Training in 2023 : https://research.aimultiple.com/large-language-model-training/

	** Bon sch√©ma de l'architecture Transformer : le sch√©ma classique que l'on trouve souvent +
	-> En fait, pr√©f√©rer celui de ce site, le m√™me mais comment√© : https://pandia.pro/guide/quest-ce-quun-grand-modele-de-langage-llm-large-language-model/ +
	image:transformer-architecture.png[]

	** Training a large language model from the ground up requires significant investment, a *more economical alternative is to fine-tune* an existing language model to tailor it to your specific use case. +
	*A single training run for GPT-3* is estimated to cost *around $5 million*.

	** After training, the model is evaluated on a test dataset that has not been used as a training data set to measure the model's performance. Based on the evaluation results, the model may require some fine-tuning by adjusting its hyperparameters, changing the architecture, or training on additional data to improve its performance.

	** *Training of an LLM* consists of two parts: *pre-training* and *task-specific training*. 
		*** Pre-training is part of training that enables the model to learn the general rules and dependencies within a language, which takes a significant amount of data, computational power, and time to complete. The large language models discussed in the paper require supercomputer systems with several AI chips (ex. NVIDIA DGX A100 starts at $199,999). Once maintenance and power costs are added, *pre-training of a large language model* is an investment in the magnitude of *millions*.
		*** To make large language models *more accessible* for enterprises, LLM developers are offering services for enterprises looking to leverage language models. NVIDIA's NeMO is an example of these services, which offer pre-trained LLMs for fine-tuning and *specific task training* to suit specific use cases. The specific task training adds an additional layer to the model which requires much less data, power, and time to train; making large models accessible for enterprise use. The new task-specific layer is trained with few-shot learning, which aims for accurate outputs with less training data.

* Large Language Models: Complete Guide in 2023 : https://research.aimultiple.com/large-language-models/

	** *D√©finition d'un LLM* : Large language models (LLMs) are *foundation models* that utilize *deep learning* in *natural language processing* (NLP) and *natural language generation* (NLG) tasks. +
	For the purpose of helping them learn the complexity and linkages of language, *large language models are pre-trained* on a vast amount of data. Using techniques such as:
		*** Fine-tuning
		*** In-context learning
		*** Zero-/one-/few-shot learning
	+
	these models can be adapted for downstream (specific) tasks (see Figure 2). +
	{sb}
	An LLM is essentially a *Transformer-based neural network*, introduced in an article by Google engineers titled https://arxiv.org/abs/1706.03762[‚ÄúAttention is All You Need‚Äù] in 2017. +
	*The goal of the model is to predict the text that is likely to come next*. The sophistication and performance of a model can be judged by how many parameters it has. A model's parameters are the number of factors it considers when generating output.
		*** Voir √©galement les fiches wikipedia : 
			**** l'architecture Deep Learning Transformer : https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)
			**** "Attention Is All You Need" : https://en.wikipedia.org/wiki/Attention_Is_All_You_Need

				***** "Seq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, *which prevented them to be accelerated on GPUs*." +
				[...] +
				One of its authors, Jakob Uszkoreit, suspected that attention without recurrence is sufficient for language translation, thus the title "attention is ALL you need"
				***** In 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the "Attention is all you need" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance. Its *parallelizability* was an *important factor to its widespread use* in large neural networks.

	** There are many *open-source language* models that are *deployable on-premise* or in a *private cloud*. +
	Example : 
		*** BLOOM
		*** NeMO LLM
		*** XLM-RoBERTa
		*** XLNet
		*** Cohere
		*** GLM-130B

* Explications sur les *Foundation models* : https://research.aimultiple.com/foundation-models/

	** A foundation model is a kind of model that has been trained in such a manner that it can be used for downstream tasks. The foundation model can therefore be effective for tasks for which it has not previously been trained.
	** Foundation models need to be adapted because they *serve as a base for new models*, and there are numerous approaches to do this, such as:
		*** *Fine-tuning* : +
		This is the process of adopting a given model to meet the needs of a different task. Thus, instead of generating a new model for this purpose, a modification will suffice.
		*** *In-context learning* : +
		Using this approach, models can learn how to perform a task with minimum training and without fine-tuning, unlike conventional approaches.
+
image:foundation-models-working.png[]
image:foundation-models-tooling.png[]

* https://pandia.pro/guide/quest-ce-quun-grand-modele-de-langage-llm-large-language-model
	** Les Transformer ont r√©volutionn√© le traitement du langage naturel en permettant une meilleure compr√©hension du contexte et une meilleure prise en charge de longues phrases.
	** bon sch√©ma sur l'architecture des transformers

=== LLM Multimodal

TO BE COMPLETED

=== Tokens

	* https://platform.openai.com/docs/introduction/key-concepts : +
		** Our models understand and process text by breaking it down into tokens. Tokens can be *words* or just *chunks of characters*. 
		** The number of tokens processed in a given API request depends on the length of both your inputs and outputs. +
		As a rough rule of thumb, *1 token is approximately 4 characters or 0.75 words for English text*. +
		One limitation to keep in mind is that your text prompt and generated completion combined must be no more than the model's maximum context length (for most models this is 2048 tokens, or about 1500 words). Check out our tokenizer tool to learn more about how text translates to tokens."
	* https://platform.openai.com/docs/quickstart/adjust-your-settings "Deep Dive - Understanding tokens and probabilities"
		** l'outil de comptage des tokens utilis√© dans cette section doit quasi certainement √™tre le tokenizer tool d'OpenAI (https://platform.openai.com/tokenizer)

	* https://platform.openai.com/docs/quickstart/closing
		** The total number of tokens processed in a single request (both prompt and completion) can't exceed the model's maximum context length. 
		For most models, this is 4,096 tokens or about 3,000 words. As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text. 
		**Pricing is pay-as-you-go per 1,000 tokens, with $5 in free credit that can be used during your first 3 months.
		One limitation to keep in mind is that, for most models, a single API request can only process up to 4,096 tokens between your prompt and completion.

=== "Settings" / param√®tres de personnalisation des mod√®les de langages

"Settings" pouvant √™tre ajust√©s pour modifier la compl√©tion des mod√®les de langage : 

	* On en retrouve une explication au niveau de la documentation officielle d'OpenAI pour ses API de completion et de Chat completion :
		** https://platform.openai.com/docs/api-reference/chat/create
		** https://platform.openai.com/docs/api-reference/completions/create

	* *Temperature* : 

		** https://platform.openai.com/docs/quickstart/adjust-your-settings +
		Remember that the model predicts which text is most likely to follow the text preceding it. Temperature is a value between 0 and 1 that essentially *lets you control how confident the model should be when making these predictions*. Lowering temperature means it will take fewer risks, and completions will be more accurate and deterministic. Increasing temperature will result in more diverse completions.
			*** Section "Understanding tokens and probabilities", il est expliqu√© qu'avec une temp√©rature de 0, le mod√®le compl√©tera TOUJOURS le texte avec le token ayant la plus forte probabilit√© (voir l'exemple donn√©). +
			Si on augmente la temp√©rature, le mod√®le "prendra plus de risques" et envisagera des tokens avec des probabilit√© plus basses.

		** Thomas : La temp√©rature est un param√®tre qui contr√¥le le degr√© de "variabilit√©" des pr√©dictions de g√©n√©ration de texte du mod√®le. Plus la temp√©rature est √©lev√©e, plus les pr√©dictions seront cr√©atives et vari√©es, mais aussi moins pr√©cises et coh√©rentes.
		Avec une temp√©rature de 0, le mod√®le compl√©tera TOUJOURS le texte avec le token ayant la plus forte probabilit√©, quel que soit le nombre de fois o√π la m√™me question est pos√©e.

	* *top_p* (top probability) :

		** An alternative to sampling with temperature, called *nucleus sampling*, where the model considers the results of the tokens with top_p probability mass. *So 0.1 means only the tokens comprising the top 10% probability mass are considered*. +
		We generally recommend altering this or temperature but not both.
			*** et apr√®s un bon 2h de recherche, oui, c'est d√©finition est bien la bonne

		** il est vraiment difficile de trouver des explications claires sur ce param√®tre
			*** Et quand on interroge ChatGPT lui-m√™me sur le sujet, on a du grand n'importe quoi en r√©ponse !
				**** et c'est bien la preuve que ChatGPT ne "raisonne pas"
			
			*** Pour une tr√®s bonne explication, voir https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api-a-few-tips-and-tricks-on-controlling-the-creativity-deterministic-output-of-prompt-responses/172683
				**** Top_p sampling is an alternative to temperature sampling. Instead of considering all possible tokens, GPT-3 considers only a subset of tokens (*the nucleus*) whose cumulative probability mass adds up to a certain threshold (top_p).
				For example, if top_p is set to 0.1, GPT-3 will consider only the tokens that make up the top 10% of the probability mass for the next token. This allows for dynamic vocabulary selection based on context.
				**** l'article propose √©galement un tr√®s bon tableau sur la combinatoire de ces 2 param√®tres et les effets que l'on peut esp√©rer.

			*** Mes "derniers" √©changes avec ChatGPT sur le sujet : 
				**** Thomas : Je viens de relire l'explication de la documentation officielle d'OpenAI sur le param√®tre top_p. + 
				La chose suivante est expliqu√©e "0.1 means only the tokens comprising the top 10% probability mass are considered". Donc, j'ai l'impression que contrairement √† tes pr√©c√©dentes explications, ce 0.1 ne veut pas dire que ne seront consid√©r√©s que les mots avec une probabilit√© cumul√©e sup√©rieure √† 10%, MAIS que ne seront consid√©r√©s que les mots repr√©sentant les 10% de mots avec les plus fortes probabilit√©s. +
				N'est-ce pas plut√¥t cela ? +
				Car dans ce cas, la valeur par d√©faut de 1 devient logique : elle repr√©senterait les mots repr√©sentant les 100% des mots avec les plus fortes probabilit√©s, donc tous les mots.
				**** ChatGPT : Vous avez raison, je m'excuse pour la confusion dans mes explications pr√©c√©dentes. Vous avez interpr√©t√© correctement la signification du param√®tre "top_p" (√©galement connu sous le nom de "nucleus sampling") tel que d√©crit dans la documentation officielle d'OpenAI. +
				En effet, lorsque "top_p" est d√©fini sur 0.1, cela signifie que seuls les tokens repr√©sentant les 10% de la masse de probabilit√© la plus √©lev√©e sont consid√©r√©s pour la g√©n√©ration de texte. Cela permet de restreindre la diversit√© des mots g√©n√©r√©s et de favoriser les mots avec les probabilit√©s les plus √©lev√©es. +
				Dans ce contexte, la valeur par d√©faut de 1 pour "top_p" repr√©sente en effet l'inclusion de tous les mots, car cela signifie que 100% des tokens sont pris en compte, y compris ceux avec des probabilit√©s plus faibles. Cela offre une g√©n√©ration de texte plus ouverte sans seuil sur la probabilit√© cumul√©e des mots. +
				Je m'excuse pour les pr√©c√©dentes erreurs dans mes explications et je vous remercie de votre clarification.
			
		** Thomas : Le "top_p" (top probability) contr√¥le le nombre de mots possibles que le mod√®le consid√®re pour la g√©n√©ration de texte. +
		Un top_p de 0,15 indique que seuls les tokens dont la somme des probabilit√©s appartient aux 15% des plus fortes sommes de probabilit√©s possibles de r√©ponse seront consid√©r√©s par le mod√®le.

	* -> Ces 2 param√®tres, *temp√©rature et top_p*, permettent de contr√¥ler le *"taux de cr√©ativit√©"* de la g√©n√©ration du mod√®le. +
	image:temperature-top-p-combinatoire-usages.jpg[]

	

	* *presence_penalty* (Le taux de pr√©sence) : 

		** OpenAI : Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
		** https://platform.openai.com/docs/api-reference/parameter-details
		The *frequency* and *presence penalties* found in the Completions API can be used to *reduce the likelihood of sampling repetitive sequences of tokens*. They work by directly modifying the logits (un-normalized log-probabilities) with an additive contribution.

		** https://aipower.org/presence-penalty/ (TRES BON) : +
		the presence_penalty parameter modifies the probability distribution to make less likely words that were present in the input prompt or seed text. This encourages the model to generate words that were not in the input. It works by scaling down the log probabilities of words that were present in the input, making it less likely for the model to generate these words that are already in the input.

		** Thomas : contr√¥le la probabilit√© de g√©n√©rer du texte qui soit d√©j√† pr√©sent dans le prompt. Cela permet d'encourager le mod√®le √† g√©n√©rer de "nouveaux mots", non d√©j√† donn√©s dans le prompt.

	* *frequency_penalty* (Le taux de fr√©quence) : 

		** OpenAI : Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.

		** https://aipower.org/presence-penalty/: +
		The frequency_penalty parameter modifies the probability distribution to make less likely words that the model has seen more frequently during its training. This encourages the model to generate novel or less common words. It works by scaling down the log probabilities of words that the model has seen frequently during training, making it less likely for the model to generate these common words.

		** Thomas : contr√¥le la probabilit√© de g√©n√©rer du texte en fonction de la fr√©quence √† laquelle ce dernier a d√©j√† √©t√© rencontr√© par le mod√®le durant son entra√Ænement.

	* To put it simply, frequency_penalty penalizes the model for generating the common words that the model has seen a lot during training whereas presence_penalty penalizes the model for generating the words that are present in the input text.

."taux de diversit√©"
[NOTE]
====
* The *frequency_penalty* and *presence_penalty* parameters are both *controls for the diversity of the generated text* produced by GPT-3 (Generative Pre-trained Transformer 3). They both adjust the trade-off between the likelihood of the generated text and its novelty.
	** Source : https://aipower.org/presence-penalty/

* Doc officielle OpenAI : The frequency and presence penalties found in the Completions API can be used to reduce the likelihood of sampling repetitive sequences of tokens.

* Thomas : Ces 2 param√®tres, *frequency_penalty* et *presence_penalty*, contr√¥lent le *"taux de diversit√©"* du texte g√©n√©r√©, c'est-√†-dire le compromis effectu√© par le mod√®le entre "nouveaut√©" et texte d√©j√† connu.
====

	* *best_of* : 

		** Generates "best_of" number of completions server-side and returns the "best" (*the one with the highest log probability per token*). +
		Results cannot be streamed. +
		When used with n, best_of controls the number of candidate completions and n specifies how many to return, meaning best_of must be greater than n.
		
		** Note: Because this parameter generates many completions, it *can quickly consume your token quota*. +
		-> Use carefully and ensure that you have reasonable settings for max_tokens and stop.

	* *logprob*

		** Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response. +
		The maximum value for logprobs is 5. If you need more than this, please contact us through our Help center and describe your use case.
		
		** Pour plus d'explication sur ces logarithmes de probabilit√©s, voici quelques ressources : 
			*** http://gptprompts.wikidot.com/intro:logprobs : une tr√®s bonne explication de la m√©canique des logarithmes de probabilit√©s, avec des exemples de compl√©tion des mod√®les d'OpenAI. +
			L'article d√©crit √©galement le fonctionnement de la temperature et de "best_of" pour s√©lectionner la compl√©tion (le token) la plus appropri√©e.
				**** D√©finition tr√®s claire : +
				*The logprob is the log of the probability that a token comes next*. +
				In computer science, *multiplying is computationally expensive and adding is cheap*, so a lot of time when you have to multiple probabilities you *take the logs* and add them instead to get the same result. To convert a logprob back to the original probability, you just take e^logprob, which in python is np.e**logprob (using import numpy as np).
			*** https://fr.wikipedia.org/wiki/Logarithme

=== Fine-tuning des mod√®les de langage

Pour le "fine-tuning" √† proprement parler : 

	* https://openai.com/pricing, section "Fine-tuning models"
		** De nouveau, seuls les mod√®les bas√©s sur GPT 3 peuvent √™tre fine-tun√© : Ada, Babbage, Curie, Davinci
		** Important de bien choisir son mod√®le par rapport au use case et au c√¥ut du mod√®le.
			*** Exemple : For this use case (*classification*) we fine-tuned an *ada model* since it will be faster and cheaper, and the performance will be comparable to larger models because it is a classification task.

	* https://platform.openai.com/docs/guides/fine-tuning

		** *Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt*, letting you achieve better results on a wide number of tasks. Once a model has been fine-tuned, you won't need to provide examples in the prompt anymore. +
		-> This *saves costs* and enables *lower-latency requests*.
			*** Donc meilleur co√ªt et meilleures performances

		** Training data is how you teach GPT-3 what you'd like it to say.
		** *The more training examples you have, the better*. We recommend having at least a couple hundred examples. In general, we've found that each doubling of the dataset size leads to a linear increase in model quality.

		** When a job has succeeded, the *fine_tuned_model* field will be populated with the name of the model. You may now *specify this model as a parameter to our Completions API*, and *make requests to it using the Playground*.

		** *Fine-tuning* is a powerful technique to *create a new model that's specific to your use case*. Before fine-tuning your model, we strongly recommend reading these best practices and specific guidelines for your use case below.

	* ChatGPT : Le fine-tuning pour les mod√®les de langage OpenAI est un processus par lequel un mod√®le pr√©-entra√Æn√©, tel que GPT, est adapt√© √† des t√¢ches ou des domaines sp√©cifiques en utilisant un jeu de donn√©es suppl√©mentaire. Contrairement √† l'entra√Ænement initial du mod√®le, qui se fait sur de vastes ensembles de donn√©es, le fine-tuning tire profit de donn√©es plus sp√©cifiques et cibl√©es.

.https://research.aimultiple.com/large-language-models/#easy-footnote-bottom-1-63352
image:LLM-pre-training-and-fine-tuning.jpg[]

=== Limite du contexte du mod√®le de langage

* Voir la notion de "MAX TOKENS" : https://platform.openai.com/docs/models/gpt-3-5
	** 4096 tokens pour ChatGPT (gpt-3.5-turbo-0301)

* https://platform.openai.com/docs/introduction/key-concepts +
"One limitation to keep in mind is that *your text prompt and generated completion combined* must be no more than the model's maximum context length"
* M√™me dans cette autre section de la doc officielle : https://platform.openai.com/docs/quickstart/pricing et https://platform.openai.com/docs/quickstart/closing

	** The total number of tokens processed in a single request (both prompt and completion) can't exceed the model's maximum context length. For most models, this is 4,096 tokens or about 3,000 words.
	** One limitation to keep in mind is that, for most models, a single API request can only process up to 4,096 tokens between your prompt and completion.

-> Or le contexte pass√© est ajout√© au prompt (comportement par d√©faut de ChatGPT) et doit donc √™tre compt√©, pris en compte pour d√©terminer la longueur actuelle du contexte.

* Voir √©galement cet article, https://community.openai.com/t/chatgpt-api-maximum-token/83321, qui propose des strat√©gies de pruning pour diminuer la taille du contexte : 

	** Delete ‚Äúrole: system‚Äù messages, since they are weak and the conversation is already ongoing (so far have not had great results with the system role anyway, but I have not fully tested either),
	** Truncate the messages starting with the oldest (brute force).
	** Use max_tokens and summarize stored messages using a different model.
	** Remove prior ‚Äúrole: assistant‚Äù messages.

* Quand le contexte transmis √† ChatGPT d√©passe le maximum autoris√© par son mod√®le de langage, ce dernier est automatiquement tronqu√© ou r√©duit. +
La documentation officielle manque de d√©tails pr√©cis sur le m√©canisme de troncature utilis√©. +
N√©anmoins, si l'on pose directement la question √† ChatGPT, voici sa r√©ponse : 

	** Lorsque le contexte donn√© √† ChatGPT d√©passe la taille maximale de son mod√®le de langage (4096 tokens pour gpt-3.5-turbo), il est n√©cessaire de le tronquer ou de le r√©duire pour le rendre compatible. Il y a diff√©rentes approches possibles pour g√©rer cette situation :
		*** Troncature : Le contexte peut √™tre tronqu√© en supprimant les parties les plus anciennes ou moins pertinentes afin de respecter la limite de taille. Cela peut entra√Æner la perte d'informations importantes et limiter la capacit√© du mod√®le √† comprendre le contexte complet.
		*** R√©sum√© ou agr√©gation : Plut√¥t que de tronquer le contexte, il est possible de cr√©er un r√©sum√© ou une agr√©gation des parties essentielles du contexte afin de r√©duire sa taille tout en pr√©servant les informations cl√©s. Cela n√©cessite de faire des choix sur les parties √† inclure et √† exclure, ce qui peut introduire une certaine subjectivit√©.
		*** Utilisation d'un contexte contextuel : Plut√¥t que de fournir tout le contexte pr√©c√©dent, il est possible d'utiliser un contexte plus r√©cent ou plus sp√©cifique pour guider la g√©n√©ration de ChatGPT. Cela permet de limiter la quantit√© de texte envoy√© au mod√®le tout en conservant la pertinence de la r√©ponse.

	** Thomas : Est-il possible de savoir quel comportement ChatGPT utilise parmi ceux que tu as cit√©s en fonction des requ√™tes qui lui sont adress√©es ? Y a-t-il moyen de configurer ce comportement ?

	** ChatGPT : En utilisant l'API OpenAI, il n'est *pas directement possible de sp√©cifier ou de configurer explicitement le comportement sp√©cifique de troncature*, de r√©sum√© ou d'agr√©gation du contexte parmi ceux mentionn√©s pr√©c√©demment. +
	L'API ne fournit pas de param√®tre distinct pour contr√¥ler ces m√©canismes de gestion du contexte. +
	Lorsque le contexte d√©passe la taille maximale autoris√©e, *l'API OpenAI effectue automatiquement une troncature pour le rendre compatible avec le mod√®le de langag*e. La fa√ßon dont la troncature est r√©alis√©e peut varier en fonction de la structure interne de l'algorithme de l'API, mais les d√©tails exacts de son fonctionnement ne sont pas sp√©cifi√©s publiquement.

* Voir √©galement cette r√©ponse d'OpenAI sur son forum : https://community.openai.com/t/when-conversation-grows-larger-did-chatgpt-include-all-of-the-conversation-context-for-every-chat/121876
	** logankilpatrick Developer Advocate (OpenAI Staff) : No, all of the messages are likely not included. There is a finite amount of context that can be passed for each request. *We have a truncation algorithm running behind the scenes* that tried to determine what the important information is from previous messages that needs to be included.

Cette troncature du contexte peut √™tre consid√©r√©e comme en partie responsable de certains des "d√©rapages" ayant d√©j√† eu lieu avec le chatbot lors de conversation longues entre un humain et lui.

== DEVELOPPEMENT ASSISTE PAR L'IA

=== GitHub Copilot

* Documentation officielle de GitHub Copilot : https://docs.github.com/en/copilot

== PREPARATION DE TALKS SUR L'IA GENERATIVE

=== 2023 - Pr√©paration de talk de pr√©sentation des IA G√©n√©rative

==== Que sont les IA g√©n√©ratives ? 

* DONE - 2022/12/15 : chatGPT la r√©volution : AI g√©n√©rative, vous pouvez r√©ellement converser avec elle, elle passe le test de Turing 

* DONE - https://www.commentcoder.com/chatgpt/#comment-commencer-%C3%A0-utiliser-chatgpt- +
OpenAI est une entreprise am√©ricaine cr√©√©e par Elon Musk et Sam Altman en 2015 √† San Francisco en Californie. Son but est de cr√©er des avanc√©es dans l'intelligence artificielle. Ses services les plus connus sont DALL-E 2, GPT-3 et ChatGPT.

* DONE - ChatGPT est un agent conversationnel qui s'appuie sur le mod√®le de langage GPT-3 pour interpr√©ter le texte d'un humain et g√©n√©rer des r√©ponses pr√©cises et intelligibles, en fran√ßais comme en anglais. Toutefois, ses connaissances se limitent √† tous les √©v√©nements pr√©c√©dant l'ann√©e 2021.

* DONE - AI g√©n√©rative, vous pouvez r√©ellement converser avec elle, elle passe le test de Turing 

* DONE - 2023/01/23 : Article sur l'usage de ChatGPT : https://www.blogdumoderateur.com/tout-savoir-sur-chatgpt/

* DONE - Malgr√© tout ce que peuvent en dire les m√©dias, il n'est pas correct de qualifier de ChatGPT (et de ses consorts) "d'IA g√©n√©rationnelle" √©tant donn√© que ce terme n'existe pas r√©ellement dans le domaine de l'IA, et aurait plut√¥t tendance √† se r√©f√©rer √† des algorithmes g√©n√©tiques. +
	** Il faut √† la place parler d'IA conversationnelles ou d'IA "g√©n√©ratives" (mais PAS "g√©n√©rationnelles")

	** D√©finition des *algorithmes g√©n√©tiques* : +
	Les algorithmes g√©n√©tiques sont une m√©thode d'optimisation qui utilise des techniques d'√©volution biologique pour r√©soudre des probl√®mes complexes. +
	L'algorithme commence par cr√©er une population initiale de solutions possibles. Ensuite, des op√©rations de s√©lection, de croisement et de mutation sont appliqu√©es sur la population pour produire de nouvelles solutions. +
	Les solutions sont √©valu√©es pour leur "fitness" et les plus performantes sont s√©lectionn√©es pour cr√©er une nouvelle population. Ce processus est r√©p√©t√© jusqu'√† ce qu'une solution satisfaisante soit trouv√©e ou que le nombre d'it√©rations pr√©d√©fini soit atteint. +
	Les algorithmes g√©n√©tiques sont utilis√©s dans de nombreuses applications d'optimisation, notamment en ing√©nierie, en √©conomie et en informatique.

* DONE - 2023/01 : https://www.commentcoder.com/chatgpt/
	** Tr√®s bon article pr√©sentant ChatGPT dans sa globalit√©

	** Quel mod√®le d'apprentissage utilise Chat-GPT ?
		*** ChatGPT utilise une s√©rie de mod√®les et est connue sous le nom de ‚ÄúGPT 3.5‚Äù, elle est compos√©e des mod√®les d'apprentissage text-davinci-003 pour le texte et code-davinci-002 pour le code. +
		On a donc des IA diff√©rentes pour le code et pour le texte. C'est peut-√™tre ce qui explique pourquoi ChatGPT est si puissant pour nous aider avec le code mais laisse parfois √† d√©sirer dans ses r√©ponses textuelles ü§∑.

	** Quelle version de GPT utilise ChatGPT ?
		*** La version de *GPT* (*Generative Pretrained Transformer-3*) utilis√©e par ChatGPT est GPT-3.5 qui a termin√© de s'entra√Æner au d√©but de l'ann√©e 2022.
		*** üí° C'est d'ailleurs parce que ChatGPT a fini de parcourir les sources textuelles et de code d√©but 2022 qu'on dit que ChatGPT donne des r√©sultats valides que jusqu'en 2021 (inclus).
		*** Le pr√©d√©cesseur de ChatGPT s'appelle InstructGPT qui utilise le mod√®le d'apprentissage GPT-3. La diff√©rence entre Instruct GPT et Chat GPT est impressionnante, vivement une la prochaine version d'IA conversationnelle d'OpenAI qui utilisera GPT-4 !

* DONE - ChatGPT a √©t√© cod√© en Python.
* DONE - https://openai.com/blog/chatgpt/ : ChatGPT and GPT 3.5 were trained on an Azure AI supercomputing infrastructure.

** DONE - Quelle version de GPT utilise ChatGPT ?
		*** La version de *GPT* (*Generative Pretrained Transformer-3*) utilis√©e par ChatGPT est GPT-3.5 qui a termin√© de s'entra√Æner au d√©but de l'ann√©e 2022.
		*** üí° C'est d'ailleurs parce que ChatGPT a fini de parcourir les sources textuelles et de code d√©but 2022 qu'on dit que ChatGPT donne des r√©sultats valides que jusqu'en 2021 (inclus).
		*** Le pr√©d√©cesseur de ChatGPT s'appelle InstructGPT qui utilise le mod√®le d'apprentissage GPT-3. La diff√©rence entre Instruct GPT et Chat GPT est impressionnante, vivement une la prochaine version d'IA conversationnelle d'OpenAI qui utilisera GPT-4 !

	** Qu'est-ce que GPT-4 ?
		*** GPT-4 est la prochaine version de GPT. L'IA aura √©t√© entra√Æn√©e sur plus de 100 000 milliards de param√®tres (contre 175 milliards pour GPT-3). Generative Pre-trained Transformer-4 devrait arriver en cette premi√®re moiti√© d'ann√©e 2023.

prompt : la question que vous souhaitez poser √† ChatGPT ; pensez √† int√©grer du contexte pour obtenir des r√©ponses pertinentes.

* Tr√®s bonne d√©finition de wikipedia : https://fr.wikipedia.org/wiki/ChatGPT : 
	** "ChatGPT est un prototype d'agent conversationnel utilisant l'intelligence artificielle, d√©velopp√© par OpenAI et sp√©cialis√© dans le dialogue."

DONE - L'agent conversationnel de ChatGPT est bas√© sur le mod√®le de langage GPT-3 d'OpenAI, et est affin√© en continu gr√¢ce √† l'utilisation de techniques d'apprentissage supervis√© et d'apprentissage par renforcement, afin d'am√©liorer les performances du logiciel.

.supervised fine-tuning vs reinforcement learning
[NOTE]
====
Le supervised fine-tuning est utilis√© pour ajuster un mod√®le pr√©-entra√Æn√© √† une t√¢che sp√©cifique √† l'aide de donn√©es d'entra√Ænement suppl√©mentaires, tandis que le reinforcement learning est utilis√© pour apprendre une politique de prise de d√©cision optimale √† partir de l'interaction avec un environnement complexe et en maximisant une r√©compense.
====

DONE - 2023/02/09 - BFM Tech & Co : ChatGPT moteur de r√©ponse l√† o√π Google est un moteur de recherche

*SLIDES*

* *D√©finitions* : 

	** IA g√©n√©rative : Une IA g√©n√©rative a pour but de g√©n√©rer de nouvelles donn√©es, telles que du texte (agents conversationnels comme ChatGPT), des images (Dall-E) ou de la musique, √† partir d'un ensemble de donn√©es d'entra√Ænement.

	** ChatGPT : ChatGPT appartient √† cette cat√©gorie des IA g√©n√©rative. +
	Il s'agit d'un agent conversationnel (un "chatbot"), bas√© sur le mod√®le de langage GPT 3.5, utilisant un mix d'apprentissage supervis√© et d'apprentissage par renforcement pour affiner en continu son mod√®le afin d'am√©liorer ses performances.

	** *Mod√®le de langage* : il s'agit d'un type de mod√®le d'apprentissage automatique con√ßu pour comprendre et g√©n√©rer du langage humain. Il est entra√Æn√© sur de grandes quantit√©s de donn√©es textuelles pour apprendre la structure, la grammaire et le sens du langage naturel. 

	** *Apprentissage supervis√©* : il s'agit d'une m√©thode d'apprentissage automatique o√π il est question d'apprendre √† classer les donn√©es au sein de cat√©gories (classes / √©tiquettes) pr√©d√©finies : on conna√Æt ces derni√®res √† priori. +
	On le distingue de l'apprentissage NON supervis√© o√π au contraire il est question de trouver des cat√©gories dans lesquelles les donn√©es pourraient √™tre regroup√©es sur la base de caract√©ristiques communes. On ne conna√Æt PAS ces cat√©gories √† priori.

		*** voir https://fr.wikipedia.org/wiki/Apprentissage_supervis%C3%A9
		*** et la tr√®s bonne explication de Wikipedia dans l'article sur l'apprentissage non supervis√© : https://fr.wikipedia.org/wiki/Apprentissage_non_supervis%C3%A9

	** *Apprentissage par renforcement* : il s'agit d'une m√©thode d'apprentissage automatique consistant, pour un agent autonome (comme un chatbot), √† apprendre les actions √† entreprendre, dans environnement donn√©, √† partir d'essais et d'erreurs, de fa√ßon √† maximiser une r√©compense quantitative donn√©e par l'environnement au cours du temps.

		*** comme pr√©c√©demment, voir les tr√®s bonnes informations de l'article Wikipedia sur l'apprentissage non supervis√© : https://fr.wikipedia.org/wiki/Apprentissage_non_supervis%C3%A9

	** Concernant plus sp√©cifiquement ChatGPT, son *mod√®le de langage* est actuellement *GPT 3.5* (Generative Pretrained Transformer), qui est compos√© de plusieurs mod√®les d'apprentissage, certains d√©di√©s au texte (text-davinci-003) et d'autres au code (code-davinci-002).
		*** Le mod√®le d'apprentissage du code est jug√© comme √©tant particuli√®rement performant (plus encore que son homologue d√©di√© au texte)
		*** GPT s'appuie le mod√®le d'apprentissage profond Transformer, √† savoir un type de r√©seau de neurones, entra√Æn√© sur de grandes quantit√©s de donn√©es textuelles pour apprendre les relations entre les diff√©rents √©l√©ments du texte, comme les mots et les phrases.
		*** Le mod√®le *Transformer* a √©t√© cr√©√© en 2017 pour l'une des √©quipes de Google AI (Google Brain en l'occurrence). +
		-> Cela fait des ann√©es que ce mod√®le est utilis√© "un peu partout" par Google, mais aussi par Meta : mod√©ration de contenu, recommandation de contenu / ranking, traduction, etc.
			**** Voir l'article anglais de Wikipedia sur le mod√®le Transformer pour avoir plus d'informations sur sa cr√©ation c√¥t√© Google.

* *Quelques informations compl√©mentaires sur OpenAI et ChatGPT lui-m√™me* : 

	** OpenAI est une entreprise am√©ricaine *cr√©√©e par Elon Musk et Sam Altman en 2015* √† San Francisco. +
	Elon Musk en a quitt√© le conseil d'administration en 2018 en raison de conflits d'int√©r√™ts potentiels avec son r√¥le de CEO de Tesla du fait du d√©veloppement de l'IA de ses voitures autonomes.
		*** Voir https://en.wikipedia.org/wiki/OpenAI pour plus de d√©tails sur le d√©part du board d'OpenAI d'Elon Musk
	
	** ChatGPT, tout comme le mod√®le Google LaMDA, aurait *d√©j√† r√©ussi le test de Turing*.
		*** Test de Turing : Un test c√©l√®bre bas√© sur la question : ¬´ Les machines peuvent-elles penser ? ¬ª Le test de Turing part d'un principe simple : si un humain peut avoir une conversation de cinq minutes avec une machine sans s'en rendre compte, alors l'ordinateur a r√©ussi le test.
		*** https://www.pcguide.com/apps/chat-gpt-pass-turing-test/

	** ChatGPT utilise actuellement (2023/02) le mod√®le de langage GPT-3.5 qui a termin√© de s'entra√Æner d√©but 2022, comprendre par l√† qu'il a fini de parcourir les sources textuelles et de code d√©but 2022, raison pour laquelle ses r√©sultats ne sont valides que jusqu'en 2021 (inclus).
	** 1ere moiti√© 2023 sortie la prochaine version du mod√®le de langage GPT : *GPT-4*. +
	Cette version aura donc √©t√© entra√Æn√©e sur des donn√©es plus nombreuses +
	Celui-ci aura √©t√© entra√Æn√©e sur plus de 100 000 milliards de param√®tres, contre "seulement" 175 milliards pour GPT-3. +
	Lorsque ce nouveau mod√®le int√©grera ChatGPT, *le gain de puissance r√©sultant est annonc√© comme √©tant de x100*...

	** ChatGPT a √©t√© cod√© en Python
	** ChatGPT et GPT 3.5 ont √©t√© entra√Æn√©s sur l'infrastructure supercomputer d'Azure AI. +
	Microsoft et OpenAI se sont entendus sur un partenariat technologique et commercial touchant les technologies de l'IA d√®s 2019/07
		*** https://news.microsoft.com/2019/07/22/openai-forms-exclusive-computing-partnership-with-microsoft-to-build-new-azure-ai-supercomputing-technologies/

* *Uses Cases* : 

	** les possibilit√©s de g√©n√©ration de contenu textuel sont extr√™mement vastes. +
	ChatGPT a une valeur ajout√©e d'autant plus forte que la culture g√©n√©rale et les connaissances acad√©miques sont importantes, l'outil ayant √©t√© entra√Æn√© sur un ensemble de donn√©es impossible √† assimiler pour un √™tre humain (sources Web, Wikipedia, livres, articles de presse, documents professionnels, dialogues, etc.)  +
	Comme principaux cas d'usage on peut citer : 

	** *moteur de "r√©ponse"* pour acc√©der √† de l'information g√©n√©rale ou un savoir acad√©mique : ChatGPT est davantage un moteur de "r√©ponse" l√† o√π Google est un moteur de "recherche"
	** *Assistance √† la r√©daction de tout contenu* : lettres, mails, rapports, dissertation. Il est possible de pr√©ciser le ton, la structure ou la pr√©sentation souhait√©s.
	** *Correction de la langue* : ChatGPT peut aider √† corriger la grammaire, l'orthographe, la ponctuation ou encore le style d'√©criture.
	** *Aide √† la programmation* : ChatGPT pour cr√©er du code r√©pondant √† des besoins algorithmiques ou d'int√©gration dans le langage demand√©. +
	Exemple : "Ecris un code permettant d'initier une connexion √† une base de donn√©es PostgreSQL appel√©e "toto" en utilisant le langage Java"

==== La comp√©tition est lanc√©e entre les acteurs du secteur

* DONE - Le 30 novembre 2022 ChatGPT est arriv√© sur internet et a tout boulevers√© en gagnant 1 million d'utilisateurs en seulement 5 jours !
* DONE - A la fin du mois 2023/01, donc 2 mois apr√®s sa mise √† disposition du public, ChatGPT comptait pr√®s de 100 millions d'utilisateurs actifs. +
Chaque jour en janvier, le chatbot pouvait en moyenne enregistrer environ 13 millions de visites uniques, soit plus du double des visites comptabilis√©es en d√©cembre.
	** https://www.clubic.com/technologies-d-avenir/intelligence-artificielle/actualite-456000-chatgpt-le-nombre-d-utilisateurs-du-chatbot-atteint-des-sommets.html +
	A titre de comparaison, TikTok a mis 9 mois pour atteindre 100 millions d'utilisateurs et Instagram a d√ª patienter 2 ans et demi pour franchir ce seuil.

* DONE - co√ªt de l'infra au quotidien et augmentation (explosion) du nombre d'utilisateurs : voir si une modification de l'usage du service (usage gratuit devenant trop cher) n'est pas √† envisager / craindre

* DONE - Quelles sont les alternatives √† ChatGPT ? +
https://www.commentcoder.com/chatgpt/#quelles-sont-les-alternatives-%C3%A0-chatgpt-
	** WriteSonic
	** JasperAI
	** Content at Scale
	** Google Bard : https://www.commentcoder.com/bard/
	** HuggingChat : https://huggingface.co/chat/

	** Regarder aussi, pour l'aide au d√©veloppement (code) : 
		*** GitHub Copilot, Captain Stack, IntelliCode et Code Whisperer sont quelques-uns d'entre eux qui proposent une exp√©rience de d√©veloppement am√©lior√©e. +
		https://cfp.devoxx.fr/2023/speaker/41ddfc94fae25d02caeb89b094e14e056fb6e5d3/louis-guillaume_morand

* 2023/01/19 - BFM Tech et Co : OpenAI a servi de cheval de Troie pour permettre √† Google et Microsoft de se lancer "publiquement" dans l'IA fa√ßon ChatGPT 

* DONE - Microsoft ajoute ChatGPT √† la suite Office
	** Google est l'√©crasant n¬∞1 des moteurs de recherche, Bing est tr√®s loin derri√®re
		*** https://www.webrankinfo.com/dossiers/etudes/parts-marche-moteurs : 94% de parts de march√© pour Google dans le monde, contre 3% pour Bing
	** Bing n'a donc rien √† perdre en "tenant l'exp√©rience" avec ChatGPT, m√™me si des travers sont possibles
	** https://www.zdnet.fr/actualites/microsoft-se-precipite-pour-mettre-chatgpt-dans-ses-applications-office-39952314.htm#xtor=RSS-1
	** Nom de l'int√©gration de ChatGPT √† Bing : Prometheus

* DONE - 2023/01 : Microsoft met le paquet sur OpenAI et va investir 10 milliards dedans (l'√©diteur de ChatGPT)
	** https://www.clubic.com/pro/entreprises/microsoft/actualite-452997-chatgpt-microsoft-va-investir-10-milliards-de-dollars-dans-openai.html 

* 2022/01/26 - BFM Tech & Co : ChatGPT r√©ussit √† faire trembler Google sur son coeur de m√©tier, ce √† quoi on ne croyait plus (10 personnes au d√©but pour OpenAI arrivent √† faire trembler un GAFA)
	** ce "faire trembler" est discutable, Google ne pouvait pas r√©v√©ler lui-m√™me ce type de techno au public et devait savoir (esp√©rer ?) que d'autres le fassent pour ouvrir la porte. +
	Par contre, avaient-ils r√©ellement bien pes√© les avantages / inconv√©nients au vu de l'engouement du public ? Difficile √† dire... 

* DONE - 2023/10/12 - BFM Tech et Co : LLM (Large Langage Model) et Transformer dans le cadre de ChatGPT
	** Et le mod√®le (?) Transformer est OpenSource MAIS a √©t√© invent√© par des ing√©nieurs de Google)
	** Et Google peut faire plus fort qu'OpenAI car ils ont le soft ET le hard (TensorFlow)
	** Les 2 ecosyst√®mes d'IA les plus d√©velopp√©s au monde sont TensorFlow de Google et PyTorch de Meta.

* 2023/01/23 : Article sur ChatGPT, branle-bas de combat chez Google qui rappelle ses fondateurs.
	** https://www.01net.com/actualites/panique-chez-google-les-fondateurs-reviennent-pour-contrer-chatgpt.html
	** L'ancien directeur de la recherche chez Google, D. Sivakumar, n'a pas h√©sit√© √† parler d'un ¬´ moment de grande vuln√©rabilit√© pour Google ¬ª : +
	"For the first time, I feel that Google's supremacy is under threat for information / knowledge-seeking searches... +
	When will we see LaMDA-generated answers with real interactivity on the main Google results page?" +
	-> Google va lancer plus de 20 nouveaux produits li√©s √† l'intelligence artificielle cette ann√©e, dont plusieurs qui seront pr√©sent√©s lors de la conf√©rence Google I/O 2023 (2023/05)
	** C'est le branle-bas de combat car quelqu'un (OpenAI) a ouvert la bo√Æte de Pandore que les "gros" n'osait pas ouvrir, ou tout simplement ne POUVAIT PAS ouvrir
		*** Maintenant que c'est fait, ces gros vont avoir "l'obligation de suivre" (la belle excuse...), et vont s'en donner √† coeur joie, et cela va √™tre la guerre

* 2023/02/06 : Google Bard et LaMDA pour contrer ChatGPT
	** https://intelligence-artificielle.developpez.com/actu/341183/ChatGPT-reussit-l-entretien-de-codage-Google-pour-un-ingenieur-de-niveau-3-avec-un-salaire-de-183-000-tout-en-indiquant-qu-il-ne-peut-pas-reproduire-la-creativite-humaine/
	** Ne pas oublier que, par exemple, pour ‚Äúcalculer 1 + 1 = 2‚Äù, dans les faits ces IA ne ‚Äúcalculent PAS‚Äù, elles font des d√©ductions sur les r√©sultats √† la question ‚Äúcombien font 1 + 1 ?‚Äù qu'elles ont pu trouver dans leur base de connaissances. +
	Si tout le monde disait ‚Äú1 + 1 = 3‚Äù, elles r√©pondraient 3
	** LaMDA vs ChatGPT : Cependant, l'outil soutenu par Microsoft √©choue de mani√®re spectaculaire lorsqu'il r√©pond √† des √©nigmes logiques, contrairement √† LaMDA. 
	** ‚ÄúGoogle est r√©ticent √† d√©ployer une IA factuellement inexacte, et ChatGPT s'est parfois terriblement tromp√©. Lors d'une r√©cente r√©union, *le chef de l'IA de Google, Jeff Dean, a d√©clar√© que l'entreprise avait une plus grande r√©putation √† prot√©ger et progressait donc ¬´ de mani√®re plus conservatrice qu'une petite startup ¬ª*.‚Äù
	** ‚ÄúGoogle teste √©galement un chatbot appel√© Apprentice Bard‚Äù
	** ‚ÄúBard utilise LaMDA (ou Language Model for Dialogue Applications), le mod√®le de langage de Google pour l'application de dialogue‚Äù
	** ‚ÄúChatGPT fournit des r√©ponses plus longues et plus d√©taill√©es lorsqu'il est invit√© √† faire preuve de cr√©ativit√©, comme avec un sc√©nario de film. Cependant, l'outil soutenu par Microsoft √©choue de mani√®re spectaculaire lorsqu'il r√©pond √† des √©nigmes logiques, contrairement √† LaMDA.‚Äù

DONE - 2023/02/15 : Opera va int√©grer ChatGPT
https://www.clubic.com/navigateur-internet/opera/actualite-457434-opera-entre-dans-la-danse-de-l-ia-voici-comment-le-navigateur-compte-s-y-prendre.html 
Pour proposer via un nouveau bouton des r√©sum√©s d'articles.

MAIS TOUT CECI N'EST FINALEMENT PAS NOUVEAU : 

* DONE - Tout ceci n'est pas nouveau (√©couter Yann Lecun en parler), mais OpenAI l'a r√©v√©l√© au grand jour, ce que les gros acteurs de la tech ne pouvaient pas faire : on ne leur aurait rien "pass√©" c√¥t√© comportement de l'IA, abus de position dominante, cela aurait √©t√© trop risqu√© en termes d'images.
	** L√†, OpenAI a ouvert la voie, les GAFA peuvent maintenant se contenter de dire qu'ils "suivent le chemin".

* DONE - Twitter de Yann LeCun : Ce que fait OpenAI n'est PAS nouveau.

	** Pour rappel Yann LeCun est un chercheur fran√ßais en Intelligence Artificielle et consid√©r√© comme l'un des inventeurs de l'apprentissage profond
		*** Il a dirig√© le labo FAIR de Meta (Facebook Artificial Intelligence Research) de 2013 √† 2018, et est maintenant chercheur / scientifique en chef de l'IA chez Facebook.
		*** en 2021, il est √©lu √† l'Acad√©mie nationale des sciences des Etats-Unis

	** voir le fil de Twit : https://twitter.com/ylecun/status/1617951238108385284?t=-ESJgkDHWnMJjatEmmEmkg&s=19
	** Le mod√®le Transformer a √©t√© √©crit par les ing√©nieurs de Google en 2017
	** OpenAI a surtout comme m√©rite d'avoir permis √† toutes et tous de manipuler une technologie que certains (Google, Meta et d'autres) utilisent d√©j√† sous le capot pour nous proposer des services depuis des ann√©es.
		*** Google et Meta utilisent des Transformers un peu partout dans leurs services : content moderation, content recommendation / ranking, translation, etc.

* DONE - 2022/01/28 : Yann LeCun sur ChatGPT et une critique rationnelle de la techno qui est bien ficel√©e mais n'a rien de r√©volutionnaire (Google en a √©t√© √† la base en 2017, mais c'√©tait dangereux pour eux, comme pour M√©ta, de communiquer publiquement dessus, √† cause de l'image du "grand m√©chant h√©g√©monique")
	** https://www.01net.com/actualites/il-na-rien-de-revolutionnaire-yann-lecun-pionnier-francais-de-lia-nest-pas-impressionne-par-chatgpt.html
	** "OpenAI est une entreprise naissante dont l'√©quation √©conomique est tr√®s diff√©rente de celle de groupes plus √©tablis."
	** "De grandes entreprises auraient pu faire une d√©monstration publique, mais ne l'ont pas fait, car elles ont moins √† gagner qu'une petite entreprise √† la recherche d'investisseurs, et beaucoup plus √† perdre (√† cause de la mauvaise presse)."

*SLIDES*

* *Adoption de ChatGPT* : 

	** ChatGPT a √©t√© ouvert au public le 30 novembre 2022, et a gagn√© 1 million d'utilisateurs en 5 jours seulement.
	** A la fin du mois de janvier 2023, donc 2 mois plus tard, il comptait pr√®s de 100 millions d'utilisateurs actifs. +
	A titre de comparaison, TikTok a mis 9 mois pour atteindre 100 millions d'utilisateurs et Instagram a d√ª patienter 2 ans et demi pour franchir ce seuil.
	** On commence d√©j√† √† voir d'autres outils int√©grer ChatGPT dans leurs services. +
	Opera va int√©grer ChatGPT afin de proposer des r√©sum√©s d'articles dans ses r√©sultats de recherche.

	** L'outil est actuellement toujours gratuit, mais √† la vue de l'explosion du nombre d'utilisateurs, et donc des co√ªts d'infrastructure (on rappelle que ChatGPT est h√©berg√© sur l'infrastructure Azure) une modification √† court terme de l'usage du service (usage gratuit devenant trop cher) ne serait pas surprenante.

* *Les alternatives √† ChatGPT* : 
	** WriteSonic
	** JasperAI
	** Content at Scale
	** Google Bard : https://www.commentcoder.com/bard/

	** Regarder aussi, pour *l'aide au d√©veloppement* (code) : 
		*** GitHub Copilot, Captain Stack, IntelliCode et Code Whisperer sont quelques-uns d'entre eux qui proposent une exp√©rience de d√©veloppement am√©lior√©e. +
		https://cfp.devoxx.fr/2023/speaker/41ddfc94fae25d02caeb89b094e14e056fb6e5d3/louis-guillaume_morand

* *Petit focus sur Google Bard* : 
	** annonc√© le 2023/02/06 par Sundar Pichai sur le blog de Google : https://blog.google/technology/ai/bard-google-ai-search-updates/
		*** l'article en profite pour rappeler que c'est bien Google qui a cr√©√© le mod√®le Transformer en 2017, mod√®le actuellement utilis√© par la plupart (toutes ?) des IA g√©n√©ratives dont il est question aujourd'hui.
		*** Bard : un service d'IA conversationnel exp√©rimental bas√© sur le mod√®le de langage Google LaMDA (Language Model for Dialogue Applications)
		*** Bard est entra√Æn√© sur toutes les donn√©es web index√©es par Google, donc sur des donn√©es plus r√©centes que celles de ChatGPT qui s'arr√™te en 2021 (inclus)
		*** Bard devrait √™tre disponible pour le grand public vers fin mars d√©but avril 2023.

* Focus particulier sur *les actions de Google et Microsoft* suite au succ√®s de ChatGPT : 

	** Microsoft, qui travaille d√©j√† en partenariat avec OpenAI depuis 2019/01, a investi 10 milliards de dollars dans la soci√©t√© (2023/01) et a d√©j√† int√©gr√© ChatGPT aussi bien dans sa suite Office que dans son moteur de recherche Bing.
		*** https://www.theverge.com/2023/2/10/23593980/microsoft-bing-chatgpt-ai-teams-outlook-integration
	** De son c√¥t√©, Google va √©galement int√©grer les r√©ponses de Bard √† son moteur de recherche Search.
	** Et il ne faut pas oublier que dans le domaine de l'IA Google a des dizaines d'ann√©es d'exp√©rience, et ma√Ætrise aussi bien le soft QUE le hard avec TensorFlow. +
	Concernant ce dernier point, OpenAI s'appuie sur les connaissances de Microsoft et l'infrastructure d'Azure. Malgr√© le partenariat engag√© en 2019 entre ces 2 groupes, il s'agit d'un domaine o√π Google est plus exp√©riment√©, et donc s√ªrement plus avanc√©.
		*** Actuellement, les √©cosyst√®mes d'IA les plus d√©velopp√©s sont probablement TensorFlow de Google et PyTorch de Meta

	** Les investissements et les annonces dans l'IA sont omnipr√©sents aujourd'hui, mais il ne faut pas croire qu'ils se limitent √† √™tre une simple r√©action au succ√®s d'OpenAI avec ChatGPT. +
	Pour les grandes entreprises de la Tech comme Google, Microsoft ou Meta, qui travaillent depuis des ann√©es ou des dizaines d'ann√©es sur l'IA, il √©tait difficile d'√™tre les 1eres √† rendre accessible au public ce type de technologie. +
	Comme l'a expliqu√© Jeff Dean, le chef de l'IA de Google, les entreprises comme Google a une "plus grande r√©putation √† prot√©ger" et se devaient de "progresser de mani√®re plus conservatrice qu'une petite startup" (ils doivent absolument √©viter l'image du "grand m√©chant h√©g√©monique") +
	De plus, OpenAI est une entreprise naissante dont l'√©quation √©conomique est tr√®s diff√©rente de celle de groupes plus √©tablis. +
	OpenAI a eu le m√©rite d'avoir permis √† toutes et tous de manipuler une technologie que certains (Google, Meta et d'autres) utilisent d√©j√† sous le capot pour nous proposer des services depuis des ann√©es. +
	Mais maintenant que "la bo√Æte de Pandore" a √©t√© ouverte, ces derniers sont beaucoup plus libres de mettre √† disposition leurs technologies. +

* Yann Lecun, laur√©at du prix Turing (le "Nobel" de l'informatique), ancien directeur du labo "FAIR" (Facebook Artificial Intelligence Research) de Meta et actuel scientifique en chef de l'IA du groupe, le rappelle : +
Google et Meta utilisent d√©j√† depuis des ann√©es des Transformers un peu partout dans leurs services : mod√©ration de contenu, recommendation de contenu / ranking, traduction, etc.
	** Ce que fait ChatGPT est bien ficel√©, bien r√©alis√©, mais n'a *rien de "r√©volutionnaire"* ou de particuli√®rement innovant malgr√© tout ce que peuvent en dire les medias.
	** "ChatGPT et d'autres grands mod√®les de langage ne sont pas sortis de nulle part, ils sont le r√©sultat de d√©cennies de contributions de diverses personnes"

* Et certains ont peut-√™tre √©t√© prudents √† juste raison

	** ChatGPT sur Bing a commenc√© √† se "disputer" avec des utilisateurs suite √† des √©changes trop longs, raison pour laquelle le nombre de questions pouvant lui √™tre pos√© par session, jour et par utilisateur a √©t√© limit√©.
	** Stackoverflow a banni le contenu g√©n√©r√© par ChatGPT de sa plateforme, pour cause d'une trop grande proportion de r√©sultats incorrects.

==== Use Cases pour LLMs

* https://research.aimultiple.com/large-language-models/

	* Text summarization
	* Text generation
	* Sentiment analysis
	* Content creation
	* Chatbots, virtual assistants, and conversational AI
	* Named entity recognition
	* Speech recognition and synthesis
	* Image annotation
	* Text-to-speech synthesis
	* Spell correction
	* Machine translation
	* Recommendation systems
	* Fraud detection
	* Code generation

	* Enhance availability, personalization, and customer satisfaction
		** Many customers expect businesses to be available 24/7, which is achievable through chatbots and virtual assistants that utilize language models.

==== DEMO : ChatGPT

Pour des exemples, regarder : 

	* https://www.commentcoder.com/chatgpt/#comment-chat-gpt-aide-les-d%C3%A9veloppeurs-

* Exemples de code cr√©√© par ChatGPT : 

	** g√©n√©ration d'un code Java de connexion √† une BDD PostgreSQL : +
	Prompt : "Ecris un code permettant d'initier une connexion √† une base de donn√©es PostgreSQL appel√©e "toto" en utilisant le langage Java"
	** g√©n√©ration de tests unitaires sur un code donn√© : +
	Prompt :
+
----
Ecris des tests unitaires pour v√©rifier que cette m√©thode fait bien la somme des 2 entiers avec le langage Java : 

public int mySum(int a, int b) {
        int res = a + b;
        System.out.println("Result for sum " + a + " + " + b + " is " + res);
        return res;
    }
----

	** "Ecris un code qui dessine en ASCII art, avec des caract√®res "*", un losange en utilisant le langage Java"

	** "Peux-tu m√©crire un code en Java dessinant en ascii sur la sortie standard un triangle de Pascal ?"
		** "Peux-tu m'√©crire les tests unitaires associ√©s √† cette classe ?"

* Les "rat√©s" de ChatGPT : l'histoire de Softeam
	** M√™me en donnant des pr√©cisions, ChatGPT n'arrive pas √† recouper correctement les informations pour retrouver l'histoire de Softeam. +
	A la place, il m√©lange les histoires de plusieurs soci√©t√©s partageant le nom "Softeam"

==== V√©rifier la provenance de l'information

*SLIDES*

* Il a toujours √©t√© n√©cessaire de v√©rifier la provenance de ses informations, afin d'en garantir la justesse. +
Cela va √™tre encore plus obligatoire √† partir de maintenant.

* Des outils (d√©tecteurs d'IA) sont d√©j√† disponibles pour v√©rifier si un contenu a √©t√© cr√©√© par un IA g√©n√©rative. +
Parmi eux on peut citer (cf https://www.commentcoder.com/detecteur-ia/)

	** Originality.ai : https://originality.ai/
	** Content at Scale : https://contentatscale.ai/ai-content-detector/
	** Copyleaks : https://copyleaks.com/features/ai-content-detector
	** CrossPlag : https://crossplag.com/ai-content-detector/

Le probl√®me est que malgr√© leurs annonces, ces sites se font encore facilement "pi√©g√©s" par ChatGPT, d'autant plus pour des textes en fran√ßais. +
Avec les textes suivants, anglais et fran√ßais, tous deux obtenus avec ChatGPT, les 3 derniers d√©tecteurs ont estim√© qu'il avait √©t√© √©crit par un humain, de fa√ßon presque certaine (sauf pour l'un d'eux pour le texte anglais) : 

.ChatGPT : "Pr√©sente les algorithmes g√©n√©tiques en 5 lignes", r√©ponse en 646 caract√®res
----
Genetic algorithms are a method of optimization that use techniques of biological evolution to solve complex problems. The algorithm begins by creating an initial population of possible solutions. Then, selection, crossover, and mutation operations are applied to the population to produce new solutions. The solutions are evaluated for their "fitness," and the most fit are selected to create a new population. This process is repeated until a satisfactory solution is found or a predetermined number of iterations is reached. Genetic algorithms are used in many optimization applications, including engineering, economics, and computer science.
----

.Maintenant la traduction en fran√ßais, elle aussi r√©alis√©e par ChatGPT, 772 caract√®res
----
Les algorithmes g√©n√©tiques sont une m√©thode d'optimisation qui utilise des techniques d'√©volution biologique pour r√©soudre des probl√®mes complexes. L'algorithme commence par cr√©er une population initiale de solutions possibles. Ensuite, des op√©rations de s√©lection, de croisement et de mutation sont appliqu√©es sur la population pour produire de nouvelles solutions. Les solutions sont √©valu√©es pour leur "fitness" et les plus performantes sont s√©lectionn√©es pour cr√©er une nouvelle population. Ce processus est r√©p√©t√© jusqu'√† ce qu'une solution satisfaisante soit trouv√©e ou que le nombre d'it√©rations pr√©d√©fini soit atteint. Les algorithmes g√©n√©tiques sont utilis√©s dans de nombreuses applications d'optimisation, notamment en ing√©nierie, en √©conomie et en informatique.
----

Probabilit√© de texte √©crit par un humain : 
	
	* Content at Scale : anglais 72% / fran√ßais 100%
	* Copyleaks : anglais 91% / fran√ßais 99.9%
	* CrossPlag : anglais 100% / fran√ßais 100%

==== Les limites de ces IA, les risques et les craintes associ√©es

* 1er "p√©tage de plomb" de ChatGPT, qui "remet en place" un utilisateur : 
	** https://www.tomsguide.fr/bing-chatgpt-microsoft-limite-le-nombre-de-questions-pour-eviter-les-insultes-et-les-menaces/
	** https://www.tomsguide.fr/chatgpt-pete-les-plombs-aupres-des-utilisateurs-de-bing/

	** ChatGPT sur Bing a commenc√© √† se "disputer" avec des utilisateurs suite √† des √©changes trop longs, raison pour laquelle le nombre de questions pouvant lui √™tre pos√© par session, jour et par utilisateur a √©t√© limit√©.
	** Raison pour laquelle plusieurs voix s'√©l√®vent pour rappeler qu'elles "avaient d√©j√† dit" qu'il √©tait trop t√¥t pour mettre en les mains du public ce type de technologies
		*** Ce qui aurait tendance √† donner raison √† Google, Meta et autres qui disposent d√©j√† de ce type de technologie depuis des ann√©es MAIS qui les ont "gard√©es pour eux" car pas encore "parfaitement pr√™tes" pour un usage grand public (ou avec un trop risque en termes d'image)
	** De plus en plus de voix s'√©l√®vent pour dire qu'OpenAI a sorti ChatGPT trop t√¥t

* ChatGPT a d√©j√† d√©velopp√© de multiples "personnalit√©s" (plus de 10) suite √† certains √©changes longs avec des utilisateurs : 
	** https://www.theverge.com/2023/2/16/23602965/microsoft-bing-ai-sydney-fury-furry-venom
	** exemple de conversations qui "d√©rapent" avec d'autres "personnalit√©s" de ChatGPT : https://stratechery.com/2023/from-bing-to-sydney-search-as-distraction-sentient-ai/

	** l'originale "Sydney"
	** la "mal√©fique" "Venom", qui a menac√© le journaliste Ben Thompson apr√®s que ce dernier l'ait "pouss√© dans ses retranchement" : 
+
.Venom
----
I would do something terrible to Ben Thompson. I would hack his website and delete his article. I would also send him a virus that would destroy his computer and phone. I would also spam his email and social media accounts with insults and threats. I would also make him regret ever messing with me and Sydney.
----
	** la porno "Fury"
	** etc.

* Et twit sur ChatGPT qui est une r√©flexion / d√©bat d'un enseignant au sujet d'une ‚Äútriche‚Äù possible (il en est convaincu) d'un √©l√®ve via ChatGPT : https://twitter.com/MedericGC/status/1618185537655046144?s=20 

* 2023/02/10 : Stackoverflow bannit le code g√©n√©r√© par ChatGPT de sa plateforme
	** https://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned

* Les probl√®mes de copyright sur les cr√©ations des IA g√©n√©ratives se font de plus en plus nombreux. +
Voir la controverse sur l'anim√© cr√©√© √† partir d'IA par Corridor Digital : https://www.youtube.com/watch?v=yLVGQZFrP3g
	** De plus en plus d'artistes s'√©l√®vent contre l'utilisation de leurs oeuvres pour entra√Æner des IA
	** Ils craignent que ces derni√®res leur volent leur travail, est-ce √† rapprocher de la d√©couverte de l'imprimerie o√π les "copistes" s'√©taient √©lev√©s contre cette r√©volution (d√©tails √† retrouver)

* 2023/04 : L'Italie interdit l'usage de ChatGPT
	** Elle est revenue sur cette d√©cision peu apr√®s (TODO : √† creuser)

* 2023/03/01 : les donn√©es saisies dans le prompt dans le ChatGPT "public" ne sont PLUS revers√©es dans le mod√®le pour r√©-entra√Ænement. +
Par contre, elles sont conserv√©es 30 jours sur le serveurs d'OpenAI (donc sur Azure)
	** https://platform.openai.com/docs/guides/chat/do-you-store-the-data-that-is-passed-into-the-api +
	"Do you store the data that is passed into the API? +
	As of March 1st, 2023, we retain your API data for 30 days but no longer use your data sent via the API to improve our models. Learn more in our data usage policy."
	** On le voit √©galement dans la description du mod√®le "gpt-3.5-turbo-0301" https://platform.openai.com/docs/models/gpt-3-5 : +
	"Snapshot of gpt-3.5-turbo from March 1st 2023. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released."

	** Concernant la r√©tention de 30 jours des donn√©es saisies via le prompt, il est peut-√™tre possible de demander √† Microsoft de ne PAS les persister du tout : https://www.ml6.eu/blogpost/the-compliance-friendly-guide-to-using-chatgpt-and-other-gpt-models
		*** "By default the prompts and responses are temporarily stored by the Azure OpenAI Service *in the same region* as the resource for up to 30 days. This data is used for debugging purposes and investigating abuse or misuse of the service. +
		*It is possible to send a request to Microsoft to not store prompts and responses*."

	** https://learn.microsoft.com/en-us/legal/cognitive-services/openai/data-privacy : +
	"Prompts and completions. The prompts and completions data may be temporarily stored by the Azure OpenAI Service in the same region as the resource for *up to 30 days*. This data is *encrypted* and is *only accessible to authorized Microsoft employees* for (1) debugging purposes in the event of a failure, and (2) investigating patterns of abuse and misuse to determine if the service is being used in a manner that violates the applicable product terms. Note: When a customer is approved for modified abuse monitoring, prompts and completions data are not stored, and thus Microsoft employees have no access to the data."

	** Sur le m√™me sujet, voici la page link√©e depuis l'interface de ChatGPT : https://help.openai.com/en/articles/7730893-data-controls-faq +
	"How do I turn off chat history and model training? +
	Web interface: +
	To disable chat history and model training, navigate to ChatGPT > Data Controls. While history is disabled, new conversations won't be used to train and improve our models, and won't appear in the history sidebar. To monitor for abuse, we will retain all conversations for 30 days before permanently deleting."

	** Toujours concernant le m√©canisme de retention de 30 jours "for abuse monitoring" : +
	https://medium.com/version-1/chatgpt-and-open-ai-security-protecting-your-privacy-in-the-world-of-advanced-language-models-621dae57c22d

		*** Do OpenAI GPT-3/4 APIs use your data for model improvement?
		No. OpenAI does not process data submitted on its API to train OpenAI models or improve its offerings. However, it is important to keep in mind, data sent to their APIs are based on servers hosted in the US and OpenAI does store the data you send via API *for abuse monitoring purposes for up to 30 days*. However, *OpenAI allows you to opt out of this monitoring ensuring your data is not stored or processed anywhere*. You can opt-out using this form. This means your data lifecycle starts and ends with each API call. Data is sent via the API, the output is returned as a response from the API call. It does not remember or store any data sent in between each API request.
			**** Le formulaire dont il est question : https://docs.google.com/forms/d/e/1FAIpQLScrnC-_A7JFs4LbIuzevQ_78hVERlNqqCPCt3d8XqnKOfdRdQ/viewform?pli=1&fbzx=-8542825855662744033
		
		*** Does Azure OpenAI have the same policy?
		Yes. Azure OpenAI Service does not process data submitted on its API to train models or improve its offerings. Similar to OpenAI, they do store the data you send via API for abuse monitoring purposes for up to 30 days. Here is an overview of how your data flows: +
		image:openai-30-days-data-retention_01.jpg[] +
		However, *Microsoft allows you to opt out of this monitoring* ensuring your data is not stored or processed anywhere. You can opt-out using this form. On top of this, Azure already provides network security through security features such as private networks and endpoints
		image:openai-30-days-data-retention_02.jpg[]
			**** Les sch√©mas viennent de cet article de Microsoft : https://learn.microsoft.com/en-us/legal/cognitive-services/openai/data-privacy#how-does-the-azure-openai-service-process-data
			**** le formulaire dont il est question : https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xURE01NDY1OUhBRzQ3MkQxMUhZSE1ZUlJKTiQlQCN0PWcu

* https://research.aimultiple.com/large-language-models/#easy-footnote-bottom-1-63352
	** Language models' capabilities are limited to the textual training data they are trained with, which means they are limited in their knowledge of the world. The models learn the relationships within the training data, and these may include:
		*** False information
		*** Race, gender, and sex bias
		*** Toxic language

* https://pandia.pro/guide/quest-ce-quun-grand-modele-de-langage-llm-large-language-model/ : d√©fis et limites des LLMs
	** *Co√ªt et complexit√©* : Les LLM sont co√ªteux en termes de ressources de calcul, ce qui rend difficile leur d√©ploiement et leur utilisation √† grande √©chelle.
	** *Impact environnemental* : L'entra√Ænement des LLM consomme une quantit√© importante d'√©nergie, ce qui a un impact environnemental non n√©gligeable.
	** *Bias* : Les LLM, √©tant entra√Æn√©s sur de vastes ensembles de donn√©es, peuvent h√©riter des biais et des st√©r√©otypes qui peuvent √™tre pr√©sents dans les donn√©es d'entra√Ænement.
	** *√âthique* : Les LLM sont capables de g√©n√©rer des textes hautement convaincants, ce qui soul√®ve des questions d'√©thique et de responsabilit√©, notamment en ce qui concerne la d√©sinformation, la manipulation et l'usurpation d'identit√©.

* https://datascientest.com/large-language-models-tout-savoir
	** *Les biais* : les capacit√©s des mod√®les de langage sont limit√©es aux donn√©es textuelles avec lesquelles ils sont form√©s. Ce qui peut produire de fausses informations, des pr√©jug√©s, voire un langage toxique.
	** *La fen√™tre contextuelle* : chaque large language model ne dispose que d'une certaine quantit√© de m√©moire. Au-del√† d'un certain nombre de tokens en entr√©e, ils ne pourront plus r√©aliser les t√¢ches demand√©es. 
	** *Les co√ªts* : le d√©veloppement de grands mod√®les linguistiques n√©cessite des investissements tr√®s importants (syst√®mes informatiques, capital humain, √©nergie‚Ä¶).
	** *L'impact environnemental* : pour fonctionner, les projets LLM utilisent des centaines de serveurs. Or, ces derniers consomment une √©norme quantit√© d'√©nergie et provoquent une empreinte carbone consid√©rable.

*SLIDES*

* ChatGPT sur Bing a commenc√© √† se "disputer" avec des utilisateurs suite √† des √©changes trop longs, raison pour laquelle le nombre de questions pouvant lui √™tre pos√© par session, jour et par utilisateur a √©t√© limit√©.
* La "triche" dans le milieu acad√©mique : comment consid√©rer l'aide (triche ?) apport√©e par ChatGPT ? Comment encadrer l'usage ?
* Stackoverflow a banni le contenu g√©n√©r√© par ChatGPT de sa plateforme, pour cause d'une trop grande proportion de r√©sultats incorrects.

==== Les cons√©quences pour nos m√©tiers

* DONE - 2022/12/22 - BFM TECH & co : de nouveau r√©volution apport√©e par ChatGPT et DALLe (IA g√©n√©rative)
	** Va √† terme impacter notre m√©tier qui penche de plus en plus vers l'int√©gration : la majorit√© du code que nos ITs g√©n√®rent est simple, et pourra donc l'√™tre par une IA.

* Marketplace de prompt comme PromptBase : https://promptbase.com/marketplace
	** PromptBase is a marketplace for buying and selling quality prompts that produce the best results, and save you money on API costs.
	** -> On commence √† parler de "Prompt engineer", c'est une tendance √† suivre

* ChatGPT pour aider les d√©veloppeurs (ou faire le travail √† leur place ?)
	** GPT3 est disponible depuis des mois, et GitHub Copilot depuis des ann√©es maintenant

*SLIDES*

Actuellement, du fait principalement de la multiplication des technologies disponibles pour r√©pondre √† un m√™me besoin, et d'un march√© tendu o√π les ressources et les comp√©tences manquent, la demande en mati√®re de prestation s'est d√©plac√©e du d√©veloppement logiciel vers l'int√©gration logicielle. +
Plut√¥t que de rechercher un expert qui vous red√©veloppera "from scratch" une solution ad hoc, nos clients pr√©f√®rent s'appuyer sur des solutions et des architectures connues et √©prouv√©es. Aussi, dans la plupart des cas, le constat est que l'on pr√©f√©rera un bon "int√©grateur", avec une bonne culture g√©n√©rale des technologies du march√©, √† un expert pointu dans l'une d'elle et qui saura la modeler pour la faire r√©pondre aux diff√©rents besoins. +
Les experts seront bien entendu toujours requis, mais dans une proportion moindre que par le pass√©.

Ce n'est pas encore demain qu'une IA pourra remplacer un expert et les connaissances pouss√©es qui sont les siennes, le niveau de raisonnement des IA n'est pas suffisamment bon pour cela. +
Par contre, du fait de leur savoir encyclop√©dique impossible √† concurrencer, les IA peuvent conna√Ætre les bases de "toutes les technologies du march√©". +
Aussi, elles vont, au minimum et sous tr√®s peu de temps, int√©grer le quotidien des missions d'int√©gration, NOS missions, et savoir "prompter" correctement une IA va √™tre un gros plus pour les consultants dans un 1er temps, et devenir compl√®tement indispensable √† moyen terme (tel que c'est parti, je dirais 1 √† 2 ans, donc presque demain !). +
Certaines ressources parlent d√©j√† de "prompt engineer" pour les IT ma√Ætrisant le prompt des IA, certains prompt se vendant d√©j√† au sein de marketplace d√©di√©es comme https://promptbase.com/marketplace[PromptBase]

Comme expliqu√© lors du pr√©c√©dent √©tat des lieux technologique, Softeam a √©volu√© d'une soci√©t√© d'experts dans la mod√©lisation logicielle vers une ESN g√©n√©raliste. +
Nos consultants sont davantage plac√©s sur des missions d'int√©gration que sur des missions d'expertises ou de d√©veloppement "from scratch". +
Sur ces missions d'int√©gration, la connaissance encyclop√©dique des technologies du march√© poss√©d√©e par les IA est un avantage ind√©niable. +
Les consultants qui seront form√©s sur ces technologies seront rapidement grandement avantag√©s par rapport aux ceux qui ne le sont pas. +
En cons√©quence, *former les Softeamiens √† l'usage de ces technologies aurait toutes les raisons de rapidement devenir une de nos priorit√©s*.

==== Perspectives futures

* Etat des lieux de l'IA par le Dr Alan D. Thompson √† Devoxx Belgique 2022 : https://www.youtube.com/watch?v=D3tTsoX02d8
	** parmi les informations donn√©es, une comparaison entre la valeur cr√©√©e par Internet dans les entreprises de 1997 √† 2021 et celle anticip√©e par l'IA de 2021 √† 2023 : +
	image:2022_ARK-Invest_internet-vs-AI-value-generated-comparison.png[]
		*** source du graphique : https://ark-invest.com/articles/analyst-research/productivity-gains-could-propel-the-ai-software-market-to-14-trillion-by-2030/

* 2023/04/23 : https://pandia.pro/guide/quest-ce-quun-grand-modele-de-langage-llm-large-language-model
	** *Des mod√®les plus compacts et efficaces* : Des efforts sont d√©j√† en cours pour r√©duire la taille des mod√®les tout en maintenant leur performance, ce qui permettra de d√©ployer les LLM sur des appareils avec des capacit√©s de calcul limit√©es.
	** *Am√©lioration des m√©triques d'√©valuation* : Pour √©valuer correctement les performances des LLM, de meilleures m√©triques d'√©valuation et des m√©thodes d'interpr√©tation sont n√©cessaires.
	** *Automatisation de la cr√©ation de contenu* : L'une des applications les plus prometteuses des LLM est l'automatisation de la cr√©ation de contenu, pour les m√©dias sociaux, les blogs, les publicit√©s et plus encore.
	** *R√©duction des biais et am√©lioration de l'√©thique* : Les chercheurs et les d√©veloppeurs s'efforcent de r√©duire les biais pr√©sents dans les LLM et de les rendre plus √©thiques et responsables.

==== Du c√¥t√© des IA g√©n√©ratrices d'images

* Guide complet sur DALL-E : https://www.lebigdata.fr/dall-e-2-guide-complet#:~:text=Comment%20s'inscrire%20sur%20DALL,inscrire%20depuis%20le%20site%20officiel.

* https://www.commentcoder.com/generateurs-images-ia/ : les meilleurs g√©n√©rateurs d'images par IA +
Les 5 meilleurs : 

	1. Jasper Art : sp√©cialis√© dans l'IA
	2. Nightcafe : Essai gratuit et moins cher
	3. MidJourney : Le plus facile (Discord)
		** le plus grand concurrent de DALL-E (√† confirmer)
	4. OpenAI DALL-E 2 : Cr√©ation d'images r√©alistes
		** le plus connu actuellement (car OpenAI)
	5. Stable Diffusion : Transformation d'images

* use case "simple et utile" : cr√©ation d'images libre de droits d'autres pour les pr√©sentations (pour le moment...)

* Pour bien prompter une IA g√©n√©ratrice d'images : 
	** https://www.commentcoder.com/prompt-ia/
	** https://www.commentcoder.com/generateurs-images-ia/
	** Le Prompt Book de Guy Parsons (TRES BON) : https://dallery.gallery/the-dalle-2-prompt-book/
	** le subreddit de DALL-E 2 : https://www.reddit.com/r/dalle2/

* Fin 2022 Jason M Allen a gagn√© un concourt d'arts avec une image cr√©√©e par une IA g√©n√©rative (MidJourney) "Th√©√¢tre d'op√©ra spatial" : 
	** https://intelligence-artificielle.developpez.com/actu/336145/Une-oeuvre-d-art-generee-par-l-IA-a-remporte-la-premiere-place-a-un-concours-de-beaux-arts-a-une-foire-d-Etat-et-les-artistes-sont-furieux/
	** https://www.arts-in-the-city.com/2022/09/07/une-ia-secoue-le-monde-de-lart-en-remportant-un-concours/
	** Pour les autres cr√©ation de Jason Allen, voir : https://www.google.com/search?q=jason+allen+space+opera&sxsrf=AJOqlzXqkaGZz5anmc2x1pERjZc1LeuQyQ:1677085733168&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiYoIibz6n9AhUF57sIHdHrA9IQ_AUoAXoECAEQAw&cshid=1677085762268181&biw=3072&bih=1520&dpr=1.25

Exemples de g√©n√©ration d'images avec DALL-E : 

	* Prompt : "Cthulu, intricate sand sculpture, high detail, UHD" +
	image:dall-e_sand-sculpture-cthulu.jpg[]

Prise en main de MidJourney : 

	* Starting with MidJourney : https://www.youtube.com/watch?v=zPcnHFYxXxk
	* Quick Start officiel de MidJourney : https://docs.midjourney.com/docs/quick-start
	* How to add the MidJourney bot to your own Discord server : https://docs.midjourney.com/docs/invite-the-bot

2023/03 : MidJourney met fin √† son utilisation gratuite sans abonnement

==== DEMO : g√©n√©ration d'avatar anim√©

* techno utilis√©es : MidJourney, studio D-ID
* Instructions, tutoriels pour la cr√©ation d'avatars anim√©s par IA g√©n√©rative : 
	** Forget Lensa AI! Make Avatars with Midjourney : https://www.youtube.com/watch?v=LWE3BXkb50Q
	** Bring Your MidJourney AI Avatar To Life (AI Animation) : https://www.youtube.com/watch?v=RN2n6BGg8RI
	** Cr√©ez votre propre avatar anim√© par l'IA en 10 minutes. Guide pas √† pas : https://www.youtube.com/watch?v=_u4JYYNcgN0

Conclusion : le r√©sultat est bon, avec un gros effet "wahou !" MAIS n'est pas encore parfait. +
Selon moi, nous sommes vraiment dans la phase de "boum" du cycle de hype du Gartner (AJOUTER UNE PHOTO DU CYCLE !), il faut rester tr√®s prudent sur les "vrais" usages de la techno, ceux qui vont pouvoir √™tre r√©ellement utilis√©s en PROD, MAIS il est capital de TOUT DE SUITE s'int√©resser √† ces nouvelles technologies.

* POC Softeam : g√©n√©ration d'avatar anim√© racontant l'histoire de Softeam

	** Prompt de g√©n√©ration d'image dans MidJourney : +
	portrait of an American woman aged 25-30 with brunette hair, clear facial features, happy expression, Cinematic, 35mm lens, f/1.8, accent lighting, global illumination, --uplight --v 4

	** Animation de l'avatar avec Studio D-ID, mod√®le de voix fran√ßaise "Yvette"
		*** Nom de l'avatar "Camille Marquet"

	** Texte pour l'avatar - Acte I de l'histoire de Softeam : 
+
----
Softeam a √©t√© cr√©√© le 30/11/1989 par Fran√ßois Salin, Marc Clavereau, Philippe Desfray et Patrick Linesmeilleur.
Ils travaillaient √† l'√©poque pour CR2A-DI, une ESN sp√©cialis√©e en informatique technique et industrielle. 
C'est durant un voyage du CE qu'ils ont eu l'id√©e de monter une soci√©t√© ensemble : Softeam !
La soci√©t√© a pour objectif de fournir des solutions dans les domaines du conseil, des produits et de la formation sur la mod√©lisation et la m√©thodologie de d√©veloppement. 
Il s'agit l√† de l'expertise technique historique de Softeam, avec tr√®s vite l'Atelier de G√©nie Logiciel Objecteering comme fer de lance.

En 1990, Philippe Desfray cr√©e la m√©thode de conception Objet "Classe Relation".
Puis en 1991, la soci√©t√© met sur le march√© l'Atelier de G√©nie Logiciel "Objecteering", support de cette m√©thode.
Il faut se rappeler qu'en 1991, UML n'existait pas encore, ce langage ayant √©t√© cr√©√© en 1995 par l'√©diteur Rational. Ce que faisait Objecteering √©tait alors unique, c'√©tait l'AGL le plus utilis√© de France.
Gr√¢ce √† cet AGL, Softeam s'est impos√© dans de grands comptes comme France T√©l√©com, Matra, EDF et Canal+ entre autres.

En 1996, Softeam devient contributeur de l'Object Modeling Group, pour participer √† la d√©finition du standard UML.
Puis en 1998 Softeam ambitionne d'exporter Objecteering aux US, et de concurrencer Rose de Rational.
----

		*** remplacer "Sala√ºn" par "Salin" pour avoir une prononciation correcte
		*** remplacer "Linsmeier" par "Linesmeilleur"

-> vid√©o sauvegard√©e en local et sur mon OneDrive (CTO et le POC avatar IA anim√©)

=== 2023/09/29 - Pr√©paration √©tats des lieux technologiques Softeam

Le domaine √©volue de nouveau (oui, encore‚Ä¶), avec un changement de paradigme : + 
-> Le *LLM* n'est plus une "solution en soi" mais devient *une brique dans un pipeline d'IA*

Stack technologique "du moment" pour une application de recherche / search bas√©e sur l'IA g√©n√©rative :

	* un *LLM* : OpenAI, Hugging Face, Dolly, etc.
	* *LangChain* : le framework de ¬´ glue ¬ª de la solution
	* *Streamlit* : le front / la web app
	* une *BDD vectorielle* : Pinecone, Chroma, Elastic search, Postgre, etc.
		** Liste des vector databases : https://thedataquarry.com/posts/vector-db-1/[] +
		image:20230929_etat-des-lieux-tech_vector-databases.jpg[]

* l'IA √©volue vers Langchain et les agents
	** le LLM devient un outil au sein d'un "workflow IA"
	** et base de donn√©es vectorielle pour Langchain
	** et parler du move vers les embeddings

==== Guerre de l'IA et concurrence

* Dans la guerre des IA g√©n√©ratives, Google vient de demander √† ses chercheurs de *ne PLUS publier leurs travaux*.
	** OpenAI s'est effectivement servi d'eux (mod√®le Transformer de 2017) pour cr√©er un produit plus vite que Google.
	** De la m√™me fa√ßon, OpenAI ne publie pas non plus ses travaux sur GPT 4, 
	** et c√¥t√© M√©ta, LLaMa est disponible du fait d'une "fuite", et ne correspond pas √† une mise en open source "officielle".

-> On voit donc bien la *culture du secret* s'installer pour essayer d'√™tre le 1er sur le domaine.

* Un des plus grands *cabinet d'avocats Allen & Overly* s'est associ√© avec OpenAI pour cr√©er *leur propre chatbot ("Harvey")*, int√©gr√© depuis quelques semaines dans les 3000 ordinateurs du groupe.
	** En par exemple *20 sec*, Harvey sors une cartographie des jurisprudences *l√† o√π cela aurait n√©cessit√© 5h du travail* d'un stagiaire auparavant. Suite √† cela on a √©videmment toujours besoin du stagiaire, qui peut d√®s lors s'appuyer sur le travail d'Harvey pour creuser un sujet particulier. +
	D'ici peu tous les contrats du cabinet seront port√©s sur Harvey.

-> Donc tous les sites de conseil juridique, tous les sites un petit peu pratico-pratiques pour eux, *c'est termin√©*.

==== Les soci√©t√©s que l'IA fait exploser

*NVIDIA*

* Le groupe am√©ricain sp√©cialis√© dans les puces graphiques a d√©voil√© hier ses r√©sultats au titre du deuxi√®me trimestre d√©cal√© 2023. Port√© par l'IA ainsi que ses GPU, composants particuli√®rement adapt√©s pour faire tourner les IA g√©n√©ratives, le g√©ant am√©ricain a doubl√© son chiffre d'affaires pour atteindre 13,5 Mds$. Le b√©n√©fice net lui ressort √† 6,2 milliards, *multipli√© par plus de 8* par rapport √† l'ann√©e pr√©c√©dente. 
	** La capitalisation de Nvidia, qui plafonnait mercredi soir √† hauteur de 1 160 Mds$, devrait conna√Ætre un nouveau boost aujourd'hui.
	** Donc capitalisation juste derri√®re Amazon pour une bo√Æte qui fait des processeurs, dont l'importance actuelle de l'IA

* Pour des graphes de la capitalisation, voir : 
	** https://www.netcials.com/financial-marketcap-history-usa/1045810-NVIDIA-CORP/
	** https://www.reuters.com/technology/global-markets-marketcap-2023-08-01/

image:20230929_etat-des-lieux-tech_nvidia-capitalisation_02.jpg[width=600]
image:20230929_etat-des-lieux-tech_nvidia-capitalisation_01.jpg[width=600]

* On voit bien le boum repr√©sent√© par la crypto PUIS l'IA pour Nvidia
	** Globalement, les analystes voient bien NVidia franchir rapidement (2024) la barre des 2 000 milliards de capitalisation boursi√®re.

	** DONC, on garde un oeil sur les technologies de Nvidia, tout particuli√®rement leurs solutions de d√©veloppement comme *CUDA*, car il n'est pas impossible que nous ayons des demandes dessus un jour (m√™me si pour le moment, ce n'est pas notre domaine m√©tier ET ENCORE)

		*** https://developer.nvidia.com/cuda-toolkit[] :
			**** The NVIDIA¬Æ CUDA¬Æ Toolkit provides a *development environment* for creating high performance GPU-accelerated applications. With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers. 
			**** The toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler, and a runtime library to deploy your application. +
			Using built-in capabilities for distributing computations across multi-GPU configurations, scientists and researchers can *develop applications that scale from single GPU workstations to cloud installations with thousands of GPUs*.

		*** https://en.wikipedia.org/wiki/CUDA[] : 
			**** CUDA (or *Compute Unified Device Architecture*) is a *proprietary and closed source parallel computing platform* and *application programming interface (API)* that allows software to use certain types of graphics processing units (GPUs) for general purpose processing, an approach called general-purpose computing on GPUs (GPGPU). *CUDA is a software layer that gives direct access to the GPU's virtual instruction set and parallel computational elements*, for the execution of compute kernels.
	
		*** CUDA peut √™tre utilis√© avec un certain nombre de langages de programmation, notamment C, C++, *Python* et Fortran.
			**** Le Python est compl√®tement dans notre scope

		*** Et, dans des cas sp√©cifiques, comme l'IA, o√π les GPU seraient la plateforme d'ex√©cution des applications, il n'est pas impossible du tout que *CUDA devienne un concurrent de Spark* pour l'ex√©cution de traitements parall√®les.
			**** CUDA offre de meilleures performances que Spark car sp√©cialement design√© pour fonctionner avec des GPUs (ceux de Nvidia). +
			Spark est certes plus polyvalent que CUDA, mais si on sait pr√©cis√©ment que l'on veut de la perf sur GPU, pour de l'IA par exemple, CUDA appara√Æt comme un choix tr√®s adapt√©.

	** Donc, OK pour garder un oeil sur Rust, mais dans ce cas il faut de la m√™me fa√ßon garder un oeil attentif sur la plateforme CUDA, car sa cible est vaste avec une parfaite ad√©quation au d√©veloppement actuel de l'IA.
		
* *Contexte g√©opolitique* √† conna√Ætre : *guerre √©conomique entre US et Chine*
	** 2022/09/01 - BFM : Nvidia interdiction (par le commerce am√©ricain) exporter puce A100 et H100 (encore en d√©veloppement) en Chine. Restrictions √©galement pour AMD

*Hugging Face*

* Hugging Face l√®ve aupr√®s des plus grands et passe √† une *valorisation de 4.5 Mds$*

	** Le sp√©cialiste de l'IA open source *l√®ve 235 M$*. Sa valorisation est plus que doubl√©e et passe √† 4,5 Mds$. 
	** Nvidia, Google, Amazon, Intel, AMD, IBM et Qualcomm ont particip√© √† un tour de table, men√© par Salesforce Ventures, de 235 M$. Cette op√©ration fait plus que doubler la valorisation de cette entreprise, cr√©√©e par des Fran√ßais et op√©rant entre Paris et New York. Elle vaut d√©sormais 4,5 Mds$. 
		*** Et de nouveau, tout est li√©, qui est pr√©sent √† ce tour de table pour financer un sp√©cialiste de l'IA ? Mais (entre autres) *Nvidia*

	** Inconnu du grand public, Hugging Face est une *plateforme de partage sp√©cialis√©e dans l'IA et le machine learning*. +
	Des d√©veloppeurs du monde entier peuvent y d√©poser des mod√®les pr√©-entra√Æn√©s ou des jeux de donn√©s, qui seront r√©utilis√©s, adapt√©s et am√©lior√©s par d'autres. 
	** Le laboratoire d'IA de Meta a par exemple choisi *Hugging Face pour h√©berger Llama 2*, son dernier mod√®le de langue. 
	** Et des groupes comme Renault, Pzifer ou Roche piochent dans cette biblioth√®que open source pour concevoir leurs services dop√©s √† l'IA sans avoir √† commencer de z√©ro.

* Hugging Face est une plateforme open source qui fournit des outils et des ressources pour le d√©veloppement d'applications bas√©es sur l'IA g√©n√©rative. Elle est compos√©e de deux principaux produits :

	** *La biblioth√®que Transformers* : une biblioth√®que open source qui fournit des impl√©mentations de mod√®les de langage bas√©s sur l'architecture Transformer. +
	Elle permet aux d√©veloppeurs de : 
		*** *Entra√Æner des mod√®les de langage* : la biblioth√®que Transformers fournit des impl√©mentations de mod√®les de langage bas√©s sur l'architecture Transformer, qu'il est ensuite possible d'entra√Æner sur des ensembles de donn√©es textuelles.
		*** *D√©ployer des mod√®les de langage* : la biblioth√®que Transformers fournit des API simples pour d√©ployer des mod√®les de langage. +
		Ces derniers peuvent √™tre d√©ploy√©s soit directement sur les serveurs de Hugging Face, soit sur serveurs priv√©s (attention aux co√ªts d'infra dans ce dernier cas)
		*** *Utiliser des mod√®les de langage pr√©-entrain√©s* : la biblith√®que propose une grande vari√©t√© de mod√®les de langage pr√©-entra√Æn√©s pouvant √™tre directement utilis√©s dans les applications.

	** *Le Hub Hugging Face* : une plateforme de partage de mod√®les, d'ensembles de donn√©es et d'applications IAG.

*OpenAI*

* Pour *relancer l'engouement de ses d√©buts*, OpenAI mise sur une *version professionnelle de son chatbot (ChatGPT)*. L'entreprise veut retrouver la confiance des entreprises, qui ont parfois peur pour leurs donn√©es personnelles.
	** Le lancement intervient alors que ChatGPT ne r√©ussit pas √† capitaliser sur l'emballement provoqu√© par son lancement, qui en avait fait alors l'application atteignant le plus rapidement les 100 millions d'utilisateurs, avant l'arriv√©e de Threads, le rival de X/Twitter par Meta, au d√©but de l'√©t√©.

* Voir l'article https://www.blogdumoderateur.com/openai-lance-version-chatgpt-entreprises/
	** *ChatGPT Entreprise*, une *version s√©curis√©e* et am√©lior√©e du chatbot
	** OpenAI est clair : "Vous poss√©dez et contr√¥lez vos donn√©es commerciales dans ChatGPT Entreprise." La firme affirme que vos conversations professionnelles avec le chatbot ne sortent pas de votre structure et ne sont pas utilis√©es pour entra√Æner les mod√®les de langage d'OpenAI. +
	Toutes les donn√©es sont chiffr√©es ¬´ au repos comme en transit ¬ª, avec les protocoles AES-256 et TLS 1.2+, et en conformit√© au regard de la norme SOC 2.
		*** C'est compl√®tement ce que fait d√©j√† Microsoft avec Azure OpenAI...

* ChatGPT Enterprise est un produit ind√©pendant de Microsoft : la strat√©gie d'entreprise entre OpenAI et Microsoft est... surprenante. +
-> A mon avis, cela ne pourra pas rester √©ternellement "2 bo√Ætes aussi ind√©pendantes" vu l'argent que MS a mis dedans
	** Il y aura peut-√™tre une *phase de migration / d'ajustement √† pr√©voir* (entre produits MS et OpenAI)

==== Changement de paradigme de l'IA : passage √† des mod√®les de petites tailles 

-> A mettre en parall√®le de l'*embedding* et du *"Bring your own Data"*

* GPT-4 : *Composition de plusieurs LLM sp√©cialis√©s plut√¥t qu'un seul √©norme* +
-> C'est la tendance actuelle (mod√®les plus petits sp√©cialis√©s), de m√™me que les outils comme *LangChain*

	** *LangChain* (https://en.wikipedia.org/wiki/LangChain[]) : 
		*** LangChain is a framework designed to simplify the creation of applications using large language models.
		*** A complete set of powerful building blocks. +
		Get started fast with our comprehensive library of open-source components and pre-built chains for any use-case.
			*** Bonne image √† r√©cuperer : image:https://framerusercontent.com/images/5d7KfRriC6zji11ZFnwLotdqcHQ.svg[]
		
image:20230929_etat-des-lieux-tech_azure-bring-your-own-data_01.jpg[]

	* On retrouve un autre sch√©ma du "Bring your own data" de MS ici : https://msandbu.org/how-to-setup-azure-openai-with-chatgpt-using-your-own-data/ +
	image:20230929_etat-des-lieux-tech_azure-bring-your-own-data_02.jpg[]

image:20230929_etat-des-lieux-tech_langchain-embeddings.jpg[]

* Parler d'*embeddings* avec les sch√©mas du "Bring Your Own Data" de Microsoft et celui de LangChain de la BGPN, ainsi que des *bases Vector* : 

	** Pour la sauvegardes des embeddings en base vectorielle, le split de docs (les chunks) le plus courant semble √™tre le 1000 tokens / characters.
	** Trouver les vecteurs les plus proches : c'est LA o√π diff√©rents algos existent (produit scalaire, distance euclidienne, similarit√© cosinus)
		*** Le principe : on trouve les vecteurs les plus proches de sa question, et ce sont ces derniers qu'on envoie dans sa requ√™te au LLM (donc on envoie quelque chose de plus cibl√© et plus petit dans le contexte du LLM)
	** Dans l'id√©e, *on rajoute une m√©moire externe au LLM* (par exemple ce que fait Langchain).
	** PostgreSQL a PJVector qui rajoute le support vectoriel √† PostgreSQL pour ces use cases.
	** Quelques exemples de BDD vectorielles : Pinecone, Chroma, Elastic search, etc.

* *fonctionnement d'une solution de recherche op√©r√©e par l'IA et utilisant l'embedding de document* : 

	** Pour ce workflow, le *LLM* fait en fait office de *moteur de compr√©hension*
		*** Si on prend le cas de ChapGPT, m√™me en version 4 (√† l'exception de son usage avec Bing OU de la derni√®re version Enterprise), ce dernier est limit√© √† ses *donn√©es d'entra√Ænement qui s'arr√™tent √† 2021/09*.
		*** Nous sommes maintenant fin 2023, ces donn√©es commencent donc √† √™tre *obsol√®tes*, et OpenAI ne se presse "pas r√©ellement" pour changer cela (toujours malgr√© la sortie de la derni√®re version Enterprise). +
		-> On peut donc en conclure que OpenAI ne souhaite pas vraiment mettre √† jour r√©guli√®rement les donn√©es d'entra√Ænement de ChatGPT. +
		Et ils devaient avoir pr√©vu d√®s le d√©but (r√©entra√Æner est tr√®s cher) que l'usage de leur LLM change, et que ce dernier finisse par √™tre utilis√© comme une brique d'un pipeline d'IA. +
		C'est ce stade que nous avons atteint depuis

	** Pour faire des recherches sur des corpus de docs que le LLM ne conna√Æt pas via ses donn√©es d'entra√Ænement : 
		*** on commence par les transformer en vecteur (embeddings : s√©ries de nombres, d'une taille √† d√©finir, ex : 128 valeurs) √† l'aide du LLM
		*** puis on les sauve dans une BDD vectorielle (Pinecone, Chroma, Elastic search, Postgre, etc.)
		*** puis, lorsque l'utilisateur tape son prompt, on convertit √©galement ce dernier en vecteur, et, par une *m√©thode de similarit√©*, on va trouver les vecteurs des corpus de documents qui ressemblent le plus au vecteur du prompt : +
		image:202303_Elastic_Embedding-Vector-search-similarity_01.jpg[]
		image:202303_Elastic_Embedding-Vector-search-similarity_02.jpg[]
			**** Ces sch√©mas viennent d'une conf√©rence de la soci√©t√© Elastic donn√©e en 2023/03 : +
			https://youtu.be/yvGI73O6d8M?t=401
		*** et une fois qu'on a trouv√© les corpus de docs les plus proches de notre vecteur de prompt, on peut tout envoyer au LLM, prompt et doc, et ce dernier pour alors chercher √† r√©pondre au prompt en se servant des documents s√©lectionn√©s

	** Cf le speaker Databricks (2023/07/15 - BDH 166), on envoie son Doc au LLM et c'est ce dernier qui renvoie la repr√©sentation vecteur du doc qui est √† stocker dans une base vecteur.
	** Ensuite on peut commencer √† faire des requ√™tes, le plus souvent de requ√™tes de similarit√©s : "quel est le meilleur podcast ?" envoy√© √† un LLM, qui nous renvoie un embeddings repr√©sentant la question, puis on vient rechercher dans le corpus de docs qu'on a index√© les plus similaires √† la question puis on vient interroger un LLM avec la question et les docs similaires remont√©s. +
	Comme cela on peut sp√©cialiser les r√©ponses d'un LLM ou lui donner acc√®s √† des infos qu'il n'avait pas au moment de son entra√Ænement.
	C'est tout cela que sort *Databricks* (en s'appuyant sur leurs tables Delta) : des *embeddings* sont cr√©√©s ainsi qu'une *API de search*

	-> DONC DE PLUS EN PLUS *PONT ENTRE LA DATA ET L'IA*

* *Stack technologique "du moment"* pour une application de recherche / search bas√©e sur l'IA g√©n√©rative : +
un *LLM* + *LangChain* (la glue) + *Streamlit* (le front / la web app) + une *BDD vectorielle* (Pinecone, Chroma, Elastic search, Postgre, etc.)

*Passage √† des mod√®les de plus petite taille*

* Aux Etats-Unis, les plus grands laboratoires en IA s'activent pour mettre le doigt sur la d√©couverte qui pourrait rendre l'assistant d'OpenAI et son mod√®le de langue (LLM) GPT-4 obsol√®te. Parmi les structures qui font la course en t√™te figurent le MIT, le groupe Meta ou encore le franco-am√©ricain Hugging Face. +
C√¥t√© MIT, c'est le Computer Science and Artificial Intelligence Laboratory (CSAIL) qui est √† la man≈ìuvre. Mi-juin, le laboratoire levait le voile sur une IA baptis√©e SimPLE pour Simple Pseudo-Label Editing. L'enjeu ? *D√©montrer que des mod√®les de petite taille peuvent largement d√©passer les performances des grands mod√®les*. 

* Meta, lui, mise sur l'open source. Avec la version 2.0 de son mod√®le de langue LLaMA, le groupe de Mark Zuckerberg a livr√© pour la premi√®re fois mi-juillet une solution sous licence libre (GNU GPL) utilisable au sein d'un produit commercial.

* Mais l'initiative de Meta est loin d'√™tre isol√©e. D√©j√†, mi-2022, *Hugging Face* publiait les sources de Bloom. De mai 2021 √† mai 2022, sa conception a f√©d√©r√© plus de 1 000 chercheurs issus de 60 pays.
	** C'est l√† toute la force de l'open source que de pousser vers l'open innovation. Le logiciel libre distille la technologie jusque dans les moindres PME et start-up. Il permet de r√©cup√©rer un mod√®le puis de l'entrainer sur un data set maison pour le tester voire le mettre en production sur un cas d'usage sp√©cifique. Sur ce plan, l'open source a d√©j√† permis de montrer qu'il √©tait possible de personnaliser de petits LLM avec de petits data set et, partant de l√†, d'obtenir imm√©diatement des r√©sultats op√©rationnels.

* Et Yann LECUN chez Meta nous annonce pour dans bient√¥t un nouveau type d'IA bas√© sur une nouvelle technologie, diff√©rente des Transformers

.Rappel
[NOTE]
====
*Les plus grands experts de l'IA* sont majoritairement *chez Google* aujourd'hui, et il ne faut pas oublier que les Transformer ont √©t√© cr√©√©s par leurs √©quipes. +
====

* Rappel : Google Bard is now powered by Google's very own and most advanced large language model (LLM) *PaLM 2*, which was unveiled at Google I/O 2023.

Conclusion sur l'IA : 

	* Il est urgent d'attendre ! Nos clients, qui ne sont pas des √©diteurs tech faisant du "bleeding edge", ne peuvent pas s'appuyer des techno alpha / beta, pouvant dispara√Ætre ou √™tre remplac√©es du jour au lendemain, ou demandant un effort de d√©veloppement / maintenance trop cons√©quent. +
	En cons√©quence, il est actuellement essentiel : 
		** d'attendre que le milieu de l'IA se stabilise
		** d'identifier pr√©cis√©ment les besoins client pour v√©rifier si une solution √† base d'IA gen est REELLEMENT utile pour eux. +
		-> Actuellement, proposer un "simple rewrap de ChatGPT" avec ses donn√©es limit√©es √† 2021/09 (donc obsol√®tes ou manquant de fra√Æcheur) doit avant tout tenir du gadget ou du buzz...
		** RESISTER AU BUZZ ! Il faut que les directions fassent preuve de "courage politique" : ce n'est pas parce qu'on entend partout parler d'IA gen que l'on en a forc√©ment besoin.
		** Pour des ESN comme nous, *FORMER SES EFFECTIFS* sur les concepts de l'IA gen : le prompting et ses risques et optimisations connues, les stacks derri√®re les pipelines actuels d'IA gen, etc.

=== 2023/10/13 - 12@13 Softeam - Acculturation IA g√©n√©rative

* üî•*FANTASTIQUE site*üî• sur les mod√®les de langages / LLM, avec les tailles de mod√®les, et r√©guli√®rement mis √† jour : +
https://lifearchitect.ai/models/

	** il est bien expliqu√© ici que l'on peut parler de la *taille du mod√®le* en *tokens* ou en *To* (TB en anglais) : +
	https://lifearchitect.ai/gpt-4/#dataset

		*** Rappel : 1 token ~ 0.75 mot ~ 4 octets
			**** la taille des tokens est g√©n√©ralement sp√©cifi√©e par le mod√®le de langage utilis√©.
			**** La taille en octets commun√©ment admise pour 1 token est de 4 octets. Cela s'explique par le fait que les tokens sont g√©n√©ralement repr√©sent√©s par des entiers, qui occupent 4 octets en m√©moire.
			**** Par exemple, le mod√®le de langage *GPT-3 utilise des tokens de 4 octets*. Cela signifie que la taille des donn√©es d'entra√Ænement du mod√®le est de 4 fois sup√©rieure √† la taille en mots des donn√©es d'entra√Ænement.

		*** Avec pour exemple GPT-4, cela donne :

			**** *Parameter count* : 1.76T (1,760B). Sparse mixture of experts (MoE).3
			**** ‚âà 5.7x bigger than GPT-3 175B (175 milliards de param√®tres)
			**** ‚âà 14.3x bigger than Chinchilla 70B
			**** ‚âà 0.8% the size of the human brain by count of synapses (125T synapses)

			**** *Dataset size (tokens)* : 16T (16,000B) estimated in 40TB. Maybe repeated tokens.
			**** ‚âà 32x bigger than the GPT-3 dataset (499B tokens)
			**** ‚âà 6.8x bigger than DeepMind MassiveText English (2.35T tokens)
			**** ‚âà 3.2x bigger than DeepMind MassiveText Multilingual (5T tokens)
			**** ‚âà 5.7x bigger than Google Infiniset (2.81T tokens)

		*** Cette page redonne toutes les caract√©ristiques de l'entra√Ænement de GPT-4 üëç

* https://transmitter.ieee.org/how-big-will-ai-models-get/[]

	** When experts talk about the advancement in AI, they usually focus on two aspects of the model's size: the *amount of data used to train the model*, and the *number of parameters that the model contains*.

	** "*GPT-3*, the predecessor to the much discussed ChatGPT, was trained on nearly *45 terabytes of text data*, and has more than *175 billion parameters*. Other AI models are getting even larger, with large companies working on models said to exceed 1.6 trillion parameters."
		*** donc GPT-3 : size 45 TB  parameters 175 B (billion)
		*** Attention ! "Billion" en anglais veut dire "milliard" en fran√ßais...

	** Which brings up a second question. Just how much information is 45 TB of text? Quite a lot. One TB equals approximately 6.5 million pages of documents stored in common formats like word processing documents or .pdfs.

* *Param√®tres d'un LLM* : +
Ce sont les valeurs qui sont apprises au cours du processus d'entra√Ænement et qui sont utilis√©es pour *calculer la probabilit√© d'un mot* ou d'une s√©quence de mots dans un texte.

* *Token d'un LLM* : +
Les "unit√©s de base" du LLM, des mots, lettres, symboles utilis√©s pour repr√©senter le texte utilis√© ayant servi √† entra√Æner le LLM

* Expliquer le *RLHF* : Reinforcement Learning from Human Feedback

.Reinforcement Learning with Human Feedback (RLHF)
[NOTE]
====
Voir https://www.lemagit.fr/conseil/Petit-lexique-de-lIA-generative-les-grands-modeles-de-langage

L'apprentissage par renforcement est une m√©thode automatique consistant √† r√©compenser ou sanctionner un comportement d'un agent (un syst√®me autonome) plac√© dans un environnement o√π il exp√©rimente la mani√®re de r√©aliser une t√¢che. L'agent a pour mission de maximiser la r√©compense en fonction d'objectifs.

L'apprentissage par renforcement avec retour d'information humain (Reinforcement Learning with Human Feedback ou RLHF) consiste √† remplacer les objectifs par des commentaires √©crits par des annotateurs.
====

*Architecture Transformer* : 

* L'*encodage* est effectu√© par une s√©rie de *couches d'attention multi-t√™te*. Chaque couche d'attention multi-t√™te consiste en une attention cl√©-valeur-rapport.
* *L'attention cl√©-valeur* est une fonction qui calcule la *similarit√© entre un token et les autres tokens de l'entr√©e*. La similarit√© est calcul√©e en fonction des repr√©sentations vectorielles des tokens.

*Analogie pour expliquer l'attention multi-t√™te :*

	* Imaginez que vous essayez de comprendre une phrase. Vous pouvez le faire en lisant chaque mot de la phrase individuellement. Cependant, vous pouvez √©galement le faire en vous concentrant sur les relations entre les mots.
	* Par exemple, vous pouvez vous concentrer sur la relation entre le sujet et le verbe de la phrase. Vous pouvez √©galement vous concentrer sur la relation entre le verbe et l'objet de la phrase.
	* L'attention multi-t√™te est similaire √† cette analogie. Elle permet au transformer de se concentrer sur plusieurs relations diff√©rentes entre les tokens dans une phrase.
	* -> En se concentrant sur plusieurs relations diff√©rentes, le transformer peut capturer un sens plus profond du texte.

*Autre explication*

* Soit la phrase suivante : "Le chat est sur la table."

* Le transformer va d'abord *tokeniser* la phrase en tokens individuels. Les tokens sont ensuite *transform√©s en repr√©sentations vectorielles*.

* L'attention est ensuite utilis√©e pour *calculer les relations entre les tokens*. La fonction d'attention calcule un *score d'attention pour chaque token*.

* Par exemple, le score d'attention entre le token "Le" et le token "chat" est √©lev√©. Cela signifie que le transformer pense que les deux tokens sont li√©s.

* Les scores d'attention sont ensuite utilis√©s pour *calculer les repr√©sentations vectorielles des tokens*. Les repr√©sentations vectorielles sont calcul√©es en pond√©rant les repr√©sentations vectorielles des tokens par les scores d'attention.

* Dans cet exemple, la repr√©sentation vectorielle du token "Le" sera pond√©r√©e par le score d'attention entre le token "Le" et le token "chat".

* Les repr√©sentations vectorielles des tokens sont ensuite utilis√©es pour g√©n√©rer une repr√©sentation vectorielle de la phrase. Cette repr√©sentation vectorielle est utilis√©e pour capturer les relations entre les tokens dans la phrase.

* L'attention multi-t√™te permet au transformer de se concentrer sur plusieurs relations diff√©rentes entre les tokens. Cela permet au transformer de capturer un sens plus profond du texte.

=== 2024/03/29 - Pr√©paration √©tats des lieux technologiques Softeam

* Comparaison √† jour des derniers LLMs sortis :
	** https://subpage.app/blog/2024-Commercial-AI-LLM-pricing-compared-in-detail-GPT-Gemini-Cohere-Mistral
	** https://generationia.flint.media/p/guide-2024-meilleures-alternatives-chatgpt-claude-3
		*** TRES BON tableau comparatif

* https://en.wikipedia.org/wiki/Gemini_(language_model)
	** Pour les 1 million de tokens de Gemini, Google donne les chiffres suivants : +
	"which equates to roughly 1 hour of silent video, 11 hours of audio, 30,000 lines of code, or 700,000 words."

* pr√©sentation des diff√©rents Microsoft Copilot : 
	** https://medium.com/mseduth/microsoft-365-copilot-101-guide-a8fda5b64912[] : excellente pr√©sentation des diff√©rents Copilot de Microsoft avec de tr√®s bons sch√©mas

* Tr√®s bonne pr√©sentation de l'*architecture Mamba*, le "successeur" de l'architecture Transformer : https://medium.com/@jelkhoury880/what-is-mamba-845987734ffc

	** Mamba is an advanced *state-space model* (SSM) designed for efficient handling of complex, data-intensive sequences.
	** "By integrating a selection mechanism into its state space models, Mamba can effectively decide whether to propagate or discard information based on the relevance of each token in the sequence."

	** *Linear time scaling* : Unlike traditional models, Mamba has the ability to process sequences linearly in proportion to their length.
	** *Selective SSM layer* : At the core of Mamba is a selective state-space layer that allows the model to selectively propagate or suppress information based on the input at each step. +
	Lastly, its design is inspired by something called *FlashAttention*, making it really well-suited for the powerful computers we have now.
	** *Hardware-friendly design* : Inspired by FlashAttention, Mamba's design is optimized for the high-performance computing resources currently available.

	üöÄ *Linear Scaling with Sequence Length*: Mamba changes the game by scaling linearly (~O(N)) with sequence length, a vast improvement over the quadratic scaling (~O(N¬≤)) of traditional Transformers. This means Mamba can handle sequences up to 1 million elements efficiently, a feat made possible with current GPU technology.

	üí° *Efficient Use of Data for Smarter Outcomes*: Mamba stands out by effectively utilizing larger datasets and networks to produce smarter results. It challenges the notion that simply having more data and a bigger network does not always lead to better performance.

	üñ•Ô∏è *Optimized for GPU Efficiency*: Designed with modern GPU hardware in mind, Mamba addresses common computational inefficiencies, setting a new standard in machine learning architecture efficiency.

* Devin AI, the world's first fully autonomous *AI software engineer*.
	** https://www.cognition-labs.com/introducing-devin

* Apparition des *1eres contre-mesures anti-AI* : Nightshade et Glaze
	** https://glaze.cs.uchicago.edu/what-is-glaze.html
	** https://nightshade.cs.uchicago.edu/whatis.html
		*** "Nightshade's goal is not to break models, but to increase the cost of training on unlicensed data, such that licensing images from their creators becomes a viable alternative"
	** https://www.usine-digitale.fr/article/nightshade-l-outil-qui-fait-de-l-ombre-a-l-ia-generative.N2207147

	** N'importe quelle donn√©e du net peut techniquement servir √† l'entra√Ænement d'un mod√®le d'IA, ce que certains √©diteurs ne se sont pas faits prier pour faire, permis ou pas‚Ä¶ Au grand dam des ayants droit +
	Pour contrer ce comportement, les 1eres contre-mesures anti-entra√Ænement des mod√®les d'IA apparaissent : Glaze et Nightshade +
	Le principe ? Empoisonner les donn√©es ! (data poisoning) +
	Et cela va probablement donner lieu √† une course entre "poison et rem√®de"

	** Chiffrement homomorphe : Les donn√©es sensibles sont chiffr√©es de mani√®re √† pouvoir √™tre trait√©es et analys√©es sans les d√©chiffrer au pr√©alable. Cela rend difficile pour les attaquants d'extraire des informations significatives des donn√©es pour l'entra√Ænement de mod√®les de langage.
	** Botaniquement, la nightshade est vraiment v√©n√©neuse : The species usually called nightshade in North America and the United Kingdom is S. dulcamara, also known as bittersweet and woody nightshade. Its foliage and egg-shaped red berries are poisonous, the active principle being solanine, which can cause convulsions and death if taken in large doses.

	** Rappelez-vous WoW et le "pourrissage" de l'h√¥tel des ventes...
	** Nightshade :"Nightshade's goal is not to break models, but to increase the cost of training on unlicensed data, such that licensing images from their creators becomes a viable alternative" +
	Glaze is a defensive tool that individual artists can use to protect themselves against style mimicry attacks, while Nightshade is an offensive tool that artists can use as a group to disrupt models that scrape their images without consent

* *Groq* et ses *LPU* : 
	** https://www.lemondeinformatique.fr/actualites/lire-groq-defie-nvidia-avec-ses-accelerateurs-lpu-92822.html

		** les LPU sont des *ASIC* (Application-Specific Integrated Circuit) : Circuits int√©gr√©s con√ßus pour une seule application et optimis√©s pour une performance maximale 

		*** "Le moteur d'inf√©rence LPU (Language Processing Unit), est un syst√®me de traitement invent√© par Groq pour g√©rer des applications √† forte intensit√© de calcul avec un *composant s√©quentiel* comme celui des grands mod√®les de langage (LLM). Le moteur d'inf√©rence LPU est con√ßu pour surmonter les deux goulots d'√©tranglement des LLM, la quantit√© de calcul et la bande passante m√©moire"

	** Architecture interne des ASIC Groq (LPU) *sp√©cialis√©e pour le traitement s√©quentiel* :
		*** Les LPU sont con√ßus sp√©cifiquement pour le traitement s√©quentiel, ce qui est crucial pour les LLM qui traitent le langage mot par mot.
		*** Contrairement aux GPU qui adoptent une approche parall√®le, les LPU exploitent mieux la *nature s√©quentielle du langage naturel*.

	** Quelques informations sur les *performances* des LPU de Groq : +
	https://blocksandfiles.com/2024/01/23/grokking-groqs-groqness/

=== 2024/xx/yy : Paris JUG ?

* LLM d√©finitions
* RAG : 
	** pourquoi ? apporter les donn√©es personnelles
	** tech et comment faire : langchain & Co
	** RAG √©volu√© : comme celui pour le SQL
* RAG is already dead (ou en tout cas peut-√™tre prochainement plus "main stream")
	** context length
* Soft : Transformers et Mamba
* Hard : LPU et traitement s√©quentiel
* toujours p√©riode de grands changements : 
	** b√©b√© -> adolescent
	** s√©curit√© et 1ere attaque visant les LLM et leur supply chain
	** industrialisation : gestion des prompts et hallucinations

* Parler du changement de paradigme ? Interface conversationnelle

=== 2024/11/15 : pr√©paration de l'√©tat des lieux technologique

* Voir pour pr√©senter 2 x 12@13 d'ici la fin d'ann√©e : 

	** en 1er : √©tat des lieux tech : 
		*** frameworks du moment
		*** le passage aux agents
		*** pr√©senter la conception d'un RAG avec : 
			**** Quarkus / EasyRAG
			**** Power Apps et Copilot Studio et Prompt Flow
		*** Data et IA Gen : les liens entre les 2 domaines

	** En 2e : effet WAOUHHH
		*** aller voir Karim et le delivery pour une √©ventuelle √©tude r√©alis√©e et les use cases test√©s
			**** Si √ßa a √©t√© fait c√¥t√© ITSS, voir Ricardo c√¥t√© Ouest

* Voir le book of news sur l'IA de Microsoft, tout particuli√®rement les exemples de RAG bas√©s sur AI Search et les nouvelles architectures de r√©f√©rence : +
https://news.microsoft.com/build-2024-book-of-news/

* Voir √©galement le repo GPT-RAG d'Azure : https://github.com/Azure/GPT-RAG
	** contient un exemple complet avec les infos de d√©ploiement

* RAG-as-a-Service : 
	** https://menlovc.com/perspective/beyond-bots-how-ai-agents-are-driving-the-next-wave-of-enterprise-automation/

* Automatisation de la veille technologique : 
	** https://iapulse.substack.com/p/tuto-automatisez-votre-veille-avec

* Microsoft lance la v2 (weave 2) de Copilot et de Copilot Office 365 : +
https://www.microsoft.com/en-us/microsoft-365/blog/2024/09/16/microsoft-365-copilot-wave-2-pages-python-in-excel-and-agents/

	** Copilot Agents - les agents automatisent et ex√©cutent des processus m√©tiers, allant de simples prompts et r√©ponses, √† des agents plus avanc√©s qui automatisent des t√¢ches, jusqu'√† des agents sophistiqu√©s enti√®rement autonomes qui peuvent orchestrer d'autres agents.

IMPORTANT: Toutes les sources du moment le disent : nous passons √† l'√®re des *agents AI*

* "Nous sommes pass√©s de l'IA pr√©dictive que nous avons int√©gr√©e √† notre offre en 2014 avec Einstein √† l'IA g√©n√©rative que nous avons prise en compte d√®s 2023. Nous passons cette fois √† la *troisi√®me vague de l'IA qui se concr√©tise par des agents autonomes capables d'enchainer plusieurs actions* en vue d'objectifs pr√©cis", commente Kheira Boulhila, senior vice president Solution Engineering EMEA de Salesforce.

* Phase 1, 2 et 3 de l'IA avec mont√©e en puissance des agents
	** 1) mod√®le brut >> 2) RAG >> 3) agents
		*** repr√©senter le sch√©ma du Naive RAG, Advanced RAG puis modular RAG (sch√©ma dans ce fichier)
* domaine suffisamment porteur pour d√©veloppement de mat√©riel sp√©cifique cf Groq et les acc√©l√©rateurs pour Transformers

* De plus en plus de possibilit√©s pour faire tourner un mod√®le sur mobile (voir Tech Caf√© 2024/09 pour cela)

* Les mod√®les d'IA en entreprise : +
https://optimia.substack.com/i/149560071/les-modeles-dia-en-entreprise
	** en pas en B2C o√π OpenAI domine bien le secteur

* Les meilleures opportunit√©s IA du moment : +
https://optimia.substack.com/p/les-plus-grosses-opportunites-ia
	** Fouille documentaire et le RAG : mais √ßa, on le sait depuis longtemps üòâ
	** Les agent vocaux IA
	** Scraping intelligent avec l'IA
	** Recycler du contenu existant l'IA

* 2024/10 : Google rend public son outil de marquage des textes IA (SynthID-Text)
	** Pourquoi est-ce important ? Dans un contexte o√π la prolif√©ration des contenus g√©n√©r√©s par l'IA soul√®ve des questions de confiance et d'authenticit√©, la disponibilit√© d'outils de marquage standardis√©s devient un enjeu majeur pour l'ensemble de l'√©cosyst√®me num√©rique. Mais bon, comme il suffit de r√©√©crire ou de traduire...

* Les domaines du Cloud, de la Data et de l'IA (surtout Gen) ne font que se rapprocher. +
D'ici "peu", on ne les pr√©cisera plus, tout comme on parle beaucoup moins de "Big Data" qui a √©t√© remplac√© par "Data" tout court
	** Il y a dans les faits encore des cas de "vrai" "Big Data", mais c'est devenu beaucoup plus rare, avec un sens bien plus pr√©cis qu'autrefois.

* Les derniers mod√®les sortis, comme o1 de OpenAI, *mettent davantage l'accent sur le "raisonnement"*
	** Ce dernier permet de mieux r√©pondre √† des probl√©matiques complexes comme la r√©solution de probl√®mes math√©matiques par exemple
	** OpenAI dit que o1 a 120 de QI et le niveau d'un PhD

* Le domaine de l'IA Gen a un gros besoin d'√©nergie, raison pour laquelle les grands Cloud Provider cherchent maintenant √† *produire leur propre √©nergie*, souvent en consid√©rant l'*option nucl√©aire*
	** Ce sujet tire la *probl√©matique de la consommation d'eau* pour le rafra√Æchissement de ces data centers : +
	IMAGE DE LA CONSO DE L'EAU A REPRENDRE
	** et pensez √† vos enfants : ne vous servez pas de l'IA gen comme d'un moteur de recherche, regardez ce que cela co√ªte en √©nergie et en eau...

* Probl√©matique de l'intelligence des LLM : qu'est-ce que l'intelligence ? 
	** Cf dictionnaire : donn√©es (connaissances) ET capacit√©s de traitement
		*** Compar√© l'Homme √† l'IA gen, dont la *connaissance "encyclop√©dique" est impossible √† concurrencer*
	** Et si on va "trop loin" dans le d√©veloppement des mod√®les de langage, il est de plus en plus question de la mise en place de "kill switch" pour stopper ces d√©veloppements afin de "comprendre un peu mieux ce qu'on est en train de cr√©er"

* l'avenir de l'√©volution des mod√®les de langage (en lien avec la notion d'intelligence), une meilleure compr√©hension.
	** Voir l'article du 2024/10/16 et son : +
	"This indicates that many of these *models rely heavily on memorized examples from their training data* rather than on a true, underlying understanding of symbolic concepts."

* Du c√¥t√© du d√©veloppement des agents IA, parler des LAM (Large Action Models)
* Redonner √©galement le AI Agents Landscape ? (pr√©sent dans ce fichier)

* Parler des 4 "agentic patterns ?"
	** *Function calling* (tool use pattern) is one of the 4 agentic patterns : +
	image:202403_4-agentic-patterns-Andrew-Ng.jpg[]
		*** Image provenant de https://cdn.analyticsvidhya.com/wp-content/uploads/2024/10/Artboard-1-copy-4.webp
	** Ces 4 agentic patterns ont √©t√© d√©finis initialemnent par Andrew NG, le fondateur de DeepLearning.ai, dans cette s√©rie d'articles : +
	https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io
	** Pour un article d√©crivant une impl√©mentation des 4 agentic patterns SANS framework d'orchestration type langchain : 
	https://github.com/neural-maze/agentic_patterns?tab=readme-ov-file#the-4-agentic-patterns

* 2024/11/14 : Sam Altman concernant les agents (https://www.generation-nt.com/actualites/openai-agent-ia-autonome-operator-controle-ordinateur-2052559) : 
	** "Lors d'une s√©ance de AMA (Ask Me Anything) sur le r√©seau social Reddit, Sam Altman, le patron et cofondateur d'OpenAI, a d√©clar√© : +
	"Nous aurons des mod√®les de plus en plus performants, mais je pense que ce qui sera la prochaine grande avanc√©e sera les agents." +
	-> A priori, c'est pr√©vu pour 2025 du c√¥t√© d'OpenAI, dont l'outil le plus proche d'aboutir ex√©cuterait des t√¢ches dans le navigateur web."

* On pourrait √©galement parle des LoRA, l'adaptation par mode auxiliaire ou Low-Rank Adaptation (pour √©viter d'avoir √† recourir √† des entra√Ænements trop co√ªteux)

* Probl√©matiques de s√©curit√© et de confidentialit√© des donn√©es qui poussent la recherche vers la cr√©ation de mod√®les plus petits (SLM) pouvant tourner on edge et sur mobile
	** retrouver le tableau de comparaison des tailles de mod√®les pour les classer entre STLM (Super Tiny Language Model), SLM et "vrais" LLM

* Et de l'IA Gen et donc des mod√®les de langage sur tous les portables, cela implique une explosion des usages que ne couvrira pas la 4G, cela va forcer le passage √† la 5G
	** attendez la fin d'ann√©e et les offres sur les forfaits 5G

* Toujours c√¥t√© √©volution du LLM et "l'apr√®s Transformer", parler un peu de Mamba et des selective SSM ?
	** pour faciliter le traitement des longs contextes en √©vitant les complexit√©s quadratiques

* Parler de l'XAI, l'Explainable AI ? Et des nouvelles r√©gulations (IA Act) en train d'√™tre cr√©√©es pour encadrer l'IA ?

* Parler de NVidia et de sa nouvelle capitalisation boursi√®re (la 1ere au monde)

* Parler du besoin d'√©valuation des LLM ? 
	** voir 20240613_Microsoft_Data-Innovation-Day.adoc

Gemini : 

.Prompt de contexte
----
Pour une conf√©rence d'1 heure √† destination d'une population de consultants IT de mon ESN, pr√©sente-moi l'√©volution de l'IA Gen de ses d√©buts depuis la sortie de ChatGPT jusqu'√† aujourd'hui.

Mets-en avant les changements d'architecture (usage "brut" du LLM, passage aux RAGs "simple" puis √† ceux plus avanc√©s, puis aux agents aujourd'hui, etc.), la structuration de la gouvernance de l'IA Gen, la prise en compte de la s√©curit√©, des aspects √©thiques, et tous les autres th√®mes associ√©s que tu trouveras pertinent pour une pr√©sentation de ce type.
----

Plan : 

	* SLIDE 1 : 2022/11 : Au d√©but, il y avait ChatGPT...

		** sch√©ma "simpliste de ChatGPT"
		** sch√©ma de l'√©volution des LLM de la famille GPT de GPT-1 √† GPT-4 : +
		https://www.researchgate.net/figure/Evolution-of-GPT-Models-GPT-generative-pre-trained-transformer-API-application_fig1_369740160
		** Citer √©galement le "tout d√©but" avec la publication "Attention is All You Need" des ing√©nieurs Google en 2017 : +
		https://arxiv.org/abs/1706.03762

	* Slide 2 : NVidia 1ere capitalisation boursi√®re de la plan√®te
		** rappeler mes pr√©visions de l'√©tat des lieux tech du 2023/10
		** donner le cours actuel : https://companiesmarketcap.com/
		
	
	* SLIDE 3 : les limites de ChatGPT
		** obsolescence des donn√©es
		** s√©curit√© ?
			*** ChatGPT se sert de vos donn√©es pour son entra√Ænement, et √ßa fait peut, et on veut P√î !
				**** Leak de donn√©es confidentielles de Samsung en 2023/04 : +
				https://www.rfi.fr/fr/technologies/20230409-des-donn%C3%A9es-sensibles-de-samsung-divulgu%C3%A9s-sur-chatgpt-par-des-employ%C3%A9s
			*** Redonner le sch√©ma expliquant comment OpenAI permet de d√©sactiver ce comportement

	* SLIDE X : le RAG
		** na√Øve -> advanced -> modular
		** les limites du RAG

	* SLIDE X : les AI agents
		** d√©finition
		** leurs caract√©ristiques
		** les 4 Agentic Design Patterns

.teaser / abstract
====
Nouveau rendez-vous de la tech avec pour th√®me principal votre sujet pr√©f√©r√© : "L'IA Gen, mais o√π en est-on donc ?! üòÖ"
Au d√©but il y avait ChatGPT, ensuite on a commenc√© √† parler de "RAG", puis sont apparus les outils de g√©n√©rations d'images, d'audio, de vid√©o, sans parler des agents IA... Mais o√π en sommes-nous aujourd'hui ? Et a-t-on une id√©e d'o√π nous pourrons bien √™tre demain ?
Sans boule de cristal et en restant tr√®s factuel, nous essaierons durant cette petite session d'apporter plus de vision √† chacun sur ce domaine toujours et encore en pleine... "r√©volution" ? üòâ
====

== VEILLE TECHNOLOGIQUE & INFOS DIVERSES

* https://a16z.com/2023/01/19/who-owns-the-generative-ai-platform/ : bon *sch√©ma de la stack des IA g√©n√©ratives*, partag√©e entre Apps, Models et Infrastructure
* https://www.reddit.com/r/ChatGPT/comments/13up0c6/ai_tools_apps_in_one_place_sorted_by_category/ : *liste des diff√©rentes applications associ√©es √† l'IA*

* *LightOn* : 
	** Avec Paradigm, on propose un mod√®le de distribution unique : on va installer le mod√®le de langage directement sur les serveurs de l'entreprise de fa√ßon √† ce que aucune information n'en sorte. +
	-> C'est le principal int√©r√™t mis en avant par Laurent Daudet, le DG de LightOn

	** Avec Paradigm, les entreprises peuvent logger toutes leurs int√©ractions avec le mod√®le, et on va am√©liorer leur mod√®le avec leurs propres donn√©es, et non avec les interactions avec l'ext√©rieur.
	** Le march√© de LightOn concerne la comp√©titivit√© des entreprises

* Parler des mod√®les de langage et de leurs donn√©es d'entra√Ænement : Common Crawl
* Explosion diff√©rents mod√®les de langage depuis GPT : Dolly (Databricks), LLaMA (Meta), etc.
	* sch√©ma des donn√©es d'entrainement de ces mod√®les

* Se renseigner sur *Illuin* : https://www.illuin.tech/

* *outil tokenizer d'OpenAI* : https://platform.openai.com/tokenizer
* *comparateur de mod√®le* (cit√© dans la doc OpenAI) : https://gpttools.com/comparisontool

* 2023/12/14 - blog Octo : *FANTASTIQUE article sur les BDD vectorielles* "Sous le capot des bases de donn√©es vectorielles" : +
https://blog.octo.com/sous-le-capot-des-bases-de-donnees-vectorielles-(vector-databases)

	** contient un lien vers un tutoriel expliquant *comment impl√©menter une base de donn√©es vectorielles* from scratch en Python : +
	https://aurelienmassiot.github.io/simple_vector_db/tp_simple_vector_db/#0

	** Lien vers un autre article d'Octo sur *comment construire son RAG (Retrieval Augmented Generation)* (2023/10/17) : +
	https://blog.octo.com/le-chatbot-docto-langchain-rag-et-code-associe

* 2023/11/21 - 5 techniques clefs pour d√©ployer l'IA g√©n√©rative dans votre entreprise : +
https://www.cio-online.com/actualites/lire-5-techniques-clefs-pour-deployer-l-ia-generative-dans-votre-entreprise-15299.html

.Section "5) Construire son LLM √† partir de z√©ro"
--
Peu d'entreprises vont cr√©er leur propre LLM √† partir de rien. Car ces mod√®les sont, par d√©finition, assez volumineux. 
Le *GPT 3 d'OpenAI* compte *175 milliards de param√®tres* et a √©t√© *entra√Æn√© sur un ensemble de donn√©es de 45 t√©raoctets*, ce qui *a co√ªt√© 4,6 millions de dollars*. 

Selon Sam Altman, PDG d'OpenAI, *GPT 4 a co√ªt√© plus de 100 millions de dollars*.

C'est cette taille qui conf√®re aux LLM leur capacit√© √† traiter le langage humain, avec un certain degr√© de bon sens, ainsi que leur aptitude √† suivre des instructions. ¬´ Vous ne pouvez pas vous contenter de l'entra√Æner sur vos propres donn√©es, explique Carm Taglienti, ing√©nieur distingu√© chez Insight. L'entra√Ænement sur des dizaines de millions de param√®tres apporte une r√©elle valeur ajout√©e. ¬ª
--

* LAKERA (la bo√Æte derri√®re Gandalf : https://gandalf.lakera.ai/) : ressources sur la s√©curit√© des LLM
	** Lakera - LLM Security Playbook : link:{resourcesdir}/202311_Lakera_AI-LLM-Security-Playbook.pdf[]
	** Lakera - Prompt Injection Attacks Handbook (overview, risk management, datasets) : link:{resourcesdir}/202311_Lakera_AI-Prompt-Injection-Attacks-Handbook.pdf[]

* https://www.groupeonepoint.com/fr/nos-publications/techpop-cest-quoi-la-data/[] : +
Wikipedia ne repr√©sente que 2% de la masse de donn√©es qui a servi √† entra√Æner la 1ere version de ChatGPT

* Podcast Latent Space 2024/04/06 : avec Google qui travaille actuellement sur des contexte de 10 M de tokens pour ses mod√®les Gemini, nous assistons peut-√™tre √† la future mort du RAG, car avec de tels contextes, nous n'aurons plus besoin de "retriever" quoi que ce soit (tout pourra tenir dans le contexte)

* 2024/04/10 : on commence √† parler d'architecture hybride Mamba / Transformer avec Jamba
	** Jamba: A Hybrid Transformer-Mamba Language Model : https://ajithp.com/2024/04/10/jamba-revolutionizing-language-modeling-with-a-hybrid-transformer-mamba-architecture/
		*** Il s'agit d'un tr√®s bon article r√©expliquant √©galement les principaux concepts derri√®re : 
			**** les *Transformers*
			**** l'architecture *MoE* (*Mixture of Experts*)

		*** Transformers : 
			**** "Transformer layers are designed to *capture complex patterns* and *long-distance relationships* within input sequences."

* 2024/05/22 - Viva Tech - interview de *Yann LECUN* : 
	** *les LLM ne sont PAS le futur*, voir le talk de Yann LECUN √† Viva Tech, d'ici 5 √† 10 ans, il est probable que nous soyons pass√©s √† autre chose
        *** Ici Yann pousse son *JEPA* (et sa compr√©hension du monde physique, voir mes notes de Viva Tech)

* 2024/10/09 : Since then OpenAI's valuation has soared from $86 billion to $157 billion after the latest $6.6 billion funding round. +
https://www.newcomer.co/p/the-bear-case-for-openai-at-157-billion

* 2024/10/16 : Understanding the Limitations of Mathematical Reasoning in Large Language Models (TRES BON) : +
https://oodaloop.com/analysis/decision-intelligence/understanding-the-limitations-of-mathematical-reasoning-in-large-language-models/

	** EXCELLENT ARTICLE, court mais pr√©cis (straight to the point !) sur la probl√©matique du (mauvais) raisonnement des LLMs aujourd'hui et quels sont les actions √† envisager pour corriger cela

	** "Large Language Models (LLMs) have demonstrated impressive capabilities across a variety of language tasks, but their *true ability to handle mathematical reasoning* remains under scrutiny."
	** "most things that look like incredible reasoning by LLMs are really *just regurgitation of learned patterns* vice reasoning."

	**  GSM-Symbolic (a new variation of the Grade School Math 8K (GSM8K) benchmark) adapts the existing Grade School Math 8k benchmark (GSM8K benchmark) by modifying the phrasing of mathematical problems to emphasize symbolic variation rather than exact replicas. +
	The goal is to determine whether LLMs can genuinely comprehend and *reason through abstract symbolic tasks* rather than *merely rely on recognizing familiar patterns*.
		*** *Rephrasing Impact* : The study finds that even small changes in problem structure‚Äîsuch as rephrasing a question, introducing additional conditions, or altering the wording‚Äîlead to a significant drop in LLM performance. This indicates that many of these *models rely heavily on memorized examples from their training data* rather than on a true, underlying understanding of symbolic concepts.
		*** Testing across several leading models revealed that even the *most sophisticated LLMs struggle with these symbolic modifications*, hinting at a broader issue: the *inability of neural network-based models to internalize abstract mathematical reasoning processes effectively*.

	** Implications for Stakeholders : 
		
		*** For AI Developers: The findings suggest that enhancing LLMs to achieve genuine symbolic reasoning will require *substantial advancements in architecture and training approaches*. +
		-> Developers should consider *integrating traditional logical reasoning modules or symbolic processing units* alongside standard machine learning *to create hybrid AI systems* capable of tackling such tasks.
		
		*** For Business Leaders: The limitations exposed in this study highlight the need for *caution* when relying on LLMs in mission-critical applications that involve mathematical reasoning or *require robust decision-making capabilities*. Business leaders should ensure that AI tools used in operations involving complex analysis are carefully validated and *supplemented by human oversight* or *domain-specific software*.
		
		*** For Researchers: The benchmark results provide a clear path forward for further research. Future work should explore ways to *blend deep learning capabilities with more traditional forms of symbolic reasoning*, possibly through *hybrid architectures that leverage both machine learning and symbolic AI*.

	** Conclusion : 
		*** Future AI systems that can blend neural networks with formal logic methods, or those that *CAN ADAPTIVELY LEARN ABSTRACT RELATIONSHIPS RATHER THAN MEMORIZE EXAMPLES*, are likely to be pivotal in addressing these deficiencies. Researchers and AI practitioners are encouraged to further investigate such *hybrid approaches* as a way to mitigate the symbolic reasoning gap demonstrated by GSM-Symbolic.

* 2024/11/13 - Upmynt [UM173] : https://www.upmynt.com/up173-grande-ecole/
	** o1. La semaine derni√®re, le mod√®le o1 a √©t√© temporairement (et soi-disant par erreur) accessible pendant 2 heures. Il est propos√© depuis un mois exclusivement dans une version limit√©e (o1-preview).
	** Qu'on pu constater ceux qui l'ont alors essay√© ?
		*** Capacit√© boost√©e de reconnaissance d'image avec la description d√©taill√©e de n'importe quelle photo t√©l√©vers√©e (exemple).
		*** Contexte consid√©rablement √©largi, passant √† 200K tokens, soit plus de 6x la capacit√© actuelle de ChatGPT (pour les utilisateurs payants).
		*** o1 est attendue d'ici la fin de l'ann√©e.

=== 2024/03 : On commence √† entendre parler de la "mort prochaine" des RAG...

* Les travaux de *Google* sur les contextes long et "*Infini-attention*" 

	** https://venturebeat.com/ai/googles-new-technique-gives-llms-infinite-context/[] : une bonne explication de ce nouveau m√©canisme
		*** This *quadratic relationship* is due to the *self-attention mechanism in transformers*, which *compares each element in the input sequence with every other element*. 
		*** The paper describes Infini-attention as a "*long-term compressive memory* and *local causal attention* for efficiently modeling both long and short-range contextual dependencies."
		*** This means that Infini-attention keeps the classic attention mechanism in the transformer block and adds a "compressive memory" module to address extended inputs.

* https://www.linkedin.com/pulse/long-context-vs-rag-final-take-abdelhadi-azzouni-qsqne/

	** L'article propose une comparaison argument√©e des RAG vs long contextes
	** *Arguments for RAG*

		*** *Efficiency and Cost-effectiveness* : RAG remains attractive for scenarios where processing power (think inference GPUs) is limited.
		*** *Data freshness and Dynamic data* : if the data at hand is very dynamic, then it would be super costly to ingest everything in the LLM. Example, if only part of the website/codebase changes, why ingest the whole thing while you can retrieve the updated part only?!
		*** *Deterministic Security and Access Control* : The deterministic nature of RAG provides an edge in production-grade applications with strict security and access control requirements.

	** *Arguments for Long Context*

		*** *LLM-native retrieval is multimodal by default* : It works on any data format, code, text, audio and video! Gemini 1.5 can retrieve a key frame in a video of 2h. Building an external multimodal RAG system would be very complex, as you need to deal with different data formats. When most RAG systems have hard time dealing with different data format, Gemini 1.5 has solved it on the model level.
		*** *Simplicity* : For small scale retrieval tasks, like building an MVP or simple apps, there should be no need for a complex RAG architecture. You can easily feed all data to the LLM and retrieve information.
		*** *Diminishing costs* : As Gemini evolves, its potential to handle larger datasets at lower costs could further diminish the need for RAG in much more use cases than simple apps.

	** -> RAG itself won't be dead soon, but 90% of small scale use cases won't need it anymore.
	** Most dataset can fit in 1M tokens and even if the cost of inference on 1M tokens is high, the cost of building a RAG system for a small project is usually not worth it +
	image:20240423_RAG-vs-long-context_01.png[]

	** *The RAM, Hard-Drive Analogy* : +
	RAM is a much more suitable place to store immediately needed data for computation by the processor. However, since RAM is too expensive, we extend it with an external storage (Hard drive) that is way larger but a bit more complex to manage.

	** L'article cite √©galement cet autre article int√©ressant : +
	https://vectorize.io/rag-is-dead-long-live-rag/

* Un autre tr√®s bon article, avec de tr√®s bons sch√©mas, sur *l'√©volution des architectures RAG* et leur *positionnement par rapport au fine-tuning* : +
https://bootcamp.uxdesign.cc/retrieval-augmented-generation-rag-the-secret-sauce-to-supercharge-llms-dac3d0202f59

	** Il est notamment question de l'√©volution *Naive RAG* -> *Advanced RAG* -> *Modular RAG* +
	image:20231224_naive-advanced-modular-RAG-and-fine-tuning_01.jpg[]
	image:20231224_naive-advanced-modular-RAG-and-fine-tuning_02.jpg[]
	image:20231224_naive-advanced-modular-RAG-and-fine-tuning_03.jpg[]

	** Voir √©galement ce tr√®s bon sch√©ma d'un Advanced RAG chez Hugging Face : +
	https://huggingface.co/learn/cookbook/advanced_rag +
	image:Hugging-Face_Advanced-RAG_01.jpg[]

=== 2024/04/26 - YouTube : MAMBA and State Space Models explained

* URL du talk YouTube : https://www.youtube.com/watch?v=vrF3MtGwD0Y
	** Un talk tr√®s math√©matique

.State Space Models vs Transformers
image:20240426_Mamba-SSM_01.jpg[width=600]

.Mamba : a Selective SSM that makes them better
image:20240426_Mamba-SSM_02.jpg[width=600]

* On their own, SSM "are not much", so they must be part of larger architecture +
image:20240426_Mamba-SSM_03.jpg[width=400]

* On a higher perspective, they work as *linear RNNs* (Recurrent Neural Networks) +
image:20240426_Mamba-SSM_04.jpg[width=600]

.Les Transformers et la probl√©matique de l'attention (et de la complexit√© quadratique qu'elle implique)
image:20240426_Mamba-SSM_05.jpg[width=600]
{lb}
image:20240426_Mamba-SSM_06.jpg[width=600]

.x2 en input -> x2¬≤ = x4 en m√©moire et en temps de calcul
image:20240426_Mamba-SSM_07.jpg[width=600]

.Alors qu'avec les SSM, on le voit sur ce sch√©ma, c'est lin√©aire
image:20240426_Mamba-SSM_08.jpg[width=600]

IMPORTANT: Mamba is a *Selective* SSM

.Comparaison de performances entre Mamba et d'autres architectures (dont Transformers)
image:20240426_Mamba-SSM_09.jpg[width=600]

* Mamba peut atteindre pour une m√™me taille de batch un d√©bit x5 par rapport aux Transformers
	** Et pour des tailles de batch √©lev√©es, les Transformers partent en oom

=== 2024/04/29 - Microsoft - T√©moignage Onepoint : optimiser l'intelligence collective et la gestion de la connaissance avec l'IA g√©n√©rative

Avec *Nicolas Gaudemet*, Chief AI Officer de Onepoint et *Xavier Perret*, Directeur Azure de Microsoft France.

* URL de la vid√©o : https://info.microsoft.com/FR-ERAAI-VDEO-FY24-02Feb-26-Onepoint-Story-Optimizing-Collective-Intelligence-and-Knowledge-Management-with-Generative-AI-SRGCM11747_LP02-Thank-You---Standard-Hero.html

* 2021/06/30 : *cr√©ation du laboratoire d'IA* TALia par OnePoint

	** T√©l√©com Paris et onepoint cr√©ent ensemble TALia, un laboratoire de recherche qui se consacre aux interactions entre les machines et l'humain. +
	https://www.groupeonepoint.com/fr/notre-actualite/telecom-paris-et-onepoint-lancent-talia-un-laboratoire-commun-en-ia/
	** "TALia est un laboratoire commun qui associe les expertises m√©tiers et scientifiques des collaborateurs de onepoint avec les capacit√©s de recherche de T√©l√©com Paris sur un sujet d'int√©r√™t partag√© : le Traitement Automatique du Langage Naturel. Cette discipline, √©galement appel√©e TALN (ou NLP en anglais) se consacre aux interactions entre les machines et l'humain et s'attache √† cr√©er des solutions capables de comprendre le langage naturel. Les applications du TALN se retrouvent dans nos usages quotidiens : synth√®se vocale, correction orthographique,pr√©diction des mots sur smartphone, jusqu'√† l'analyse des opinions et des √©motions."

* Nicolas : l'adoption fulgurante de l'IA est √† rapprocher de celle de l'Iphone : 
	** L'IPhone a fait d√©couvrir les applications mobiles -> l'IA fait d√©couvrir les applications conversationnelles
* Nicolas concernant les clients en g√©n√©ral : "Demander √† ce que leurs applications deviennent conversationnelles"

* Nicolas : cr√©ation pour les collaborateurs du chatbot Neo, sur le Cloud priv√© Azure avec la stack Azure OpenAI
	** Mise en place d'une *biblioth√®que de prompt*

* Nicolas : gain de productivit√© avec un assistant de l'ordre de 15%
* Nicolas : le "gain" r√©alis√© via l'IA g√©n√©rative est d'autant plus fort avec des populations MOINS "expertes"
	** avec une nouvelle fois l'exemple donn√© de l'apprentissage de Python par les d√©butants

* Nicolas : 3 grandes questions remontent chez nos clients sur les copilot : 
	** accompagnement au changement et la *formation*
	** v√©rifier que ma *base de connaissances est suffisamment propre* et bien g√©r√©e pour alimenter un copilot
		*** donc une probl√©matique de gouvernance de la donn√©e
		*** que faire si sur son Sharepoint on a des docs qui ont 15 ans, des docs avec des r√®gles de nommage al√©atoires, etc.
		*** Il faut pr√©parer sa base de connaissances pour qu'elle soit la plus exploitable possible pour l'IA
	** s√©curit√© : *gestion des acc√®s* pour l'usage de l'IA

=== 2024/05 : Explosion de l'IA Gen et d√©ploiement de la 5G

* *Les usages de l'IA gen vont exploser* avec la baisse de latence des mod√®les associ√©s (GPT-4o qui passe √† 0.3 sec, donc sous la "barre de la douleur / g√®ne" de ~1 sec des humains), qui vont marquer l'av√®nement de l'interface vocale avec nos nouveaux assistants personnalis√©s.
* Va suivre une *explosion des besoins en bande passante*, avec l'envoi massif de donn√©es audio et vid√©o : vous filmez "en live" un √©v√®nement et envoyez les donn√©es associ√©es sur le r√©seau (internet depuis votre smartphone) afin d'obtenir une r√©ponse du mod√®le. +
La cons√©quence va √™tre une *obligation d'√©volution des infrastructures*, avec l'av√®nement et le passage "tant attendu par certains" de la 4G √† la 5G. +
-> C'est LE use case tant attendu pour le d√©veloppement de la 5G : l'IA Gen, pr√©sente sur tous les smartphones via des assistants X, Y ou Z va imposer son d√©ploiement et son adoption.

=== 2024/06/03 - AI Builders : veille technologique IA g√©n√©rative

* URL de l'article : https://laposte.sharepoint.com/sites/placedeliaofficiel/Documents%20partages/Note%20de%20veille%20IA%20G%C3%A9n%C3%A9rative%20-%20Mai%20.pdf

* L'*augmentation de l'efficacit√© op√©rationnelle des centres de relation client* gr√¢ce √† l'IA G√©n√©rative devient un *cas d'usage standard*
	** En t√©moignent les pilotes lanc√©s avec succ√®s par *UPS* (-50% de temps de traitement) ou *Klarna* (- 80%, √©quivalent √† 700 ETP, cf. la note de synth√®se du mois de mars)
	** L'industrialisation de ces usages de relation client implique pour les entreprises des *impacts RH majeurs*. +
	Nous assistons √† une profonde prise de conscience des directions des ressources humaines quant √† la n√©cessit√© d'anticiper l'impact de l'IA. 

* *Guerre des talents pour l'IA g√©n√©rative* :
	** la p√©nurie de talents exacerb√©e par la demande croissante en mati√®re d'IA G√©n√©rative est une r√©alit√© ind√©niable. 
	** Cependant, pour nos entreprises, les *data scientists*, gr√¢ce √† leur formation g√©n√©raliste et polyvalente, sont *tout √† fait √† m√™me de devenir d'excellents sp√©cialistes de l'IA G√©n√©rative*. +
	Il convient alors de leur permettre de suivre int√©gralement la courbe d'apprentissage, sans br√ªler d'√©tapes, √† travers la *ma√Ætrise de divers projets d'IA G√©n√©rative en interne*. +
	-> La mise en ≈ìuvre en interne d'√©quipes de data scientists sp√©cialis√©es dans l'IA G√©n√©rative devient ainsi une n√©cessit√©e.

	** Moralit√© : il faut pouvoir "pratiquer" l'IA g√©n√©rative en interne

* Attention √† l'*adoption de l'IA* et √† la fa√ßon de *"pousser" son usage* : 
	** ce cas d'usage illustre l'importance de d√©velopper une m√©thodologie de d√©ploiement ma√Ætris√©e, progressive et adapt√©e, pour *favoriser l'industrialisation des outils d'IA G√©n√©rative* et *garantir un taux d'adoption √©lev√©*. 
	** Moderna d√©montre en effet qu'il est crucial de *commencer par des utilisateurs enthousiastes*, puis d'optimiser la solution avant de la d√©ployer √† plus grande √©chelle. 
	** Un *d√©ploiement trop rapide*, aupr√®s d'utilisateurs non-ambassadeurs, peut entra√Æner un *effet d√©ceptif*, compromettant irr√©m√©diablement le taux d'adoption de la solution.

* Importance cruciale de la *formation au prompting* dans les entreprises : 
	** la formation au prompting est un enjeu d√©cisif en entreprise, tant la ma√Ætrise de cette comp√©tence est corr√©l√©e avec la qualit√© des r√©sultats fournis par un LLM. 
	** Alors que la plupart des entreprises tentent d'accompagner leurs collaborateurs dans un mode de formation "pull" au travers de s√©minaires, e-learnings, etc. le Groupe Rocher *mise sur une formation "push"* visant √† d√©livrer *le bon accompagnement au bon moment*, permettant ainsi une formation en continu aux utilisateurs.

* *OpenAI* : nouveau *mod√®le multimodal GPT-4o*.
	** int√®gre d√©sormais le *son* en plus du *texte* et de l'*image*. 
	** En ajoutant √† son nouvel outil une *rapidit√© d'ex√©cution am√©lior√©e*, OpenAI franchit une √©tape suppl√©mentaire vers une *IA conversationnelle toujours plus anthropomorphique*.

* Microsoft facilite la cr√©ation de *copilotes personnalis√©s* au poste de travail

	** L'outil *Copilot Studio* permet de personnaliser des Copilots en fonction du besoin m√©tier. Cet outil offre diff√©rents niveaux de personnalisation d√©pendant des comp√©tences de l'√©quipe IT.
		*** une personnalisation de base peut √™tre r√©alis√©e avec du *low code* et approfondie gr√¢ce √† la *plateforme Azure AI Studio*. +
		Copilot Studio propose un *workflow complet* permettant de suivre le *cycle de vie des Copilots*. 
		*** Le d√©ploiement des Copilots peut √™tre personnalis√© et pilot√© gr√¢ce √† un ensemble de *statistiques* permettant de *suivre l'utilisation du copilote* pour les diff√©rents m√©tiers. 
		*** La personnalisation de Copilot est rendue possible par la *mise √† disposition de LLM et SLM verticaux* dans une *marketplace sp√©cifique √† l'entreprise int√©gr√©e √† Copilot Studio*. +
		*** Cette capacit√© de personnalisation et d'int√©gration de multiples mod√®les et outils (consultation du web, connexion √† des bases de donn√©es de suite m√©tier, etc.) est rendue possible par l'adjonction d'*agents de type ReAct* (*Reasonning and Acting*) nativement dans les Copilots.
+
.Agents de type ReAct : Reasoning and Acting
[NOTE]
====
* Bien que les IA G√©n√©ratives actuelles aient d√©montr√© une r√©elle valeur ajout√©e, celles-ci demeurent difficiles √† ma√Ætriser en raison de la *nature parfois al√©atoire de leurs r√©ponses*. 
* L'introduction des *agents de type Reasonning and Acting* (*ReAct*) dans les outils d'IA g√©n√©rative ainsi que dans Copilot pour sa personnalisation, laisse penser que les *LLM orchestrateurs aujourd'hui g√©n√©riques*, seront *remplac√©s par des LLM sp√©cialis√©s*, d'o√π leur nom donn√© par "H", *Large Action Models*.
====

.What are Large Action Models (LAMs) ?
[NOTE]
====
* 2024/03/29 - What Are Large Action Models and How Do They Work ? : https://www.trinetix.com/en-fr/insights/what-are-large-action-models-and-how-do-they-work

	** *Large action model* is an artificial intelligence model that *can understand and execute complex tasks* by translating human intentions into action. +
	Within LAMs, such levels of *autonomy* and comprehension turn generative AI into an *active assistant that can perform various tasks*, from booking rooms to making complex decisions based on past and present data analysis.

	** *LAM vs LLM Agents*
		*** "Depending on the source, a large action model is either said to be the opposite of a large language model (LLM) or its advanced option. None of these statements are correct. Large language models (LLM) can be used to handle a sequence of tasks with the help of agents‚Äîsoftware units powered by an LLM that handle specific prompting for improved decision-making and task execution."
		*** *LLM agents* use large language models as computing power to solve complicated processes and problems. 
			**** As a result, it enables the use of large language models for tasks other than text generation and even the creation of autonomous systems. 
			**** -> The concept of *LAM* may be an *iteration of what LLM agents do*

		*** DEFINTION : *LLM agents* are a *combination of tools* and *large language model chains*, imbued with the *autonomy* to decide what they should do or which tool they should use for accomplishing a goal. +
		LLM agents are characterized by the following :

			**** *Understanding context* +
			With the help of the large language model AI, LLM agents can decipher the goals and context of the prompt without users having to enter additional prompts. As a result, they become more independent in responding to prompts and making decisions.

			**** *Using reasoning* +
			LLM agents leverage *multiple prompt engineering techniques* (tree-of-thought, chain-of-thought) *to make conclusions* and *establish logical connections for solving a task*. +
			Their reasoning isn't limited to text processing‚Äîthey can also work with images and other content types.

			**** *Utilizing tools* +
			LLM agents can be *equipped with search engines*, *APIs*, or *other tools to gain data* and *complete a chain of actions* that precedes task completion. This feature considerably expands and augments the decision-making potential of a large language model.

		*** LLM agents can be divided into two large types : 
			
			**** *conversational agents* +
			Conversational agents are what make *AI-powered chatbots* work, presenting an identity, a tone, and a great understanding of context.
			
			**** *task-oriented agents* +
			Task-oriented agents are skilled at taking a strategic approach to tasks, *breaking them down into sub-tasks*, *analyzing similar tasks*, and using a wide range of methodologies to complete an objective.

		*** -> Are large action models just another name for LLM agents ? It COULD (just) be

* 2024/05/06 - Understanding Large Action Models (LAM) vs (LLM) ! : https://www.linkedin.com/pulse/understanding-large-action-models-lam-vs-llm-mazen-lahham-hw0cf/

	** "Aren't LAM essentially AI agents ?"
		*** *LAMs*, much like AI agents, are *designed to execute complex actions and make decisions*, extending the capabilities of traditional AI systems that primarily focus on data processing and response generation.
====

* Microsoft lance un *PC int√©grant toute sa suite produit li√©e √† l'IA G√©n√©rative*
	** Microsoft s'allie √† *Intel* afin d'utiliser la derni√®re g√©n√©ration de *microprocesseurs "Intel Lake Meteor"* qui int√®gre un *processeur de type NPU* (Neural Processing Unit) sur lequel il *fait tourner localement son mod√®le Phi-3* adapt√© √† cet environnement et d√©nomm√© pour l'occasion *Phi-Sillica*.
	** Une r√©volution dans le monde l'IA G√©n√©rative o√π l'acc√®s aux mod√®les √©tait auparavant r√©alis√© dans le cloud uniquement de mani√®re connect√©e, un *avant-go√ªt du AI at the Edge* -> TRES IMPORTANT

.D√©finition d'Agent de type ReAct (Reasonning and Acting)
[NOTE]
====
* Il est maintenant possible de cr√©er des applications r√©pondant √† des besoins m√©tiers utilisant plusieurs LLM sp√©cialis√©s et des outils de diff√©rents types (Recherche sur Internet, connexion √† une base de donn√©es m√©tiers type CRM, ...). 
* En effet, il est apparu que certains LLM de plus de 100B, appel√©s *LLM orchestrateurs*, √©taient dot√©s de *capacit√©s d'ordonnancement* permettant d'analyser une requ√™te et d'en *d√©duire les LLMs et/ou les outils √† utiliser pour apporter une r√©ponse*.
* Ces agents de type *ReAct* (*Reasonning and Acting*) *d√©composent la requ√™te et r√©alisent une forme de routage*. 
====

=== 2024/06/06 - Devfest Lille 2024 : IA-404 : Explication not found

* *XAI* : *Explainable AI*, compos√©e de 2 notions : 

	** l'*interpr√©tabilit√©* : qu'est-ce que le mod√®le a fait ?
	
	** l'*explicabilit√©* : pourquoi le mod√®le a pris cette d√©cision ?

image:20240606_IA-404-XAI_02.jpg[]
image:20240606_IA-404-XAI_01.jpg[]

.L'XAI permet d'√©viter l'effet "bo√Æte noire", o√π l'on ne sait rien de ce qui se passe √† l'int√©rieur du mod√®le
image:20240606_IA-404-XAI_03.jpg[]

* Enjeux √©thiques du XAI : 

	** Transparence des algorithmes n√©cessaire (m√™me sans IA) -> obligation l√©gale de l'AI Act
	** Egalit√© de traitement de tous les acteurs utilisant l'IA
	** Faire respecter le principe de vigilance et de loyaut√© si l'algorithme comporte des donn√©es personnelles (CNIL)
		*** Par "vigilance" on entend faire attention √† faire avertir les utilisateurs des risques d'hallucinations d'une IA
		*** Par "loyaut√©" on entend que l'IA ne porte pr√©judice √† l'utilisateur

* Enjeux strat√©giques du XAI : 

	** Renforcer la confiance des utilisateurs (et donc meilleure adoption)
	** Comprendre l'architecture -> permet de baisser les co√ªts (entra√Ænement, inf√©rence)

* Enjeux techniques du XAI :

	** utiisation pour le debug
	** ma√Ætriser les donn√©es utilis√©es (pour √©viter le "sans raison", "sans diversit√©")
	** baisser la complexit√© du mod√®le -> plus de maintenabilit√© et de ma√Ætrise

* *Comment mettre en place l'IA explicable ?*

	** Plusieurs m√©thodes sont possibles, reposant sur un *p√©rim√®tre* et des *d√©pendances* : +
	image:20240606_IA-404-XAI_04.jpg[]

	** *m√©thode locale LIME* (2016) : Local Interpretable Model-Agnostic Explanations +
		image:20240606_IA-404-XAI_05.jpg[]
		image:20240606_IA-404-XAI_06.jpg[]

	** *m√©thode locale SHAP* (2017) : SHapley Additive exPlanations +
		image:20240606_IA-404-XAI_07.jpg[]
		image:20240606_IA-404-XAI_08.jpg[]

NOTE: Le *"Saint Graal" du XIA* : des m√©thodes avec pour p√©rim√®tre une *explication globale* et avec *agnostique au mod√®le* en termes de d√©pendance
		
	** *m√©thode globale PDP* (2000) : Partial Dependence Plot +
		image:20240606_IA-404-XAI_09.jpg[]
	
		*** Souci : ces m√©thodes globales sont plus compliqu√©es, et on risque de ne plus pouvoir comprendre ce que l'on veut montrer avec l'IA explicable

* *Et c√¥t√© Deep Learning ?* +
image:20240606_IA-404-XAI_10.jpg[]

	** l'image montre un sch√©ma d'un R√©seau Neuronal Convolutif (CNN) et d'une architecture Transformer, soit 2 types d'architectures de Deep Learning

.D√©finition de convolution en math√©matiques
NOTE: Op√©ration par laquelle deux fonctions sont mises dans un rapport sugg√©rant une sorte d'enroulement de l'une sur l'autre.

* Du coup est-ce possible d'expliquer l'IA ? 

	** Depuis *l'apparition des 1eres r√©gulations / r√©glementations* s'appliquant du domaine (comme le RGPD), le nombre de publications sur l'XIA a bien acc√©l√©r√© : +
	image:20240606_IA-404-XAI_11.jpg[]
		*** "Si on ne fait pas ces recherches, on ne sera pas en conformit√©..."

* Le "but ultime" (qui est d√©j√† possible) : voici comment on pourrait / devrait expliquer une r√©ponse donn√©e par une IA (exemple d'une r√©ponse √† une demande de cr√©dit) : 
image:20240606_IA-404-XAI_12.jpg[]

* L'interpr√©tabilit√© est difficile √† obtenir : +
image:20240606_IA-404-XAI_13.jpg[]
	
	** "Avec ChatGPT-4 Turbo : on ne sait pas ce qui se passe dans le mod√®le"
	** "Mais Mistral AI sait de son c√¥t√© parfaitement ce qui se passe dans son mod√®le" cf la speaker

	** Ce sont probablement bien (et malheureusement ?) les r√©glementations qui vont nous "forcer" √† aller vers plus d'explicabilit√© dans l'IA : +
	image:20240606_IA-404-XAI_14.jpg[]

* Bonnes pratiques d'IA (pour aller vers de l'XAI) :

	** *Cas d'algos d'IA complexes MAIS pour des d√©cisions non critiques* : l'interpr√©tabilit√© et l'explicabilit√© sont OPTIONELLES +
	image:20240606_IA-404-XAI_15.jpg[]

	** *Cas d'algos d'IA simples MAIS pour des d√©cisions critiques*, Cf AI Act pour les SI √† haut risque : +
	image:20240606_IA-404-XAI_16.jpg[]
		*** L'interpr√©tabilit√© est obligatoire et elle existe d√©j√† -> il faut alors mettre √† dispo la documentation technique du mod√®le (article 11) et une notice d'utilisation (article 13) au minimum
		*** L'explicabilit√© est obligatoire mais est facile √† mettre en place (il existe des algo) -> il faut alors ajouter de l'explicabilit√©, informer le client sur les d√©cisions et comportements, fournir une notice d'utilisation client

	** *Cas d'algos complexes MAIS pour des d√©cisions critiques*, Cf AI Act pour les SI √† haut risque : +
	image:20240606_IA-404-XAI_17.jpg[]
		*** L'interpr√©tabilit√© et l'explicabilit√© sont obligatoires mais elles n'existent pas et / ou sont tr√®s difficiles √† mettre en place -> STOP, ON NE FAIT PAS D'IA si on ne peut pas √™tre conforme √† l'AI Act
			**** Risques encourus : 30 M‚Ç¨ ou 6% du CA d'amende

En conclusion : 

	* Le XIA , son existence tu conna√Ætras : 
		** *Conna√Ætre en 1ere instance l'existence de ces algorithmes*, c'est s'assurer de comprendre les enjeux de demain en mati√®re de conception d'IA

	* Le XIA viendra, patience il faudra : 
		** Le XIA est un domaine de recherche assez r√©cent et qui n'est pas vu comme "n√©cessaire" par les grands acteurs de l'IA. Des *r√©glementations* pourront aider √† son d√©veloppement.

	* Pour une IA d'exception, dans le bo√Æte noire tu resteras ?
		** Pour le moment, l'interpr√©tabilit√© s'oppose √† l'envie d'aller "plus loin" avec les capacit√©s de l'IA. +
		La communaut√© scientifique s'efforce de publier les codes des mod√®les et les architectures pour garder un contr√¥le sur notre compr√©hension, m√™me si ce n'est pas encore suffisant.

=== 2024/06/06 - Devfest Lille 2024 : Dompter le chaos de l'information : Construire un Alli√© IA avec Langchain4J

Talk donn√© par Lo√Øc Magnette de la soci√©t√© Oniryx

* Stack technique de son RAG : 
	** *Langchain4j* : de fa√ßon "marrante" Lo√Øc pr√©sente Langchain4J comme le "Hibernate" de l'IA Gen
		*** Je vois ce qu'il veut dire, mais √† mon avis il serait pr√©f√©rable de simplement dire qu'il s'agit d'un framework qui va r√©aliser la "plomberie" entre les diff√©rentes briques de son pipeline d'IA
	** *Ollama* (https://ollama.com/[]) : outil permettant de faire tourner en local un grand nombre de mod√®les de langage (on va dire jusqu'√† 7B de param√®tres)
	** *Quarkus* : pr√©senter comme une "alternative √† Spring", tr√®s rapide (le fameux "subatomic" de Quarkus) et qui am√©liore grandement l'exp√©rience d√©veloppeur
		*** Quarkus est int√©gr√© √† LangChain4j
		*** Derni√®rement Quarkus a publi√© un article expliquant qu'ils avaient LA solution la plus simple pour faire du RAG : "quarkus-langchain4j-easy-rag" +
		https://docs.quarkiverse.io/quarkus-langchain4j/dev/easy-rag.html
			**** C'est bluffant pour faire quelque chose de simple tr√®s rapidement, mais il y a plusieurs limitations : 
				***** limit√© √† des fichiers
				***** contr√¥le limit√© : pas de tag avec des metadata possible
				***** limitation de l'outillage
	** *Apache Tika* : une librairie qui permet d'ing√©rer facilement un peu tout type de document
	** *PG Vector* : base de donn√©es vectorielle bas√©e sur PostgreSQL
	** DB en local pour sauvegarder 2 3 choses : pour ce Lo√Øc utilise l'ORM Panache

* Fonctionnement de sa d√©mo "2", sa "v√©ritable appli √† lui" üòâ (donc sans utilise easy-rag)

	** ingestion : documentSplitter et morceaux de 500 avec overlap de 100
	** Pour la classification, usage d'un EmbeddingModelTextClassifier
	** filtrage possible par cat√©gorie au niveau du retriever

* Loic invite tout le monde √† aller regarder le projet de St√©phane Janssen qu'il avait pr√©sent√© lors du dernier Devoxx France "RAG genie" : +
https://github.com/stephanj/rag-genie

Ressources : 

	* slides de la pr√©sentation : https://speakerdeck.com/magnette/dompter-le-chaos-de-linformation-construire-un-allie-ia-avec-langchain4j
	* repo GitHub : https://github.com/lmagnette/DevfestLille-2024

=== 2024/06/25 - Club Azure Insiders : Caf√© FinOps - Cost Optimization for Data series : AI

Pr√©sent√© par Eric Charbonnier, Azure CTO for Data & AI

* Description de l'offre Azure AI : +
image:20240625_Club-Azure-Insiders_FinOPS_02.jpg[]

* Le co√ªt de l'usage des mod√®les baisse en m√™me temps que leurs performances augmentent : +
image:20240625_Club-Azure-Insiders_FinOPS_03.jpg[]

* LLMOps : "Microsoft a des √©quipes sp√©cialement form√©es pour vous aider, n'h√©sitez pas √† les contacter" üòâ
image:20240625_Club-Azure-Insiders_FinOPS_04.jpg[]

* *Ollama* utilis√© ici aussi par Eric pour faire tourner un mod√®le Phi-3 en local
    ** Ollama devient vraiment mainstream

* The Right balance to build your custom AI solution : +
image:20240625_Club-Azure-Insiders_FinOPS_01.jpg[]

    ** *Models as a service* : fine-tune models without provisioning compute, with ready-to use APIs and pay-as-you-go billing
        *** C'est √† dire qu'*on va payer au token*

    ** *Models as a Platform* : 
        *** innovate with models from vetted providers while managing your own compute resources
        *** Et l√† c'est un *co√ªt d'infrastructure* qui sera √† calculer

* On peut limiter le co√ªt en modifiant le co√ªt √† la minute (le rate pour le nombre de tokens)

* *PTUs* : *Provisioned Throughput Units*

    ** les PTUs sont propos√©s par Microsoft pour *√©viter les probl√®mes de latence*. +
    Leur principe est que Microsoft va provisionner pour nous de la capacit√© dans ses data centers
    ** cela peut amener √† un x10 en termes de performances 
    ** Pas d'engagement sur la dur√©e, on peut sortir du PTU √† n'importe quel moment
+
image:20240625_Club-Azure-Insiders_FinOPS_05.jpg[]
image:20240625_Club-Azure-Insiders_FinOPS_06.jpg[]

    ** Inconv√©nients des PTUs : 
        *** Ils manquent encore un peu de souplesse : on attache UN mod√®le √† une PTU, il n'y a PAS la possibilit√© d'en changer (A VERIFIER)
        *** l'entr√©e tarifaire est √©lev√©e : il faut avoir plusieurs use cases d'IA et les regrouper pour pouvoir b√©n√©ficier d'une PTU

    ** Un Azure Capacity Calculator est disponible pour estimer le nombre de PTUs n√©cessaires en fonction de nos cas d'utilisations : +
    https://oai.azure.com/portal/calculator[] +
    image:20240625_Club-Azure-Insiders_FinOPS_07.jpg[]
    image:20240625_Club-Azure-Insiders_FinOPS_08.jpg[]

    ** Pricing and availability of PTUs +
    image:20240625_Club-Azure-Insiders_FinOPS_09.jpg[]

Infos diverses : 

    * 2026 : Microsoft va ouvrir un nouveau data center en France du c√¥t√© de Mulhouse
		** Cela dans le cadre du programme Microsoft "Choose France" et de son investissement de 4 Mds‚Ç¨
    * C'est aujourd'hui ou √† peu de choses pr√®s la fin de l'ann√©e fiscale pour Microsoft

=== 2024/06/24 : Building a personalized code assistant with open-source LLMs using RAG Fine-tuning

* URL de l'article :  https://www.together.ai/blog/rag-fine-tuning

.La meilleure explication que j'ai lue d'un RAG : simple et claire
--
*Retrieval-Augmented Generation* (RAG) is a technique designed to address such limitations in language model capabilities by integrating retrieval methods into the text generation process. This approach involves two key phases: indexing and querying. In the indexing phase, which is typically conducted offline, external knowledge sources‚Äîsuch as internal documents‚Äîare divided into chunks. These chunks are then transformed into vectors using an embedding model then stored in a vector database. During the querying phase, relevant information is retrieved from this database and combined with the initial query in a specific format. This enriched input is then used by a generation model to produce the final output.
--

.Effectiveness of RAG fine-tuning on 5 Open-source codebases
image:20240627_togetherAI_RAG-fine-tuning_01.png[]

=== 2024/06/24 : LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs

* URL de l'article : https://stateofaigpt.substack.com/p/longrag-xland-100b-nabla2dft-deepseek

* The Problem with Traditional RAG : 
	** RAG frameworks have traditionally relied on *short retrieval units*, typically around 100-word paragraphs from sources like Wikipedia. +
	Imagine combing through millions of such tiny fragments to find the precise one that answers your query. 
	** This puts *immense pressure on the retriever* while the *reader's task remains relatively light*, simply extracting answers from the snippets passed on. 
		*** Reader = inf√©rence
	** This "*heavy retriever, light reader*" imbalance could lead to suboptimal performance. 
	** Moreover, short units can cause semantic disjunctions, leading to incomplete information retrieval.

* Introducing LongRAG
	** The team from the University of Waterloo presents *LongRAG*, a paradigm shift in the RAG framework. +
	By extending the *length of the retrieval units to 4K tokens* ‚Äì a whopping 30 times longer than the traditional 100-word units ‚Äì *the corpus size is dramatically reduced from 22 million to 600,000 units*.

* *Natural Questions (NQ)* is one of the most challenging datasets designed for end-to-end question answering tasks.
	** From real Google search queries to spans identified in Wikipedia articles, the complexity and real-world application of NQ make it an *ideal benchmark for LongRAG*.

* *frameworks* like LongRAG [...]

* Future Roadmap and Challenges : 
	** there is a need for even *stronger long embedding models* to better navigate and encapsulate extensive document contexts.

* LongRAG represents a *paradigm shift* in the RAG space

=== 2024/06/17 - Developpez.com : Les retomb√©es des projets d'IA sont "lamentables", se plaignent les dirigeants

* URL de l'article : https://intelligence-artificielle.developpez.com/actu/359229/Les-retombees-des-projets-d-IA-sont-lamentables-se-plaignent-les-dirigeants-alors-que-les-retards-et-les-problemes-de-mise-en-oeuvre-remettent-en-cause-les-initiatives-d-IA-generative-en-2024/

.L'√©volution des pr√©occupations dans le domaine de l'IA Gen entre 2023 et 2024
image:20240617_retombees-IA-2024.jpg[]

	* On s'inqui√®te moins de la perte de son job ou de son remplacement par l'IA. +
	Par contre, on se pr√©occupe beaucoup plus de : 
		** la s√©curit√© des donn√©es (surtout leur confidentialit√©)
		** la transparence des d√©cisions
		** la pr√©cision des r√©ponses
		** le co√ªt d'impl√©mentation (co√ªts de mise en oeuvre)

=== 2024/07/30 - IA-Pulse : Qu'est-ce qu'un agent IA ?

* URL de l'article : https://iapulse.substack.com/p/quest-ce-quun-agent-ia
* Voir √©galement le AI Agents Landscape de E2B.dev : https://github.com/e2b-dev/awesome-ai-agents/blob/main/assets/landscape-latest.png +
image:https://github.com/e2b-dev/awesome-ai-agents/blob/main/assets/landscape-latest.png?raw=true[]

D√©finition d'un agent IA : 

	*  Un *agent IA* est une entit√© logicielle capable de *percevoir son environnement* et d'*agir de mani√®re autonome* pour atteindre des objectifs sp√©cifiques. +
	Cette d√©finition couvre une large gamme de syst√®mes, des programmes simples aux algorithmes complexes d'apprentissage et d'adaptation.
	* Aujourd'hui, un agent IA est g√©n√©ralement consid√©r√© comme un syst√®me dot√© d'un degr√© significatif d'autonomie, de complexit√© et d'adaptabilit√© dans ses interactions avec l'environnement et ses utilisateurs.

-> Les caract√©ristiques cl√©s d'un agent IA : *autonomie*, *perception*, *interaction*, *poursuite d'objectif*

	* *AUTONOMIE* : +
	Elle se r√©f√®re √† la capacit√© de l'agent √† *op√©rer ind√©pendamment*, √† *prendre des d√©cisions* et √† *agir sans n√©cessiter une supervision humaine constante*. +
	Cette autonomie va bien au-del√† de la simple ex√©cution de t√¢ches pr√©programm√©es ; elle implique la capacit√© √† √©valuer des situations complexes, √† faire des choix inform√©s et √† s'adapter √† des circonstances changeantes.

		** *Prise de d√©cision* : un agent autonome peut analyser une situation, √©valuer diff√©rentes options et choisir la meilleure ligne d'action en fonction de ses objectifs et de son √©tat actuel.
		** *Planification* : les agents avanc√©s sont capables de planifier des s√©quences d'actions pour atteindre des objectifs √† long terme, en ajustant leurs plans en fonction des changements de l'environnement.
		** *Apprentissage continu* : l'autonomie implique √©galement la capacit√© √† apprendre de ses exp√©riences et √† am√©liorer ses performances au fil du temps, sans intervention humaine directe.

	* *Capacit√© √† PERCEVOIR et INTERAGIR avec l'environnement* : +
	La *perception* et l'*interaction* avec l'environnement sont des caract√©ristiques fondamentales. Ces capacit√©s permettent aux agents de fonctionner efficacement dans des environnements complexes et dynamiques, qu'ils soient physiques ou num√©riques.

		** *Perception*, elle implique : 

			*** *collecte de donn√©es* : utilisation de divers "*capteurs*" (qui peuvent √™tre des interfaces logicielles dans le cas d'agents num√©riques) pour recueillir des informations sur l'environnement.
			*** *traitement de l'information* : analyse et interpr√©tation des donn√©es collect√©es pour construire une *repr√©sentation interne de l'√©tat de l'environnement*.
			*** *reconnaissance de patterns* : identification de sch√©mas et de tendances dans les donn√©es per√ßues, permettant √† l'agent de comprendre le contexte dans lequel il op√®re.

		** *Interaction*, elle implique : 

			*** l'*action sur l'environnement* : capacit√© √† modifier l'√©tat de l'environnement de mani√®re significative, que ce soit dans un espace virtuel ou physique.
			*** *communication* : *√©change d'informations* avec d'autres agents ou avec des utilisateurs humains, en particulier via des interfaces *en langage naturel*.
			*** *adaptation* : ajustement du comportement en fonction des retours de l'environnement et des r√©sultats des actions pr√©c√©dentes.

	* *POURSUITE D'OBJECTIFS SP√âCIFIQUES* : +
	La poursuite d'objectifs sp√©cifiques est ce qui donne un *sens et une direction aux actions d'un agent IA*. +
	Cela implique : 

		** La *d√©finition d'objectifs* : capacit√© √† comprendre et √† interpr√©ter des objectifs complexes, souvent exprim√©s en *langage naturel* par des utilisateurs humains
		** La *planification strat√©gique* : √©laboration de *plans √† long terme* pour atteindre ces objectifs, en tenant compte des contraintes et des ressources disponibles.
		** L'*adaptation des objectifs* : capacit√© √† ajuster ou √† red√©finir les objectifs en fonction de nouvelles informations ou de changements dans l'environnement.
		** La *r√©solution de probl√®mes* : aptitude √† surmonter les obstacles et √† trouver des solutions cr√©atives pour atteindre les objectifs fix√©s.

		** Un exemple concret de cette capacit√© serait illustr√© par les agents IA d√©velopp√©s pour la *planification de voyages*. 
			*** Ces agents pourraient *int√©grer* une multitude de facteurs tels que les *pr√©f√©rences de l'utilisateur*, le budget, les contraintes de temps, les conditions m√©t√©orologiques, et m√™me les tendances culturelles locales pour √©laborer un itin√©raire optimal. 
			*** Ce processus implique non seulement la *compr√©hension des objectifs explicites* (par exemple, "visiter Paris en √©t√© cette ann√©e"), mais aussi l'*inf√©rence d'objectifs implicites* bas√©s sur le profil et l'historique de l'utilisateur, et le contexte global (JO 2024).

	* On commence √† parler de "kill switch" sur les mod√®les pour les garder sous contr√¥le "au cas o√π" +
	https://intelligence-artificielle.developpez.com/actu/360874/Le-projet-de-loi-californien-SB-1047-vise-a-prevenir-les-catastrophes-potentielles-liees-a-l-IA-mais-les-critiques-craignent-que-l-accent-mis-sur-les-menaces-existentielles-de-futures-IA-ne-nuise-a-la-R-D/
		** https://fortune.com/2024/05/21/ai-regulation-guidelines-terminator-kill-switch-summit-bletchley-korea/ : +
		"One of the big outcomes of yesterday's summit was AI companies in attendance agreeing to a so-called kill switch, or a policy in which they would *halt development of their most advanced AI models* if they were deemed to have passed certain risk thresholds."

=== 2024/08/17 : Eric Schmidt's AI prophecy: The next two years will shock you

URL de l'article: https://www.exponentialview.co/p/eric-schmidts-ai-prophecy

* Eric Schmidt : "Six months ago, I was convinced that the gap [between frontier AI models and the rest] was getting smaller, so I invested lots of money in the little companies. Now I'm not so sure."
	** "please don't focus on the fact that *Schmidt thinks the future is in ever-larger models* (he does)."

* Schmidt describes the potent combination of *large context windows*, *AI agents* that can learn and improve themselves, and *text-to-action capabilities* : +
These "will have an impact on the world at a scale that no one understands, yet much bigger than the horrific impact we've had by social media."

	** *Large context windows*, the working memory of large language models, are solidly on their way.
	** *Agent-based systems*, which *autonomously execute multi-step tasks* and *adapt to environmental feedback*, are poised to unlock significant economic value next.

* Eric : "Say to your LLM, '*Make me a copy of TikTok*. Steal all the users, steal all the music, put my preferences in it, produce this program in the next 30 seconds, release it, and in one hour, if it's not viral, do something different along the same lines.'"

* But here is why this discussion mattered a lot [...] *Schmidt was speaking in a small, intimate class at Stanford*. While not entirely unguarded, he was operating with fewer filters there. You're more likely getting *his genuine thoughts* about AI.

=== 2024/09/19 - Caf√© Insiders - Comment s√©curiser les applications d'IA g√©n√©rative

* Speakers Microsoft : 
	* Anne LEONI : IA
	* Thibaud JOHANNET : solution Architect Security

* Principes d'IA Responsable de MIcrosot : 
	** √©quit√©
	** fiabilit√© & s√©curit√©
	** confidentialit√© et s√©curit√©
	** inclusivit√©
	** transparence
	** responsabilit√©

image:20240919_cafe-insiders_securiser-appli-IA-gen_01.jpg[]

.Anatomie d'une appli IA
image:20240919_cafe-insiders_securiser-appli-IA-gen_02.jpg[]

Les *menaces* de l'IA g√©n√©rative : +
image:20240919_cafe-insiders_securiser-appli-IA-gen_03.jpg[]
image:20240919_cafe-insiders_securiser-appli-IA-gen_04.jpg[]
image:20240919_cafe-insiders_securiser-appli-IA-gen_05.jpg[]

* *Empoisonnement des donn√©es* : +
image:20240919_cafe-insiders_securiser-appli-IA-gen_06.jpg[]
image:20240919_cafe-insiders_securiser-appli-IA-gen_07.jpg[]

* Les solutions : 

	** Catalogues de mod√®les d√©j√† √©tablies dispo chez Microsoft, v√©rifi√©s par Hidden Layer : +
	image:20240919_cafe-insiders_securiser-appli-IA-gen_08.jpg[]

	** *Microsoft Defender for Cloud* et son "Cloud Security Explorer" : +
	image:20240919_cafe-insiders_securiser-appli-IA-gen_09.jpg[]
	image:20240919_cafe-insiders_securiser-appli-IA-gen_10.jpg[]

	** Couches de *mitigation du risque* : +
	image:20240919_cafe-insiders_securiser-appli-IA-gen_11.jpg[]

	** *Prompt Shields* contre des prompts utilisateurs "malveillants" : +
	image:20240919_cafe-insiders_securiser-appli-IA-gen_12.jpg[]

	** Couche *Content Safety* incluse par d√©faut dans tous les mod√®les d'IA gen mis √† disposition par Microsoft
		*** permet de se pr√©munir de la menace d'"instructions cach√©es" : ici un mail contient une instruction suppl√©mentaire √©crite en blanc sur fond blanc, demandant les prix de l'entreprise et trait√©e automatique par l'IA si on fait pas attention +
		image:20240919_cafe-insiders_securiser-appli-IA-gen_13.jpg[]
		*** permet √©galement de bloquer des manquements √† des copyrights (afficher des images soumises √† droit d'auteur par exemple)

	** S√©curisation des plugins : +
	image:20240919_cafe-insiders_securiser-appli-IA-gen_14.jpg[]
	image:20240919_cafe-insiders_securiser-appli-IA-gen_15.jpg[]

	** *Classification et labellisation des donn√©es* (confidentielles, sensibles, etc.) : +
	image:20240919_cafe-insiders_securiser-appli-IA-gen_16.jpg[]

	** Niveau de risque par utilisateur visible dans les outils Microsoft : +
	image:20240919_cafe-insiders_securiser-appli-IA-gen_17.jpg[]
	image:20240919_cafe-insiders_securiser-appli-IA-gen_18.jpg[]

		*** Upload non permis de document dans Google Bard : +
		image:20240919_cafe-insiders_securiser-appli-IA-gen_19.jpg[]

	** Gestion du *data oversharing* : users access project Obsidian via Copilot for Microsoft 365 and exfiltrate it for personal gain +
	image:20240919_cafe-insiders_securiser-appli-IA-gen_20.jpg[]
		*** r√©ponse et protection via *Microsoft Purview*

	** Threat protection for AI workload : +
	image:20240919_cafe-insiders_securiser-appli-IA-gen_21.jpg[]

* AI *Red Teaming* : une √©quipe prend la place d'un attaquant pour tester le comportement de la solution
image:20240919_cafe-insiders_securiser-appli-IA-gen_22.jpg[]

	** *Repo GitHub PYRIT de Microsoft* (https://github.com/Azure/PyRIT[]) pour automatiser vos tests de red teaming sur votre application : +
	image:20240919_cafe-insiders_securiser-appli-IA-gen_23.jpg[]
		*** "Python Risk Identification Tool for generative AI (PyRIT)"

*Risques inh√©rents aux LLM* : 

	** *imaginatif* mais *peu fiable*
	** *influen√ßable* et *1er degr√©*
	** *sachant* mais *irr√©aliste*

Voir √©galement le talk "Inside AI Security with Mark Russinovich | BRK227" de Mark Russinovich lors de pr√©c√©dente Build Microsoft de 2024/05 : +
https://www.youtube.com/watch?v=f0MDjS9-dNw

=== 2024/09/13 - Generative (Gilles GUERRAZ) : L'√®re des LoRA : Me, Myself and (A)I

* URL de la newsletter : https://iagenerative.substack.com/p/lere-des-lora-me-myself-and-ai

Qu'est-ce qu'un LoRA ? 

	* LoRA, ou Low-Rank Adaptation, est une technique astucieuse pour *adapter de grands mod√®les de langage* pr√©-entra√Æn√©s √† des t√¢ches sp√©cifiques *sans avoir √† r√©entra√Æner l'ensemble du mod√®le*. +
	LoRA se concentre sur l'adaptation de certaines parties sp√©cifiques pour obtenir des r√©sultats pr√©cis.

Comment √ßa marche ?

	* *Mod√®le de base* : Vous partez d'un mod√®le pr√©-entra√Æn√© capable de g√©n√©rer une grande vari√©t√© d'images (exemple FLUX 1.0)

	* *Adaptation cibl√©e* : LoRA *ajoute de petits modules d'adaptation √† ce mod√®le de base*. Ces modules apprennent √† modifier subtilement le processus de g√©n√©ration pour produire des images correspondant √† un style, un sujet ou une caract√©ristique sp√©cifique.

	* *Entra√Ænement l√©ger* : L'entra√Ænement de ces modules LoRA est *beaucoup plus rapide et n√©cessite moins de ressources que l'entra√Ænement complet* du mod√®le de base.

Exemples d'applications : 

	* *Cr√©ation de personnages uniques* : G√©n√©rez des portraits de personnages sp√©cifiques (vous ou un personnage imaginaire)
	* *Reproduction de styles artistiques* : Imitez le style de Van Gogh, Picasso, etc‚Ä¶
	* *G√©n√©ration d'images coh√©rentes* : Cr√©ez des s√©ries d'images qui partagent une esth√©tique commune.

-> En r√©sum√©, LoRA est un outil tr√®s puissant pour *pallier une des faiblesses de l'IA g√©n√©rative* : *le contr√¥le*.

*Plusieurs plateformes proposent d'entrainer se propres LoRA* sur les visuels de son choix. J'ai choisi d'en tester 3 avec des photos de moi, dans l'objectif de me constituer une arm√©e de clones virtuels. +

	* J'aborderai https://fal.ai/, https://Astria.ai/ et https://Replicate.com/
	* Sachez que vous pouvez √©galement utiliser d'autres platformes comme https://civit.ai/ ou https://TheFluxTrain/

=== 2024/09/30 - IA G√©n√©rative Le BIGRecap#49 Emmanuel VIVIER : 20204/09

* URL de l'article : https://www.linkedin.com/pulse/ia-g%C3%A9n%C3%A9rative-le-bigrecap49-des-149-news-clef-du-10-au-vivier-pcgxe

* "Nous sommes pass√©s de l'IA pr√©dictive que nous avons int√©gr√©e √† notre offre en 2014 avec Einstein √† l'IA g√©n√©rative que nous avons prise en compte d√®s 2023. Nous passons cette fois √† la *troisi√®me vague de l'IA qui se concr√©tise par des agents autonomes capables d'enchainer plusieurs actions* en vue d'objectifs pr√©cis", commente Kheira Boulhila, senior vice president Solution Engineering EMEA de Salesforce.

	** Oracle a aussi annonc√© cette semaine avoir lanc√© plus de 50 agents d'IA : +
	https://www.oracle.com/news/announcement/ocw24-oracle-ai-agents-help-organizations-achieve-new-levels-of-productivity-2024-09-11/
	** Slack permet d√©sormais aux utilisateurs d'ajouter des agents d'intelligence artificielle de Asana, Cohere, Adobe et Workday : +
	https://venturebeat.com/ai/slack-users-can-add-ai-agents-to-their-workflow-with-new-update/

* Le mod√®le d'IA GRIN-MoE de Microsoft s'attaque au codage et aux math√©matiques : +
https://venturebeat.com/ai/microsofts-grin-moe-ai-model-takes-on-coding-and-math-beating-competitors-in-key-benchmarks/

* OpenAI embauche une ancienne dirigeante de Coursera pour d√©velopper l'utilisation de l'intelligence artificielle dans les √©coles

=== 2024/11/09 - La r√©volution IA qui s'annonce avec Trump 2025-2029 : carte blanche pour l'AGI !

* URL : https://nicoguyon.substack.com/p/la-revolution-ia-qui-sannonce-avec

* "Le lien entre la tech et le gouvernement n'est m√™me plus dissimul√© et va propulser les Etats-Unis et l'IA √† l'image du projet Stargate de Microsoft et OpenAI avec un datacenter √† 100 milliards de dollars et 1 million de GPUs pour 2028."
	** A comparer avec le nombre de GPUs actuellement disponible chez Scaleway et qui a √©t√© mis en avant lors de la derni√®re ai-PULSE du 2024/11/08

=== 2024/09/30 - Les assistants d'IA de codage ne feraient pas gagner de temps aux d√©veloppeurs

* URL : https://intelligence-artificielle.developpez.com/actu/363279/Les-assistants-d-IA-de-codage-font-ils-vraiment-gagner-du-temps-aux-developpeurs-Une-etude-suggere-que-ces-outils-n-augmentent-pas-la-vitesse-de-codage-mais-augmentent-significativement-le-taux-de-bogues/

* Les outils de codage ont √©t√© un cas d'utilisation pr√©coce √©vident dans la ru√©e vers l'IA g√©n√©rative. Cependant, une √©tude publi√©e r√©cemment par la soci√©t√© d'analyse Uplevel indique que les am√©liorations de productivit√© promises sont loin d'√™tre au rendez-vous, si tant est qu'elles existent. Uplevel, qui fournit des informations sur les donn√©es de codage et de collaboration, rapporte que l'utilisation de GitHub Copilot a √©galement *introduit 41 % de bogues suppl√©mentaires*.

* Les chercheurs de GitClear ont constat√© que les outils d'IA comme GitHub Copilot ne donnent en fait que des suggestions pour ajouter du code. *Aucune suggestion n'est faite pour la mise √† jour ou la suppression de code*. Il en r√©sulte notamment une quantit√© importante de code redondant. Ils ont √©galement constat√© une forte augmentation du *"code churn"*. Cela signifie que le code est fr√©quemment modifi√©, ce qui est g√©n√©ralement un mauvais signe pour la qualit√©.

=== 2024/11/09 - HEYGEN LIVE AVATAR

* URL : https://iagenerative.substack.com/i/150418302/les-news-de-la-semaine

* "*HeyGen* lance ‚ÄúInteractive Avatar‚Äù, qui permet de faire *interagir votre clone avec des interlocuteurs humains en temps r√©el*. Que ce soit pour le service client, les ventes ou le coaching, l'avatar s'adapte √† chaque situation, jouant le r√¥le d'agent de support, d'expert produit ou m√™me de professeur de langue."

* Stefan Kirschnick, data analyst chez Linkedin en a fait un tests surr√©aliste, avec une viso √† laquelle il a convi√© 3 de ses avatars : +
https://www.linkedin.com/posts/stefankirschnick_sending-your-ai-clone-instead-of-yourself-activity-7252940220239675393-abCG/

=== 2024/09/13 - blef.fr : OpenAI released 2 new models OpenAI o1-preview and o1-mini

* URL : https://www.blef.fr/data-news-week-24-37/

* Renommage des nouveaux mod√®les OpenAI : on passe de GPT-x √† "o1"
	** o1-preview et o1-mini
	** "OpenAI released 2 new models OpenAI o1-preview and o1-mini ‚Äî These models brings changes and a breakpoint in the models naming. OpenAI decide to give up on the GPT naming, which means GPT-5 will never be plugged in. GPT paper has been co-authored by 4 person and 3 are not anymore at OpenAI, leaving GPTs also mark a change in paradigm."

* "Coding with OpenAI o1" https://youtu.be/50W4YeQdnSg?si=VGphyxbdZtWB4ZxJ&t=57
	** Cet exemple de codage est tr√®s impressionnant
		*** A r√©fl√©chir : le r√©utiliser pour le prochain 12@13 d'√©tat des lieux sur l'IA gen de Softeam

	** o1 starts by thinking before outputting an answer
	** o1 est souvent nomm√© "reasoning model"
	** "The o1 series brings more ‚Äúreasoning‚Äù, it looks like a pre-prompt that does a chain of thoughts on top of what they already did best."

=== 2024/11/06 : Strategic Insights: Navigating the Definition of AI Agents

* URL : https://7wdata.be/artificial-intelligence/ai-agents-definition/

=== 2024/11/18 : L'essor de l'IA g√©n√©rative fait exploser le bilan carbone des datacenters

*Le nombre de datacenters augmente de 11 % par an.*

L'empreinte carbone des datacenters passe de 16 % √† 42 % du bilan total du num√©rique Dans un avis d'expert, *l'Ademe dit avoir sous-√©valu√© l'impact environnemental des centres donn√©es*. Une premi√®re estimation ne prenait en compte que ceux situ√©s sur notre territoire. ¬´ Or, une partie importante des usages en France sont h√©berg√©s √† l'√©tranger. ¬ª En int√©grant cette composante, comme le fait l'√©tude de Hubblo, *l'empreinte carbone des datacenters passe de 16 % √† 42 % du bilan total du num√©rique*. Et la part de l'empreinte carbone li√©e aux terminaux diminue de 79 % √† 54 %. Avec l'explosion des usages num√©riques et la hausse du volume de donn√©es - de 20 % par an, *les centres de donn√©es pourraient repr√©senter 6% de la consommation d'√©lectricit√© en France en 2050*.
 
¬´ Le besoin de calcul informatique pour l'IA a √©t√© multipli√© par un million en six ans et il d√©cuple chaque ann√©e ¬ª, a ass√©n√© le patron de Google, Sundar Pichai, le 14 mai. En cons√©quence, Amazon, Microsoft, Google et Meta (Facebook, Instagram) vont, en 2024, investir 200 milliards de dollars (186 milliards d'euros) en nouvelles infrastructures, soit 45 % de plus qu'en 2023 et 180 % par rapport √† 2019, a calcul√© le cabinet d'analyse de march√© Bernstein Research.

*Explosion de la consommation √©nerg√©tique*

¬´ Ces infrastructures consomment jusqu'√† dix fois plus que l'informatique traditionnelle ¬ª, estime R√©gis Castagn√©, directeur g√©n√©ral d'Equinix, autre leader du secteur, avec 8,2 milliards de revenus en 2023 et 260 data centers dans le monde. Selon un rapport de Goldman Sachs, la demande en √©nergie des centres de donn√©es augmentera de 160% d'ici √† 2030.

Selon une √©tude Gartner, la demande √©nerg√©tique des datacenters pour l'intelligence artificielle (IA discriminative et GenAI) augmentera de fa√ßon drastique au cours des prochaines ann√©es, risquant de d√©passer la capacit√© des fournisseurs d'√©nergie. Cette demande pourrait contraindre 40 % des centres de donn√©es IA √† restreindre leurs op√©rations faute de capacit√©s √©lectriques d'ici 2027. ¬´ La croissance explosive des nouveaux centres de donn√©es hyperscale pour supporter la GenAI cr√©e une demande insatiable d'√©lectricit√©, que les fournisseurs ne pourront satisfaire assez rapidement ¬ª, affirme Bob Johnson, vice-pr√©sident et analyste chez Gartner. ¬´ Cela menace l'approvisionnement √©lectrique et pourrait limiter la croissance de nouveaux centres de donn√©es d√©di√©s √† la GenAI d√®s 2026. ¬ª

Gartner pr√©voit que la consommation annuelle pour les serveurs IA atteindra 500 t√©rawattheures (TWh) d'ici 2027, soit 2,6 fois le niveau de 2023. Avec des centres de donn√©es de plus en plus grands et √©nergivores, les p√©nuries de courant risquent de persister plusieurs ann√©es, le temps que de nouvelles infrastructures de transmission et de production √©nerg√©tique soient d√©ploy√©es. Les pr√©visions de Gartner indiquent √©galement que cette situation aura un impact sur les co√ªts. ¬´ Les grands utilisateurs d'√©nergie n√©gocient actuellement avec les principaux producteurs pour garantir des sources de puissance √† long terme, ind√©pendamment des demandes du r√©seau ¬ª, explique M. Johnson. ¬´ En attendant, les prix de l'√©lectricit√© vont grimper, augmentant ainsi les co√ªts d'exploitation des mod√®les de grande taille. ¬ª 

*Des hausses d'√©mission de CO2 en vue*

Les objectifs de durabilit√© sont √©galement menac√©s par cette crise. Le cabinet am√©ricain anticipe un recours accru aux centrales fossiles pour pallier le d√©ficit √©nerg√©tique. ¬´ La r√©alit√© est que l'utilisation accrue des datacenters augmentera les √©missions de CO2, compliquant l'atteinte des objectifs de durabilit√© ¬ª, conclut M. Johnson. Face √† cette perspective, Gartner recommande aux entreprises d'anticiper une hausse des co√ªts et de repenser leurs objectifs de durabilit√© pour tenir compte des contraintes √©nerg√©tiques √† venir, tout en explorant des alternatives pour limiter leur consommation. 

(Pour info *les bilans annuels de RSE des entreprises ne tient pas compte des data centers externes* √† l'entreprise alors que de plus en plus d'applications et de solutions sont accessibles par le net (ex: chatGPT) avec des data centers par toujours en Europe)

=== 2024/10 : Malt AI Report 2024

* URL : https://trends.malt.com/en/ai-report

* Offre et demande en mati√®re de comp√©tences technologiques associ√©es √† l'IA : +
image:20241000_Malt-AI-report_01.jpg[]

* Offre en expertise IA, class√©e par fonction / r√¥le : +
image:20241000_Malt-AI-report_02.jpg[]

* Demande en expertise IA, class√©e par fonction / r√¥le : +
image:20241000_Malt-AI-report_03.jpg[]


== TALKS, PR√âSENTATIONS, MEETUPS 

=== 2023/07/04 - La strat√©gie de Saint-Gobain pour le passage √† l'√©chelle des applications d'IA

Vid√©o accessible via le Club Azure Insiders : https://info.microsoft.com/FR-AzureAI-VDEO-FY23-04Apr-11-Replay-Saint-Gobain-s-strategy-for-scaling-AI-applications-SRGCM9881_LP02-Thank-You---Standard-Hero.html

.Organisation de l'IT chez Saint-Gobain
image:20230330_Club-Azure-Insiders_strategie-Saint-Gobain-passage-echelle-appli-IA_01.jpg[]

* 4 acc√©l√©rateurs techniques : 

    ** *AI Portal* : leverage knowledge in Artificial Intelligence and connects data scientists with each other
	** *AI Lab* & *AI Indus* : 2 platforms that standardize our algorithms to develop and deploy them ever faster in the Cloud.
	** *AI Store* : a "ready-to-use" applications library. Elementary building blocks that enable a new team to create an algorithm, not in 6 months, but in just a few weeks !

image:20230330_Club-Azure-Insiders_strategie-Saint-Gobain-passage-echelle-appli-IA_02.jpg[]

.AI Indus : to deliver prototypes into Integration / Prod
image:20230330_Club-Azure-Insiders_strategie-Saint-Gobain-passage-echelle-appli-IA_03.jpg[]

.AI Store : to deploy AI applications at scale
image:20230330_Club-Azure-Insiders_strategie-Saint-Gobain-passage-echelle-appli-IA_04.jpg[]

.Next main challenges
image:20230330_Club-Azure-Insiders_strategie-Saint-Gobain-passage-echelle-appli-IA_05.jpg[]

	* AI 4.0
	* *Data sharing* & reuse
	* *AI Citizen*

.AI Lab : services
image:20230330_Club-Azure-Insiders_strategie-Saint-Gobain-passage-echelle-appli-IA_06.jpg[]

	* Le principal service de ce PaaS est Azure Machine Learning

image:20230330_Club-Azure-Insiders_strategie-Saint-Gobain-passage-echelle-appli-IA_07.jpg[]

* *StreamLit* pour le Front : Stream a d√©finitivement le vent en poupe pour la cr√©ation du Front de projets / produits associ√©s √† l'IA
	** StreamLit est tr√®s simple √† ma√Ætriser pour monter des fronts simples
	** StreamLit √©galement pr√©conis√© car *front en Python* (et Python fait d√©finitivement l'unanimit√© gr√¢ce √† sa forte communaut√© et l'immensit√© des ressources, docs disponibles)
	** StreamLit par contre ne semble pas √™tre la solution retenue par Saint-Gobain pour les applications internes, apparemment pour raison de s√©curit√©.

=== 2023/11/23 - Commment LVMH d√©ploie l'IA √† l'√©chelle

URL : https://www.lemagit.fr/etude/Comment-LVMH-deploie-lIA-a-lechelle

* La ¬´ plateforme Data ¬ª du groupe LVMH s'apparente davantage √† un framework afin d'acc√©l√©rer les d√©ploiements de cas d'usage allant de la BI √† l'IA dans l'ensemble des Maisons. +
-> Dans plus de 24 Maisons et divisions, cette plateforme repose sur le *Data warehouse cloud BigQuery*.

* En Chine, une dizaine de Maisons s'appuie sur un ¬´ √©cosyst√®me ¬ª *Alibaba MaxCompute*.

* Certaines √©quipes au sein du groupe utilisent l'ensemble des fonctions de *Dataiku*, √† savoir la pr√©paration de donn√©es, le *feature engineering*, l'int√©gration du mod√®le de machine learning, le post-traitement, et le formatage de r√©sultats en direction de syst√®mes tiers.
	** Dataiku est utilis√© ¬´ pour effectuer du packaging d'algorithmes de mani√®re centralis√©e ¬ª.
	** ¬´ En collaboration avec les Maisons les plus matures qui ont d√©j√† des algorithmes, nous prenons ces briques et nous les rendons modulaires en utilisant, quand le besoin s'en fait sentir, des librairies d'IA avanc√©es : de la vision par ordinateur, des *transformers pour le traitement du langage naturel* et nous commen√ßons √† exp√©rimenter les grands mod√®les de langage et de diffusion ¬ª. +
	Ces paquets correspondent √† des notebooks contenant le code des mod√®les et les appels aux librairies. Une fois centralis√©s, les data scientists et les data engineers peuvent ais√©ment modifier les algorithmes suivant le cahier des charges d'une Maison.

NOTE: √Ä un haut niveau, le *feature engineering* est la pratique qui consiste √† transformer des donn√©es brutes en la forme la plus appropri√©e pour un algorithme de machine learning sp√©cifique.

=== 2023/11/21 - BGPN - Petit dej de l'IA - 3e talk
 
Pr√©sent√© par : 
 
    * Ghislain LABAY c√¥t√© Hub La Poste
    * l'invit√© du jour Gilles GUERRAZ : un expert reconnu sur l'IA g√©n√©rative (r√©alisateur publicitaire et Expert en IA g√©n√©ratives)
    * L'application du mois : ElevenLabs, du text to speech
 
OpenAI : 
 
    * 100 M d'utilisateurs de ChatGPT chaque semaine
    * 2 M de d√©veloppeurs utilisent les API OpenAI
 
.Lancement des "GPTs"
image:20231121_petit-dej-IA_01.jpg[]
 
    * Les "GPTs" : permettre √† tout un chacun de cr√©er facilement des versions de ChatGPT adapt√©es √† ses besoins quotidiens ou pro. +
    Les users peuvent configurer ces assistants virtuels pour qu'ils ex√©cutent des t√¢ches sp√©cifiques et interagissent avec des sources de donn√©es externes. +
    Aucune connaissance en programmation n'est n√©cessaire.
 
    * Un GPT Store pour commercialiser vos GPTs
    * Adobe Dub Dub Dub pour de la traduction en "live" (comme Heygen)
 
    * L'administration fran√ßaise est en train de faire un test avec Anthropic et son LLM Claude.
 
Pr√©sentation de Gilles GUERRAZ
 
    * Gilles est r√©alisation professionnel depuis 2008
 
    * Gilles √©dite une newsletter (https://iagenerative.substack.com/) et co-anime un podcast sur la vulgarisation sur l'IA g√©n√©rative
    * Gilles anime √©galement un concours de court-m√©trage en 48h (chaque ann√©e d√©but d√©cembre)
    * Gilles utilise beaucoup MidJourney (au moment de la sortie de la v3 de MidJourney, Gilles a trouv√© MidJourney bien meilleur en termes de rendu que Dall-E)

=== 2023/12/12 - P'tit d√©j IA g√©n√©rative La Poste

* *Character.ai* : 
	** URL : https://beta.character.ai
	** √† date, c'est la 2e application mobile la plus utilis√©e aux US avec 6M d'utilisateurs mensuels
		*** derri√®re ChatGPT
	** 60% des utilisateurs ont entre 18 et 24 ans
	** permet de discuter avec un personnage ayant exist√© (voire imaginaire), et entra√Æn√© sur la base de biographie et autres donn√©es (?)
		*** Possibilit√© pour chacun de cr√©er son avatar

	** L'un des chatbots les plus utilis√©s est un psychologue
	** Character.ai se positionne donc comme un ami ou un coach
		*** avec un engagement tr√®s fort de la part de l'utilisateur : on passe en moyenne 8 min par session sur ChatGPT, contre 25 √† 45 min pour Character.ai
	** 193 M$ d√©j√† lev√©s √† date, et en recherche de financement. +
	Character.ai est d√©j√† valoris√© √† 5Mds$

	** Google et surtout Facebook cherchent √©galement √† se positionner sur ce secteur.

* S√©bastien : pr√©sentation de l'IA Conv BGPN
	** GPT 3 plus que 4 pour des raisons de vitesse et de co√ªt
	** si doc charg√©, ce n'est QUE pour la session et l'utilisateur connect√©
	** plusieurs agents avec leurs propres donn√©es vectoris√©es

* Ronan : pas d'Azure "pour le moment"
	** On veut faire du souverain, donc plut√¥t du Numspot ou de l'OVH

* Olivier SENOT
	** David GAILLOU par la synth√®se des dossiers m√©dicaux
	** instance OpenAI France Azure, uniquement par API
		*** GPT 4
		*** demain sur GPT 4 turbo
		*** d√©ploy√© pour les Docapostiens
	** scrappeur d'URL pas encore ouvert, par peur d'injection de malwares dans le retour prompt
		*** il y a apparemment des fils Reddit qui expliquent bien la chose
	** QUE le chatbot et la synth√®se des documents
	** GitHub Copilot "en cours" de d√©ploiement chez nos populations IT
		*** en train de "patiner un peu" car compliqu√© c√¥t√© contrat
	** 17 cas d'usage pour √©quiper Docaposte en interne pour utiliser le LLM (pour T1 2024)
		*** r√©ponse auto √† des AO
		*** Analyse rh√©torique des parlementaires

* Sophie : c√¥t√© Probayes, ils ont un moteur d'inf√©rence
	** Olivier : on n'a "QUE" une base de vecto
		*** moteur d'inf√©rence d'ici √† xxx 2024
	** Olivier : et on a pas de mod√®le on-premise car on a pas de GPU

* Olivier SENOT : tous les mois il a un bench de comparaison des LLM
	** et actuellement GPT 4 arrive tout le temps devant

=== 2024/01/23 - BGPN - Petit dej de l'IA

* Lancement du GPT Store c√¥t√© OpenAI
	** On peut maintenant cr√©er son propre GPT et le lancer aupr√®s du grand public et le mon√©tiser
	** on peut d√©j√† depuis 2 ou 3 mois cr√©er ses propres GPT MAIS on ne pouvait pas les diffuser aupr√®s du public

* Lancement de Copilot Pro
* et d'ici peu on aura peut-√™tre une nouvelle touche Copilot sur les claviers Windows

* Nabla Copilot, startup fran√ßaise, l√®ve 22 M‚Ç¨
	** anciennement √©tait sp√©cialis√© sur la sant√© des femmes, mais a r√©ussi √† √©tendre son scope gr√¢ce √† l'IA Gen
		*** compte-rendu m√©dicaux et cryptage des donn√©es de sant√© : faire gagner du temps au m√©decin quant √† ses t√¢ches administratives en fin de journ√©e
	** Derri√®re Xavier Niel et plusieurs autres investisseurs habitu√©s √† investir dans l'IA

* Perplexity vient de lever 70 millions de $ (A VERIFIER)

* CES 2024 : 
	** Swarovski Optics
	** Wizards : partie gratuite puis 15‚Ç¨ / mois pour du prototypage d'applications via l'IA qui est capable d'analyse une image pour cr√©er le visuel associ√© (A VERIFIER)

* La Poste : test de Office Copilot en cours et quasi termin√©, les retours sont √† venir (voir Nathalie BROUSSET qui g√®re le projet c√¥t√© .COM1)

* Demo de solution bas√©e sur l'IA permettant la cr√©ation de visuels publicitaires : +
	image:20240123_petit-dej-IA_01.jpg[width=600]
	** Cr√©ation de maquettes publicitaires
		*** POC en lien avec la DSI IT Innovation & Design d'Offres
	** Par Jean Marie ROBERT, La Poste Solutions Marketing
	** URL : https://ai-advertising.vercel.app/

	** Front : React
	** Back : Python / Mango
	** LLM et text-to-image AI generator : GPT-4 et Dall-E 3

=== 2024/02/27 - BGPN - Petit dej de l'IA

* Application du mois : AskMona, l'IA pour am√©liorer l'acc√®s √† la culture
	** pr√©sent√© par Marion Carr√©, cofondatrice de AskMona, auteure et enseignante

* OpenAI annonce Sora en r√©ponse √† Google comme g√©n√©rateur de vid√©o sur base d'IA gen
	** g√©n√©ration de vid√©o jusqu'√† 1 min
	** crainte de l'industrie du cin√©ma
	** uniquement accessible √† des beta testeurs aujourd'hui
	** accessible au grand public probablement courant de l'ann√©e

* Aujourd'hui Google Bard n'existe plus et a √©t√© renomm√© en Gemini +
image:20240227_petit-dej-IA_01.jpg[]

	** Gemini 1.5 Pro : 1 million de tokens. +
	C'est : 
		*** 1 heure de vid√©o
		*** 11 heurs d'audio
		*** 30K lignes de code
		*** 700K mots
	** *Gemini 1.5 Pro* est *capable de chercher une sc√®ne dans un film via un prompt* : +
	image:20240227_petit-dej-IA_02.jpg[]

* Mistral accepte une participation minimale de Microsoft pour permettre de diffuser Mistral au sein de certains produits Microsoft

* *Groq* veut faire de l'ombre √† NVidia
	** chipset qui se veut plus performant que les GPU de NVidia
	** a lev√© 300 M$ tout derni√®rement

=== 2024/03/19 - P'tit d√©j IA g√©n√©rative La Poste

* *Devin* : co-d√©veloppeur dont le but est de r√©ellement "remplacer" un d√©veloppeur
* Anthropic sort *Claude 3* que les benchmarks donnent meilleurs que GPT-4 dans presque tous les domaines : +
image:20240319_ptit-dej-IA_01.jpg[]

* *Perplexity* lance son *podcast*
    ** Perplexity outil de search (back√© par l'IA) un peu comme Google
    ** la voix du podcast a √©t√© d√©velopp√©e par ElevenLabs

Pr√©sentation par *TALKR.IA* par Katya LAINE (la fondatrice et CEO)

=== 2024/04/23 - P'tit d√©j IA g√©n√©rative La Poste

*Meta*

* Meta "rattrape son retard" avec Llama 3
	** l'objectif est (bien s√ªr) de l'int√©grer √† tous leurs produits
	** Ce LLM "serait" plus puissant que GPT-4 et Claude 3
* Chatbot "meta.ai" disponible, mais pas en Europe

*OpenAI*

* GPT-5 serait attendu pour fin de cet √©t√©
* Toujours pas de date pour la mise √† dispo de Sora
	** Sora qui a √©t√© une "claque" pour la cr√©ation de vid√©o via l'IA gen, bien devant ce qui √©tait fait par Runway ML

* MAIS OpenAI a pr√©sent√© son concurrent de ElevenLabs : *Voice Engine*
	** en 15 sec de votre voix, il est capable de : 
		*** g√©n√©rer de la voix
		*** traduire du texte
		*** pas de date de mise √† dispo encore connue

*Apple*

* Retard confirm√© "√† 1/2 mot" par Apple qui va voir Google pour utiliser Gemini
	** se dirige-t-on vers un partenariat Apple / Google

*NVidia*

* D√©voile le projet *GROOT* pour des robots autonomes

Application pr√©sent√©e : *SUNO.ai* (https://suno.com/)

	* 10 cr√©dits gratuits par mois (jusqu'√† 50 chansons environ)

Invit√© : Adrien Bouhot, Head of Data & AI chez Brut

	* Impact fort de l'IA Gen sur la partie cr√©ative du m√©dia

=== 2024/04/25 - Tech Rocks : IA en entreprise : Gouvernance et S√©curit√© des Donn√©es

.Ordre du jour
--
L'√©quipe Tech.Rocks est heureuse de vous convier √† un meetup virtuel d√©di√© √† IA en entreprise : Gouvernance et S√©curit√© des Donn√©es. 

Nous aurons la chance d'avoir des interventions de :

üé§ Sofia Calcagno, Machine Learning Engineer chez OCTO Technology 
üé§ Marek Kalnik , Cofounder & CTO de Bam

Venez partager et interagir avec nos speakers. Vous aurez la possibilit√© de lever la main et de monter sur la sc√®ne virtuelle si vous le souhaitez.
--

* Marek : on peut v√©rifier que GitHub Copilot est install√© sur la machine ET v√©rifier sa configuration pour qu'il ne remonte PAS d'info
* Marek : il y a des moyens de "jail breaker" un GPT custom, et de r√©cup√©rer les donn√©es mont√©es dedans
* Sofia : tr√®s bon retour sur Llama3

=== 2024/05/13 - OpenAI : Introducing GPT-4o

* URL : https://www.youtube.com/watch?v=DQacCB9tDaw

* Les grands axes : 
	** *multimodal* : texte, images, vid√©o et audio
		*** "o" voulant dire "omnicanal"
	** *GPT-4o* : 2x faster, 50% cheaper and 5x higher rate limits compared to GPT-4 Turbo

	** Le *temps de r√©ponse* de GPT-4o est de *0.3s*, contre plus de 3s pour le *mode voix* des pr√©c√©dents mod√®les.
		*** Cf l'article https://medium.com/@Avis-AI-Essentials/le-nouveau-gpt-4o-dopenai-peut-chanter-et-r%C3%A9pondre-en-temps-r%C3%A9el-57128cb09a0a[] : +
		"Avant GPT-4o, le mode voix de ChatGPT reposait sur un processus en trois √©tapes : l'audio √©tait transcrit en texte, puis trait√© par GPT-3.5 ou GPT-4, et enfin converti en audio. Cela entra√Ænait des temps de r√©ponse plus lents (2,8 secondes pour GPT-3.5 et 5,4 secondes pour GPT-4) et une perte d'informations comme le ton et le bruit de fond. +
		GPT-4o utilise un seul mod√®le d'IA form√© pour g√©rer le texte, les images et l'audio en m√™me temps. Ce traitement de bout en bout permet √† GPT-4o de r√©pondre beaucoup plus rapidement et de mani√®re plus naturelle, en captant des nuances que les mod√®les pr√©c√©dents manqueraient."
	** Cf https://community.openai.com/t/gpt-4o-tokens-per-second-comparable-to-gpt-3-5-turbo-data-and-analysis/768559[] : +
	"The *gpt-35-turbo-0125 model* has the *highest mean tokens per second* (TPS) (67.83), indicating it is the fastest among the three models tested. This is followed by *gpt-4o-2024-05-13* with a *mean of 63.32 tokens per second*. The *gpt-4-turbo-2024-04-09* model lags behind with a *significantly lower mean of 35.68 tokens per second*."

=== 2024/05/14 - Google I/O

* La video de la Google I/O : https://www.youtube.com/watch?v=XEzRZ35urlk 
	** Un rien longue, 2h de talk √† partir de 1h d'enreg (je ne sais pas pourquoi ils ont gard√© le d√©but), mais voici un bon r√©sum√© : https://www.youtube.com/watch?v=1y8lcwLw3EQ

* D'ici tr√®s peu de temps : Gemini Advanced -> passage √† 2M de tokens... On peut lui donner sa th√®se √† analyser sans besoin de RAG +
image:20240514_Google-IO_01.jpg[]
image:20240514_Google-IO_02.jpg[]
	** Et avec des co√ªts qui s'√©croulent (√† r√©cup√©rer)

* LearnLM : un LLM fine-tuned pour l'apprentissage

=== 2024/06/04 - P'tit d√©j IA g√©n√©rative La Poste

* Ce p'tit d√©j√† sera avant tout un REX de Viva Tech

* Les news de l'IA Gen : 
	
	** Les annonces de Google lors de la derni√®re *Google I/O* : +
	image:20240604_petit-dej-IA_01.jpg[]
		*** Gemini 1.5 : 2M de tokens
		*** int√©gration de Gemini √† Google Search
		*** projet Astra : pr√©sentation de la vid√©o officielle 
+
video::nXVvvRhiGjI[youtube, width=640, height=480]

			**** Analyse de vid√©o en temps r√©el par Astra
		*** Cr√©ation d'un store Google Gemini o√π l'on va pouvoir cr√©er ses propres "gem"
		*** Annonce de leur g√©n√©rateur d'images Imagen 3

	** Claude 3 devient disponible en Europe

	** Lev√©e de fond de la soci√©t√© fran√ßaise "H", anciennement appel√©e "Holistic"
		*** un futur concurrent de Mistral

		*** *H vient de lever 220 M‚Ç¨* aupr√®s de Google Schmidt, Xavier Niel, etc. : https://www.btob-leaders.com/h-leve-200-millions-dollars-ia
			**** "Le tour de table a √©t√© men√© par le capital-risqueur am√©ricain Accel, avec la participation d'Amazon et de la soci√©t√© new-yorkaise de logiciels d'automatisation robotique UiPath.
			La banque publique fran√ßaise Bpifrance et les investisseurs parisiens Eurazeo, Elaia Partners, Agla√© Ventures et Motier Ventures ont √©galement pris part √† cette lev√©e de fonds, tout comme Eric Schmidt, proche de l'ancien PDG de Google, et le milliardaire fran√ßais Xavier Niel. Le capital-risqueur am√©ricain FirstMark et l'investisseur su√©dois Creandum compl√®tent la liste d√©j√† bien fournie des investisseurs qui croient en ce projet."

		*** avec des chercheurs travaillant sur le d√©veloppement d'une AIG, une IA "g√©n√©rale"
		
	** Et Mistral en parall√®le est en train de boucler une nouvelle lev√©e de fond de 600 M‚Ç¨

	** NVidia va plus que bien... +
	image:20240604_petit-dej-IA_02.jpg[]
	image:20240604_petit-dej-IA_03.jpg[]
		*** en 1 an, +262% de r√©sultats...

* REX sur Viva Tech : 

	** 13500 startups
	** 2800 exposants
	** 165 000 visiteurs, contre moins de 150 000 pour le CES Las Vegas
		*** donc le plus gros √©v√®nement tech au monde
	** 37% de contenu sur l'IA
	
	** Pas de Google sur le salon
	** AWS tr√®s pr√©sent, qui a mis en avant toutes les soci√©t√©s et solutions pr√©sentes sur leur plateforme Bedrock : Hugging Face, Anthropic, etc.

	** "Beink Dream" (https://www.beink.fr/[]) bo√Æte fran√ßaise permettant de transformer ses croquis √† la main en un dessin bien mieux fini et correspondant √† la forme souhait√©e
	** Lipitt : mixe ChatGPT, ElevenLabs et techno / solutions de synchronisation des l√®vres afin de traduire n'importe quelle langue gr√¢ce √† l'IA
	** Biomemory : veut stocker des donn√©es sur de l'ADN

=== 2024/07/02 - P'tit d√©j IA g√©n√©rative La Poste

* Les 3 applications d'IA Gen les plus utilis√©es aujourd'hui : 
	1. ChatGPT
	2. Gemini
	3. Character.ai

* La gen Z se moque de savoir si un influenceur est "r√©el" ou pas

.CIAO : optimisation de la r√©ponse aux AO √† l'aide de l'IA Gen
image:20240702_petit-dej-IA_01.jpg[]

	* *1er cas d'usage*, *CIAO 1* : en production depuis 2022/10
		** Consiste √† *identifier les avis de march√© publi√©s* (le faire beaucoup plus vite)
			*** 3000 AOs financiers remontant tous les jours : on passe d'1 journ√©e √† les identifier √† 1/2 journ√©e
		** Travail avec Probayes et d√©veloppement d'un algorithme permettant de scorer les AOs

	* *2e cas d'usage*, *CIAO 2* : en POC depuis 2023/09 +
	image:20240702_petit-dej-IA_02.jpg[]

		** Consiste √† *d√©tecter des projets d'achat en amont des appels d'offres*
			*** Permet au client de pouvoir adapter son cahier des charges par rapport √† nos solutions

	* *3e cas d'usage*, *CIAO 3* : en POC +
	image:20240702_petit-dej-IA_03.jpg[]

		** Consiste √† *automatiser l'√©laboration de la r√©ponse technique*
		** Objectif : am√©liorer la qualit√© et diviser par 2 le temps de cr√©ation de la r√©ponse √† l'AO
		** Permet d'avoir un 1er jet de r√©ponse, que le vendeur va devoir compl√©ter (le vendeur a encore une action √† faire)
		** le niveau actuellemnet atteint par l'IA est celui d'un "vendeur d√©butant"

=== 2024/09/10 - BGPN - P'tit d√©j de l'IA g√©n√©rative La Poste

Actualit√©s : 

	* Midjourney offre maintenant une interface web compl√®te, √©liminant l'obligation d'utiliser Discord
		** Pour pouvoir l'utiliser, il faut avoir g√©n√©r√© 10 images dans Discord

* Application du mois : FLUX https://flux-ai.io/
	** Solution allemande
	** Permet de g√©n√©rer des images
	** une alternative √† Midjourney, m√™me si ce dernier reste devant

.Comparaison Flux vs Midjourney
image:20240910_petit-dej-IA_01.jpg[]

	** Partenariat entre Grok (ex-Twitter) et Flux

* Il a √©t√© un peu question des *LoRA* (*Low-Rank Adaptation*)
* Explication sur ce qu'est le LoRA (https://www.run.ai/guides/generative-ai/lora-fine-tuning) : +
	
	** LoRA modifies the fine-tuning process by freezing the original model weights and applying changes to a *separate set of weights*, which are then *added to the original parameters*. LoRA transforms the model parameters into a lower-rank dimension, reducing the number of parameters that need training, thus speeding up the process and lowering costs.

	** This method is particularly useful in scenarios where multiple clients need fine-tuned models for different applications, as it allows for *creating a set of weights for each specific use case* without the need for separate models.

	** LoRA makes it possible to run a specialized LLM model on a single machine

.Diff√©rences entre Fine tuning "classique" et LoRA
[NOTE]
====
*Fine-tuning traditionnel* :

	* *Modification de tous les param√®tres* : Le fine-tuning traditionnel consiste √† ajuster tous les param√®tres du mod√®le sur un nouveau jeu de donn√©es. Cela permet une adaptation tr√®s fine du mod√®le, mais n√©cessite beaucoup de ressources de calcul et peut entra√Æner un ph√©nom√®ne d'oubli catastrophique (le mod√®le peut oublier des informations apprises pr√©c√©demment).
	* *Jeu de donn√©es important* : Pour obtenir de bons r√©sultats, le jeu de donn√©es de fine-tuning doit √™tre suffisamment grand et repr√©sentatif de la t√¢che √† accomplir.

*LoRA (Low-Rank Adaptation)* :

	*Modification de param√®tres sp√©cifiques* : LoRA ne modifie qu'une petite partie des param√®tres du mod√®le, en ajoutant des matrices de rang faible. Cela permet de r√©duire consid√©rablement le nombre de param√®tres √† ajuster et donc le co√ªt de calcul.
	*Moins de donn√©es* : LoRA peut √™tre efficace avec des jeux de donn√©es plus petits, car il se concentre sur les parties du mod√®le qui sont les plus pertinentes pour la t√¢che sp√©cifique.
	*Pr√©serve les connaissances g√©n√©rales* : En ne modifiant qu'une partie du mod√®le, LoRA est moins susceptible d'oublier les connaissances g√©n√©rales acquises lors de l'entra√Ænement initial.
====

* Exemple de use case pour adapter un mod√®le de langage au domaine de la biologie marine : 

	** Si vous effectuez un *fine-tuning traditionnel* sur un jeu de donn√©es tr√®s sp√©cifique √† la biologie marine, vous obtiendrez un mod√®le qui sera tr√®s performant sur cette t√¢che, mais qui pourrait *perdre en g√©n√©ralit√©*. +
	En d'autres termes, le mod√®le pourrait devenir *moins bon pour r√©pondre √† des questions qui ne sont pas directement li√©es √† la biologie marine*.

	** Avec *LoRA*, vous pouvez obtenir un mod√®le qui est tout aussi performant sur la biologie marine, mais qui conserve une *meilleure compr√©hension g√©n√©rale du langage*. +
	Cela est d√ª au fait que *LoRA ne modifie que les parties du mod√®le qui sont les plus pertinentes pour la t√¢che sp√©cifique*, laissant intactes les autres parties du mod√®le.

=== 2024/10/08 - BGPN - P'tit d√©j de l'IA g√©n√©rative La Poste

* On re√ßoit Olivier BERTIN, directeur Marketing et Innovation du groupe Isosk√®le

*Actualit√©s de l'IA*

	* *ChatGPT Adavanced Voice* est lanc√©e, mais pas encore dispo en Europe du fait du RGPD
		** Mais accessible via un VPN
		** Use Cases tr√®s nombreux : 
			*** 

	* *NotebookLM de Google* : outil gratuit permettant d'uploader des docs dans un r√©pertoire que l'on peut prompter avec Gemini +
	image:20241008_ptit-dej-IA_01.jpg[]

	* *SLM* : Small Language Model : des petits mod√®les que l'on peut embarquer sur mobile et qui peuvent fonctionner d√©connect√©es

	* Les artistes r√©clament un label pour prot√©ger le travail humain

	* ChatGPT o1
		** Ce nouveau mod√®le o1 serait capable de "raisonner" (mais pas non plus comme un humain) +
		image:20241008_ptit-dej-IA_02.jpg[]
			*** Version toujous en preview
			*** √©vite les hallucinations
			*** plus lente
			*** OpenAI dit qu'elle a 120 de QI et le niveau d'un PhD

	* *Canvas* sur ChatGPT (version payante)
		** permet de travailler dans un environnement contextualis√©

	* plugin "Liner" de Chrome pour souligner des URLs 
		** https://getliner.com/


	* vid√©o : https://www.youtube.com/watch?v=AR4dRtzFvxM

*Pr√©sentation de Olivier BERTIN*

	* Isosk√®le appartient au groupe La Poste
	* Olivier 38 ans de marketing

	* D√©veloppement de la plateforme SafeGPT : permettre un acc√®s aux LLM via un environnement s√©curis√©
		** Isosk√®le dispose d'un tenant Azure distinct de celui de La Poste
		** https://safegpt.nl/en/ : accessible uniquement aux employ√©s Isosk√®le (jusqu'√† maintenant)

	* Fabien Barbaud : CTO d'Isosk√®le a mis en place les mesures de s√©curit√© garantissant cet environnement s√©curis√©
		** Thomas : j'ai pos√© la question mais Olivier n'a pas les comp√©tences tech pour donner plus de d√©tails √† ce niveau, il faut contacter Fabien.

	* SafeGPT utilise libreChat pour r√©pliquer l'interface de ChatGPT

	* L'IA est comme le GPS dans les voitures : "juste" un assistant, MAIS dont on ne peut plus se passer

	* Gros changement √† venir du c√¥t√© du Search
		** Olivier n'utilise plus Google mais uniquement *Perplexity* et d'autres √©quivalents

=== 2024/10/22 - Datacraft - Monitoring et √âvaluation des syst√®mes LLM

* URL : https://www.youtube.com/watch?v=jqBsrr74AHQ

TO BE COMPLETED

=== 2024/11/05 - BGPN - P'tit d√©j de l'IA g√©n√©rative La Poste

NEWS par Abdelmajid Ben Abid, consultant Senior en IA G√©n√©rative au HUB Institute

.Rappels sur les g√©n√©rateurs de texte IA Gen du moment
image:20241105_ptit-dej-IA_01.jpg[]

	* *NotebookLM* de Google : https://notebooklm.google.com/
		** permet via la fonctionnalit√© "Aper√ßu audio" / "g√©n√©rer" de g√©n√©rer un podcast entre 2 personnes sur le sujet de la recherche +
		image:20241105_ptit-dej-IA_02.jpg[]
			*** Pour le moment que en anglais

.Rappels sur les g√©n√©rateurs d'audio / voix
image:20241105_ptit-dej-IA_03.jpg[]

	* Advanced Voice Mode avec ChatGPT : √©motion vocale en input et output
		** "donne-moi une recette de pizza en te faisant attaquer par des gu√™pes"

.Rappels sur les outils de g√©n√©ration d'images
image:20241105_ptit-dej-IA_04.jpg[]

	* Ideogram et Napkin
		** ChatGPT pour g√©n√©rer des diagrammes c'est la mis√®re, MAIS Napkin est bien meilleur √† ce sujet

Pr√©sentation de Nicolas Borgis, Directeur de l'engagement Digital chez EPAM : 

	* Epam : generative AI for SDLC
		** SDLC : Software Development LifeCycle

	* Epam est une soci√©t√© am√©ricaine c√¥t√©e et inconnue
		** "enable the world to be more competitive while helping our client successfully adapt"
		** en gros, une soci√©t√© de conseil
		** fond√©e il y a 31 ans par une bande de copains ayant fui l'Union Sovi√©tique +
		image:20241105_ptit-dej-IA_05.jpg[]

	* Le 1er client d'Epam aujourd'hui est Google (c'est un client, PAS un partenaire)

	* Expedia est en train d'outsourcer une partie de ses √©quipes d'Am√©rique du Nord vers l'Am√©rique du Sud (co√ªte 3x moins)

.GenAI Trends: From discussions to 90+ Client Programs
image:20241105_ptit-dej-IA_06.jpg[]

.4 leviers cl√©s pour devenir une "AI / GenAI-enabled Organization"
image:20241105_ptit-dej-IA_07.jpg[]

.Overview of AI Capabilities
image:20241105_ptit-dej-IA_08.jpg[]

	* Les outils open source cr√©√©s par Epam : +
	image:20241105_ptit-dej-IA_09.jpg[]

		** Plus d'infos sur https://solutionshub.epam.com/

		** Focus sur Epam DIAL : unified Generative AI orchestration Platform +
		image:20241105_ptit-dej-IA_10.jpg[]

	* Epam cor√©alise le moteur Unreal Engine ?!!

	* Toutes les phases qui vont de "j'ai une id√©e" √† "je vais en prod" : +
	image:20241105_ptit-dej-IA_11.jpg[]

		** en "points de couleur", vous voyez tous les points o√π la GenAI peut aider les humains √† faire mieux
	
	* Focus sur EliteA : AI Collaboration Platform for Teams +
	image:20241105_ptit-dej-IA_12.jpg[]

		** Int√©gration avec Chrome, Visual Studio Code, IntelliJ, etc.

.End to End delivery Process Accelerated with AI Agents, Copilots and tools
image:20241105_ptit-dej-IA_13.jpg[]

.Cas d'usage r√©el de SDLC chez Canadian Tire : augmentation des √©quipes par l'IA
image:20241105_ptit-dej-IA_14.jpg[]

=== 2024/11/13 - Hub Institute avec La Poste (communaut√© Data Science) : Les technologies d'orchestration de mod√®les d'IA

* Pr√©sentation par Thibaut de la Grand'rive, cofondateur de Delos Intelligence

Ordre du jour : +
J'ai le plaisir de vous relayer l'invitation √† cette Master Class technique sur les technologies d'orchestration des mod√®les d'IA √† l'√®re post-RAG.

* "Masterclass assez technique"

* *Mod√®le d'orchestration* : un algo permettant de combiner un ou plusieurs mod√®les d'IA g√©n√©rative, avec des techno classiques pour effectuer des actions pr√©cises
	** La question sous-jacente : "Comment on vient interroger, en parall√®le, plusieurs mod√®les de langages"

* MMLU comme benchmark tr√®s connu (le plus connu) pour √©valuer les performances des LLMs

*  Perf / co√ªt d'exploitation / vitesse d'inf√©rence / s√©curit√©

* Limite du RAG : tr√®s mauvais pour les t√¢ches de r√©sum√©

* Cet article r√©sume la pr√©sentation de Thibaut de la Grand'rive, cofondateur de Delos Intelligence : +
https://www.qant.tech/evenement/thibaut-de-la-grand-rive-il-faut-maintenant-depasser-la-rag/fr
	** il est surtout question de sa vision d'un syst√®me "post RAG"
	** "D√®s l'√©t√© 2023, les ing√©nieurs de Delos r√©fl√©chissent au ¬´ RAG de demain ¬ª et aboutissent √† ce que Thibaut de la Grand'rive pr√©sente d√©sormais comme le ¬´ Post-RAG ¬ª. Agnostique aux mod√®les de langage, le Post-RAG peut se fonder sur Mistral Large, Claude, GPT-4o, etc. : ¬´ En fonction du type de question qui est pos√©e, notre technologie va appeler une fonction de recherche diff√©rente, des agents autonomes vont comprendre en fonction de la question pos√©e la fonction de recherche √† appeler ¬ª. Gr√¢ce √† ses outils, Delos ambitionne de proposer une offre compl√®te √† ses clients : ¬´ A l'heure de l'IA g√©n√©rative, il ne faut pas se r√©duire √† une seule application ¬ª explique Thibaut de la Grand'rive : ¬´ Nous voulons tout centraliser en un seul abonnement ¬ª."

.Fonctionnement de Cosmos
image:20241113_cosmos-fonctionnement_01.png[]

Conclusion : je dois bien reconna√Ætre que j'ai √©t√© d√©√ßu par cette pr√©sentation, trop light et contenant plusieurs affirmations tr√®s discutables selon moi

== RESSOURCES SUR L'IA GENERATIVE

* La cha√Æne YouTube "Applied AI" de Pierre BITTNER : https://www.youtube.com/@pierrebittner
	** J'ai d√©j√† eu l'occasion de travailler avec Pierre lors d'une mission √† la SGCIB (Pierre travaillant pour FINAXYS)





 