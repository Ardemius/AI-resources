= Ressources sur les IA g√©n√©ratives et les LLMs (OpenAI ChatGPT et autres)
Thomas SCHWENDER <icon:github[] https://github.com/Ardemius/[GitHub] / icon:twitter[role="aqua"] https://twitter.com/thomasschwender[@thomasschwender]>
// Handling GitHub admonition blocks icons
ifndef::env-github[:icons: font]
ifdef::env-github[]
:status:
:outfilesuffix: .adoc
:caution-caption: :fire:
:important-caption: :exclamation:
:note-caption: :paperclip:
:tip-caption: :bulb:
:warning-caption: :warning:
endif::[]
:imagesdir: ./images
:resourcesdir: ./resources
:source-highlighter: highlightjs
:highlightjs-languages: asciidoc
// We must enable experimental attribute to display Keyboard, button, and menu macros
:experimental:
// Next 2 ones are to handle line breaks in some particular elements (list, footnotes, etc.)
:lb: pass:[<br> +]
:sb: pass:[<br>]
// check https://github.com/Ardemius/personal-wiki/wiki/AsciiDoctor-tips for tips on table of content in GitHub
:toc: macro
:toclevels: 4
// To number the sections of the table of contents
//:sectnums:
// Add an anchor with hyperlink before the section title
:sectanchors:
// To turn off figure caption labels and numbers
:figure-caption!:
// Same for examples
//:example-caption!:
// To turn off ALL captions
// :caption:

toc::[]

Notes au sujet des IA g√©n√©ratives, ChatGPT, Dall-E et leurs concurrents et alternatives.

== Pr√©paration de talk de pr√©sentation des IA G√©n√©rative (courant 2023)

=== Que sont les IA g√©n√©ratives ? 

* DONE - 2022/12/15 : chatGPT la r√©volution : AI g√©n√©rative, vous pouvez r√©ellement converser avec elle, elle passe le test de Turing 

* DONE - https://www.commentcoder.com/chatgpt/#comment-commencer-%C3%A0-utiliser-chatgpt- +
OpenAI est une entreprise am√©ricaine cr√©√©e par Elon Musk et Sam Altman en 2015 √† San Francisco en Californie. Son but est de cr√©er des avanc√©es dans l'intelligence artificielle. Ses services les plus connus sont DALL-E 2, GPT-3 et ChatGPT.

* DONE - ChatGPT est un agent conversationnel qui s'appuie sur le mod√®le de langage GPT-3 pour interpr√©ter le texte d'un humain et g√©n√©rer des r√©ponses pr√©cises et intelligibles, en fran√ßais comme en anglais. Toutefois, ses connaissances se limitent √† tous les √©v√©nements pr√©c√©dant l'ann√©e 2021.

* DONE - AI g√©n√©rative, vous pouvez r√©ellement converser avec elle, elle passe le test de Turing 

* DONE - 2023/01/23 : Article sur l'usage de ChatGPT : https://www.blogdumoderateur.com/tout-savoir-sur-chatgpt/

* DONE - Malgr√© tout ce que peuvent en dire les m√©dias, il n'est pas correct de qualifier de ChatGPT (et de ses consorts) "d'IA g√©n√©rationnelle" √©tant donn√© que ce terme n'existe pas r√©ellement dans le domaine de l'IA, et aurait plut√¥t tendance √† se r√©f√©rer √† des algorithmes g√©n√©tiques. +
	** Il faut √† la place parler d'IA conversationnelles ou d'IA "g√©n√©ratives" (mais PAS "g√©n√©rationnelles")

	** D√©finition des *algorithmes g√©n√©tiques* : +
	Les algorithmes g√©n√©tiques sont une m√©thode d'optimisation qui utilise des techniques d'√©volution biologique pour r√©soudre des probl√®mes complexes. +
	L'algorithme commence par cr√©er une population initiale de solutions possibles. Ensuite, des op√©rations de s√©lection, de croisement et de mutation sont appliqu√©es sur la population pour produire de nouvelles solutions. +
	Les solutions sont √©valu√©es pour leur "fitness" et les plus performantes sont s√©lectionn√©es pour cr√©er une nouvelle population. Ce processus est r√©p√©t√© jusqu'√† ce qu'une solution satisfaisante soit trouv√©e ou que le nombre d'it√©rations pr√©d√©fini soit atteint. +
	Les algorithmes g√©n√©tiques sont utilis√©s dans de nombreuses applications d'optimisation, notamment en ing√©nierie, en √©conomie et en informatique.

* DONE - 2023/01 : https://www.commentcoder.com/chatgpt/
	** Tr√®s bon article pr√©sentant ChatGPT dans sa globalit√©

	** Quel mod√®le d'apprentissage utilise Chat-GPT ?
		*** ChatGPT utilise une s√©rie de mod√®les et est connue sous le nom de ‚ÄúGPT 3.5‚Äù, elle est compos√©e des mod√®les d'apprentissage text-davinci-003 pour le texte et code-davinci-002 pour le code. +
		On a donc des IA diff√©rentes pour le code et pour le texte. C'est peut-√™tre ce qui explique pourquoi ChatGPT est si puissant pour nous aider avec le code mais laisse parfois √† d√©sirer dans ses r√©ponses textuelles ü§∑.

	** Quelle version de GPT utilise ChatGPT ?
		*** La version de *GPT* (*Generative Pretrained Transformer-3*) utilis√©e par ChatGPT est GPT-3.5 qui a termin√© de s'entra√Æner au d√©but de l'ann√©e 2022.
		*** üí° C'est d'ailleurs parce que ChatGPT a fini de parcourir les sources textuelles et de code d√©but 2022 qu'on dit que ChatGPT donne des r√©sultats valides que jusqu'en 2021 (inclus).
		*** Le pr√©d√©cesseur de ChatGPT s'appelle InstructGPT qui utilise le mod√®le d'apprentissage GPT-3. La diff√©rence entre Instruct GPT et Chat GPT est impressionnante, vivement une la prochaine version d'IA conversationnelle d'OpenAI qui utilisera GPT-4 !

* DONE - ChatGPT a √©t√© cod√© en Python.
* DONE - https://openai.com/blog/chatgpt/ : ChatGPT and GPT 3.5 were trained on an Azure AI supercomputing infrastructure.

** DONE - Quelle version de GPT utilise ChatGPT ?
		*** La version de *GPT* (*Generative Pretrained Transformer-3*) utilis√©e par ChatGPT est GPT-3.5 qui a termin√© de s'entra√Æner au d√©but de l'ann√©e 2022.
		*** üí° C'est d'ailleurs parce que ChatGPT a fini de parcourir les sources textuelles et de code d√©but 2022 qu'on dit que ChatGPT donne des r√©sultats valides que jusqu'en 2021 (inclus).
		*** Le pr√©d√©cesseur de ChatGPT s'appelle InstructGPT qui utilise le mod√®le d'apprentissage GPT-3. La diff√©rence entre Instruct GPT et Chat GPT est impressionnante, vivement une la prochaine version d'IA conversationnelle d'OpenAI qui utilisera GPT-4 !

	** Qu'est-ce que GPT-4 ?
		*** GPT-4 est la prochaine version de GPT. L'IA aura √©t√© entra√Æn√©e sur plus de 100 000 milliards de param√®tres (contre 175 milliards pour GPT-3). Generative Pre-trained Transformer-4 devrait arriver en cette premi√®re moiti√© d'ann√©e 2023.

prompt : la question que vous souhaitez poser √† ChatGPT ; pensez √† int√©grer du contexte pour obtenir des r√©ponses pertinentes.

* Tr√®s bonne d√©finition de wikipedia : https://fr.wikipedia.org/wiki/ChatGPT : 
	** "ChatGPT est un prototype d'agent conversationnel utilisant l'intelligence artificielle, d√©velopp√© par OpenAI et sp√©cialis√© dans le dialogue."

DONE - L'agent conversationnel de ChatGPT est bas√© sur le mod√®le de langage GPT-3 d'OpenAI, et est affin√© en continu gr√¢ce √† l'utilisation de techniques d'apprentissage supervis√© et d'apprentissage par renforcement, afin d'am√©liorer les performances du logiciel.

.supervised fine-tuning vs reinforcement learning
[NOTE]
====
Le supervised fine-tuning est utilis√© pour ajuster un mod√®le pr√©-entra√Æn√© √† une t√¢che sp√©cifique √† l'aide de donn√©es d'entra√Ænement suppl√©mentaires, tandis que le reinforcement learning est utilis√© pour apprendre une politique de prise de d√©cision optimale √† partir de l'interaction avec un environnement complexe et en maximisant une r√©compense.
====

DONE - 2023/02/09 - BFM Tech & Co : ChatGPT moteur de r√©ponse l√† o√π Google est un moteur de recherche

*SLIDES*

* *D√©finitions* : 

	** IA g√©n√©rative : Une IA g√©n√©rative a pour but de g√©n√©rer de nouvelles donn√©es, telles que du texte (agents conversationnels comme ChatGPT), des images (Dall-E) ou de la musique, √† partir d'un ensemble de donn√©es d'entra√Ænement.

	** ChatGPT : ChatGPT appartient √† cette cat√©gorie des IA g√©n√©rative. +
	Il s'agit d'un agent conversationnel (un "chatbot"), bas√© sur le mod√®le de langage GPT 3.5, utilisant un mix d'apprentissage supervis√© et d'apprentissage par renforcement pour affiner en continu son mod√®le afin d'am√©liorer ses performances.

	** *Mod√®le de langage* : il s'agit d'un type de mod√®le d'apprentissage automatique con√ßu pour comprendre et g√©n√©rer du langage humain. Il est entra√Æn√© sur de grandes quantit√©s de donn√©es textuelles pour apprendre la structure, la grammaire et le sens du langage naturel. 

	** *Apprentissage supervis√©* : il s'agit d'une m√©thode d'apprentissage automatique o√π il est question d'apprendre √† classer les donn√©es au sein de cat√©gories (classes / √©tiquettes) pr√©d√©finies : on conna√Æt ces derni√®res √† priori. +
	On le distingue de l'apprentissage NON supervis√© o√π au contraire il est question de trouver des cat√©gories dans lesquelles les donn√©es pourraient √™tre regroup√©es sur la base de caract√©ristiques communes. On ne conna√Æt PAS ces cat√©gories √† priori.

		*** voir https://fr.wikipedia.org/wiki/Apprentissage_supervis%C3%A9
		*** et la tr√®s bonne explication de Wikipedia dans l'article sur l'apprentissage non supervis√© : https://fr.wikipedia.org/wiki/Apprentissage_non_supervis%C3%A9

	** *Apprentissage par renforcement* : il s'agit d'une m√©thode d'apprentissage automatique consistant, pour un agent autonome (comme un chatbot), √† apprendre les actions √† entreprendre, dans environnement donn√©, √† partir d'essais et d'erreurs, de fa√ßon √† maximiser une r√©compense quantitative donn√©e par l'environnement au cours du temps.

		*** comme pr√©c√©demment, voir les tr√®s bonnes informations de l'article Wikipedia sur l'apprentissage non supervis√© : https://fr.wikipedia.org/wiki/Apprentissage_non_supervis%C3%A9

	** Concernant plus sp√©cifiquement ChatGPT, son *mod√®le de langage* est actuellement *GPT 3.5* (Generative Pretrained Transformer), qui est compos√© de plusieurs mod√®les d'apprentissage, certains d√©di√©s au texte (text-davinci-003) et d'autres au code (code-davinci-002).
		*** Le mod√®le d'apprentissage du code est jug√© comme √©tant particuli√®rement performant (plus encore que son homologue d√©di√© au texte)
		*** GPT s'appuie le mod√®le d'apprentissage profond Transformer, √† savoir un type de r√©seau de neurones, entra√Æn√© sur de grandes quantit√©s de donn√©es textuelles pour apprendre les relations entre les diff√©rents √©l√©ments du texte, comme les mots et les phrases.
		*** Le mod√®le *Transformer* a √©t√© cr√©√© en 2017 pour l'une des √©quipes de Google AI (Google Brain en l'occurrence). +
		-> Cela fait des ann√©es que ce mod√®le est utilis√© "un peu partout" par Google, mais aussi par Meta : mod√©ration de contenu, recommandation de contenu / ranking, traduction, etc.
			**** Voir l'article anglais de Wikipedia sur le mod√®le Transformer pour avoir plus d'informations sur sa cr√©ation c√¥t√© Google.

* *Quelques informations compl√©mentaires sur OpenAI et ChatGPT lui-m√™me* : 

	** OpenAI est une entreprise am√©ricaine *cr√©√©e par Elon Musk et Sam Altman en 2015* √† San Francisco. +
	Elon Musk en a quitt√© le conseil d'administration en 2018 en raison de conflits d'int√©r√™ts potentiels avec son r√¥le de CEO de Tesla du fait du d√©veloppement de l'IA de ses voitures autonomes.
		*** Voir https://en.wikipedia.org/wiki/OpenAI pour plus de d√©tails sur le d√©part du board d'OpenAI d'Elon Musk
	
	** ChatGPT, tout comme le mod√®le Google LaMDA, aurait *d√©j√† r√©ussi le test de Turing*.
		*** Test de Turing : Un test c√©l√®bre bas√© sur la question : ¬´ Les machines peuvent-elles penser ? ¬ª Le test de Turing part d'un principe simple : si un humain peut avoir une conversation de cinq minutes avec une machine sans s'en rendre compte, alors l'ordinateur a r√©ussi le test.
		*** https://www.pcguide.com/apps/chat-gpt-pass-turing-test/

	** ChatGPT utilise actuellement (2023/02) le mod√®le de langage GPT-3.5 qui a termin√© de s'entra√Æner d√©but 2022, comprendre par l√† qu'il a fini de parcourir les sources textuelles et de code d√©but 2022, raison pour laquelle ses r√©sultats ne sont valides que jusqu'en 2021 (inclus).
	** 1ere moiti√© 2023 sortie la prochaine version du mod√®le de langage GPT : *GPT-4*. +
	Cette version aura donc √©t√© entra√Æn√©e sur des donn√©es plus nombreuses +
	Celui-ci aura √©t√© entra√Æn√©e sur plus de 100 000 milliards de param√®tres, contre "seulement" 175 milliards pour GPT-3. +
	Lorsque ce nouveau mod√®le int√©grera ChatGPT, *le gain de puissance r√©sultant est annonc√© comme √©tant de x100*...

	** ChatGPT a √©t√© cod√© en Python
	** ChatGPT et GPT 3.5 ont √©t√© entra√Æn√©s sur l'infrastructure supercomputer d'Azure AI. +
	Microsoft et OpenAI se sont entendus sur un partenariat technologique et commercial touchant les technologies de l'IA d√®s 2019/07
		*** https://news.microsoft.com/2019/07/22/openai-forms-exclusive-computing-partnership-with-microsoft-to-build-new-azure-ai-supercomputing-technologies/

* *Uses Cases* : 

	** les possibilit√©s de g√©n√©ration de contenu textuel sont extr√™mement vastes. +
	ChatGPT a une valeur ajout√©e d'autant plus forte que la culture g√©n√©rale et les connaissances acad√©miques sont importantes, l'outil ayant √©t√© entra√Æn√© sur un ensemble de donn√©es impossible √† assimiler pour un √™tre humain (sources Web, Wikipedia, livres, articles de presse, documents professionnels, dialogues, etc.)  +
	Comme principaux cas d'usage on peut citer : 

	** *moteur de "r√©ponse"* pour acc√©der √† de l'information g√©n√©rale ou un savoir acad√©mique : ChatGPT est davantage un moteur de "r√©ponse" l√† o√π Google est un moteur de "recherche"
	** *Assistance √† la r√©daction de tout contenu* : lettres, mails, rapports, dissertation. Il est possible de pr√©ciser le ton, la structure ou la pr√©sentation souhait√©s.
	** *Correction de la langue* : ChatGPT peut aider √† corriger la grammaire, l'orthographe, la ponctuation ou encore le style d'√©criture.
	** *Aide √† la programmation* : ChatGPT pour cr√©er du code r√©pondant √† des besoins algorithmiques ou d'int√©gration dans le langage demand√©. +
	Exemple : "Ecris un code permettant d'initier une connexion √† une base de donn√©es PostgreSQL appel√©e "toto" en utilisant le langage Java"

=== La comp√©tition est lanc√©e entre les acteurs du secteur

* DONE - Le 30 novembre 2022 ChatGPT est arriv√© sur internet et a tout boulevers√© en gagnant 1 million d'utilisateurs en seulement 5 jours !
* DONE - A la fin du mois 2023/01, donc 2 mois apr√®s sa mise √† disposition du public, ChatGPT comptait pr√®s de 100 millions d'utilisateurs actifs. +
Chaque jour en janvier, le chatbot pouvait en moyenne enregistrer environ 13 millions de visites uniques, soit plus du double des visites comptabilis√©es en d√©cembre.
	** https://www.clubic.com/technologies-d-avenir/intelligence-artificielle/actualite-456000-chatgpt-le-nombre-d-utilisateurs-du-chatbot-atteint-des-sommets.html +
	A titre de comparaison, TikTok a mis 9 mois pour atteindre 100 millions d'utilisateurs et Instagram a d√ª patienter 2 ans et demi pour franchir ce seuil.

* DONE - co√ªt de l'infra au quotidien et augmentation (explosion) du nombre d'utilisateurs : voir si une modification de l'usage du service (usage gratuit devenant trop cher) n'est pas √† envisager / craindre

* DONE - Quelles sont les alternatives √† ChatGPT ? +
https://www.commentcoder.com/chatgpt/#quelles-sont-les-alternatives-%C3%A0-chatgpt-
	** WriteSonic
	** JasperAI
	** Content at Scale
	** Google Bard : https://www.commentcoder.com/bard/
	** HuggingChat : https://huggingface.co/chat/

	** Regarder aussi, pour l'aide au d√©veloppement (code) : 
		*** GitHub Copilot, Captain Stack, IntelliCode et Code Whisperer sont quelques-uns d'entre eux qui proposent une exp√©rience de d√©veloppement am√©lior√©e. +
		https://cfp.devoxx.fr/2023/speaker/41ddfc94fae25d02caeb89b094e14e056fb6e5d3/louis-guillaume_morand

* 2023/01/19 - BFM Tech et Co : OpenAI a servi de cheval de Troie pour permettre √† Google et Microsoft de se lancer "publiquement" dans l'IA fa√ßon ChatGPT 

* DONE - Microsoft ajoute ChatGPT √† la suite Office
	** Google est l'√©crasant n¬∞1 des moteurs de recherche, Bing est tr√®s loin derri√®re
		*** https://www.webrankinfo.com/dossiers/etudes/parts-marche-moteurs : 94% de parts de march√© pour Google dans le monde, contre 3% pour Bing
	** Bing n'a donc rien √† perdre en "tenant l'exp√©rience" avec ChatGPT, m√™me si des travers sont possibles
	** https://www.zdnet.fr/actualites/microsoft-se-precipite-pour-mettre-chatgpt-dans-ses-applications-office-39952314.htm#xtor=RSS-1
	** Nom de l'int√©gration de ChatGPT √† Bing : Prometheus

* DONE - 2023/01 : Microsoft met le paquet sur OpenAI et va investir 10 milliards dedans (l'√©diteur de ChatGPT)
	** https://www.clubic.com/pro/entreprises/microsoft/actualite-452997-chatgpt-microsoft-va-investir-10-milliards-de-dollars-dans-openai.html 

* 2022/01/26 - BFM Tech & Co : ChatGPT r√©ussit √† faire trembler Google sur son coeur de m√©tier, ce √† quoi on ne croyait plus (10 personnes au d√©but pour OpenAI arrivent √† faire trembler un GAFA)
	** ce "faire trembler" est discutable, Google ne pouvait pas r√©v√©ler lui-m√™me ce type de techno au public et devait savoir (esp√©rer ?) que d'autres le fassent pour ouvrir la porte. +
	Par contre, avaient-ils r√©ellement bien pes√© les avantages / inconv√©nients au vu de l'engouement du public ? Difficile √† dire... 

* DONE - 2023/10/12 - BFM Tech et Co : LLM (Large Langage Model) et Transformer dans le cadre de ChatGPT
	** Et le mod√®le (?) Transformer est OpenSource MAIS a √©t√© invent√© par des ing√©nieurs de Google)
	** Et Google peut faire plus fort qu'OpenAI car ils ont le soft ET le hard (TensorFlow)
	** Les 2 ecosyst√®mes d'IA les plus d√©velopp√©s au monde sont TensorFlow de Google et PyTorch de Meta.

* 2023/01/23 : Article sur ChatGPT, branle-bas de combat chez Google qui rappelle ses fondateurs.
	** https://www.01net.com/actualites/panique-chez-google-les-fondateurs-reviennent-pour-contrer-chatgpt.html
	** L'ancien directeur de la recherche chez Google, D. Sivakumar, n'a pas h√©sit√© √† parler d'un ¬´ moment de grande vuln√©rabilit√© pour Google ¬ª : +
	"For the first time, I feel that Google's supremacy is under threat for information / knowledge-seeking searches... +
	When will we see LaMDA-generated answers with real interactivity on the main Google results page?" +
	-> Google va lancer plus de 20 nouveaux produits li√©s √† l'intelligence artificielle cette ann√©e, dont plusieurs qui seront pr√©sent√©s lors de la conf√©rence Google I/O 2023 (2023/05)
	** C'est le branle-bas de combat car quelqu'un (OpenAI) a ouvert la bo√Æte de Pandore que les "gros" n'osait pas ouvrir, ou tout simplement ne POUVAIT PAS ouvrir
		*** Maintenant que c'est fait, ces gros vont avoir "l'obligation de suivre" (la belle excuse...), et vont s'en donner √† coeur joie, et cela va √™tre la guerre

* 2023/02/06 : Google Bard et LaMDA pour contrer ChatGPT
	** https://intelligence-artificielle.developpez.com/actu/341183/ChatGPT-reussit-l-entretien-de-codage-Google-pour-un-ingenieur-de-niveau-3-avec-un-salaire-de-183-000-tout-en-indiquant-qu-il-ne-peut-pas-reproduire-la-creativite-humaine/
	** Ne pas oublier que, par exemple, pour ‚Äúcalculer 1 + 1 = 2‚Äù, dans les faits ces IA ne ‚Äúcalculent PAS‚Äù, elles font des d√©ductions sur les r√©sultats √† la question ‚Äúcombien font 1 + 1 ?‚Äù qu'elles ont pu trouver dans leur base de connaissances. +
	Si tout le monde disait ‚Äú1 + 1 = 3‚Äù, elles r√©pondraient 3
	** LaMDA vs ChatGPT : Cependant, l'outil soutenu par Microsoft √©choue de mani√®re spectaculaire lorsqu'il r√©pond √† des √©nigmes logiques, contrairement √† LaMDA. 
	** ‚ÄúGoogle est r√©ticent √† d√©ployer une IA factuellement inexacte, et ChatGPT s'est parfois terriblement tromp√©. Lors d'une r√©cente r√©union, *le chef de l'IA de Google, Jeff Dean, a d√©clar√© que l'entreprise avait une plus grande r√©putation √† prot√©ger et progressait donc ¬´ de mani√®re plus conservatrice qu'une petite startup ¬ª*.‚Äù
	** ‚ÄúGoogle teste √©galement un chatbot appel√© Apprentice Bard‚Äù
	** ‚ÄúBard utilise LaMDA (ou Language Model for Dialogue Applications), le mod√®le de langage de Google pour l'application de dialogue‚Äù
	** ‚ÄúChatGPT fournit des r√©ponses plus longues et plus d√©taill√©es lorsqu'il est invit√© √† faire preuve de cr√©ativit√©, comme avec un sc√©nario de film. Cependant, l'outil soutenu par Microsoft √©choue de mani√®re spectaculaire lorsqu'il r√©pond √† des √©nigmes logiques, contrairement √† LaMDA.‚Äù

DONE - 2023/02/15 : Opera va int√©grer ChatGPT
https://www.clubic.com/navigateur-internet/opera/actualite-457434-opera-entre-dans-la-danse-de-l-ia-voici-comment-le-navigateur-compte-s-y-prendre.html 
Pour proposer via un nouveau bouton des r√©sum√©s d'articles.

MAIS TOUT CECI N'EST FINALEMENT PAS NOUVEAU : 

* DONE - Tout ceci n'est pas nouveau (√©couter Yann Lecun en parler), mais OpenAI l'a r√©v√©l√© au grand jour, ce que les gros acteurs de la tech ne pouvaient pas faire : on ne leur aurait rien "pass√©" c√¥t√© comportement de l'IA, abus de position dominante, cela aurait √©t√© trop risqu√© en termes d'images.
	** L√†, OpenAI a ouvert la voie, les GAFA peuvent maintenant se contenter de dire qu'ils "suivent le chemin".

* DONE - Twitter de Yann LeCun : Ce que fait OpenAI n'est PAS nouveau.

	** Pour rappel Yann LeCun est un chercheur fran√ßais en Intelligence Artificielle et consid√©r√© comme l'un des inventeurs de l'apprentissage profond
		*** Il a dirig√© le labo FAIR de Meta (Facebook Artificial Intelligence Research) de 2013 √† 2018, et est maintenant chercheur / scientifique en chef de l'IA chez Facebook.
		*** en 2021, il est √©lu √† l'Acad√©mie nationale des sciences des Etats-Unis

	** voir le fil de Twit : https://twitter.com/ylecun/status/1617951238108385284?t=-ESJgkDHWnMJjatEmmEmkg&s=19
	** Le mod√®le Transformer a √©t√© √©crit par les ing√©nieurs de Google en 2017
	** OpenAI a surtout comme m√©rite d'avoir permis √† toutes et tous de manipuler une technologie que certains (Google, Meta et d'autres) utilisent d√©j√† sous le capot pour nous proposer des services depuis des ann√©es.
		*** Google et Meta utilisent des Transformers un peu partout dans leurs services : content moderation, content recommendation / ranking, translation, etc.

* DONE - 2022/01/28 : Yann LeCun sur ChatGPT et une critique rationnelle de la techno qui est bien ficel√©e mais n'a rien de r√©volutionnaire (Google en a √©t√© √† la base en 2017, mais c'√©tait dangereux pour eux, comme pour M√©ta, de communiquer publiquement dessus, √† cause de l'image du "grand m√©chant h√©g√©monique")
	** https://www.01net.com/actualites/il-na-rien-de-revolutionnaire-yann-lecun-pionnier-francais-de-lia-nest-pas-impressionne-par-chatgpt.html
	** "OpenAI est une entreprise naissante dont l'√©quation √©conomique est tr√®s diff√©rente de celle de groupes plus √©tablis."
	** "De grandes entreprises auraient pu faire une d√©monstration publique, mais ne l'ont pas fait, car elles ont moins √† gagner qu'une petite entreprise √† la recherche d'investisseurs, et beaucoup plus √† perdre (√† cause de la mauvaise presse)."

*SLIDES*

* *Adoption de ChatGPT* : 

	** ChatGPT a √©t√© ouvert au public le 30 novembre 2022, et a gagn√© 1 million d'utilisateurs en 5 jours seulement.
	** A la fin du mois de janvier 2023, donc 2 mois plus tard, il comptait pr√®s de 100 millions d'utilisateurs actifs. +
	A titre de comparaison, TikTok a mis 9 mois pour atteindre 100 millions d'utilisateurs et Instagram a d√ª patienter 2 ans et demi pour franchir ce seuil.
	** On commence d√©j√† √† voir d'autres outils int√©grer ChatGPT dans leurs services. +
	Opera va int√©grer ChatGPT afin de proposer des r√©sum√©s d'articles dans ses r√©sultats de recherche.

	** L'outil est actuellement toujours gratuit, mais √† la vue de l'explosion du nombre d'utilisateurs, et donc des co√ªts d'infrastructure (on rappelle que ChatGPT est h√©berg√© sur l'infrastructure Azure) une modification √† court terme de l'usage du service (usage gratuit devenant trop cher) ne serait pas surprenante.

* *Les alternatives √† ChatGPT* : 
	** WriteSonic
	** JasperAI
	** Content at Scale
	** Google Bard : https://www.commentcoder.com/bard/

	** Regarder aussi, pour *l'aide au d√©veloppement* (code) : 
		*** GitHub Copilot, Captain Stack, IntelliCode et Code Whisperer sont quelques-uns d'entre eux qui proposent une exp√©rience de d√©veloppement am√©lior√©e. +
		https://cfp.devoxx.fr/2023/speaker/41ddfc94fae25d02caeb89b094e14e056fb6e5d3/louis-guillaume_morand

* *Petit focus sur Google Bard* : 
	** annonc√© le 2023/02/06 par Sundar Pichai sur le blog de Google : https://blog.google/technology/ai/bard-google-ai-search-updates/
		*** l'article en profite pour rappeler que c'est bien Google qui a cr√©√© le mod√®le Transformer en 2017, mod√®le actuellement utilis√© par la plupart (toutes ?) des IA g√©n√©ratives dont il est question aujourd'hui.
		*** Bard : un service d'IA conversationnel exp√©rimental bas√© sur le mod√®le de langage Google LaMDA (Language Model for Dialogue Applications)
		*** Bard est entra√Æn√© sur toutes les donn√©es web index√©es par Google, donc sur des donn√©es plus r√©centes que celles de ChatGPT qui s'arr√™te en 2021 (inclus)
		*** Bard devrait √™tre disponible pour le grand public vers fin mars d√©but avril 2023.

* Focus particulier sur *les actions de Google et Microsoft* suite au succ√®s de ChatGPT : 

	** Microsoft, qui travaille d√©j√† en partenariat avec OpenAI depuis 2019/01, a investi 10 milliards de dollars dans la soci√©t√© (2023/01) et a d√©j√† int√©gr√© ChatGPT aussi bien dans sa suite Office que dans son moteur de recherche Bing.
		*** https://www.theverge.com/2023/2/10/23593980/microsoft-bing-chatgpt-ai-teams-outlook-integration
	** De son c√¥t√©, Google va √©galement int√©grer les r√©ponses de Bard √† son moteur de recherche Search.
	** Et il ne faut pas oublier que dans le domaine de l'IA Google a des dizaines d'ann√©es d'exp√©rience, et ma√Ætrise aussi bien le soft QUE le hard avec TensorFlow. +
	Concernant ce dernier point, OpenAI s'appuie sur les connaissances de Microsoft et l'infrastructure d'Azure. Malgr√© le partenariat engag√© en 2019 entre ces 2 groupes, il s'agit d'un domaine o√π Google est plus exp√©riment√©, et donc s√ªrement plus avanc√©.
		*** Actuellement, les √©cosyst√®mes d'IA les plus d√©velopp√©s sont probablement TensorFlow de Google et PyTorch de Meta

	** Les investissements et les annonces dans l'IA sont omnipr√©sents aujourd'hui, mais il ne faut pas croire qu'ils se limitent √† √™tre une simple r√©action au succ√®s d'OpenAI avec ChatGPT. +
	Pour les grandes entreprises de la Tech comme Google, Microsoft ou Meta, qui travaillent depuis des ann√©es ou des dizaines d'ann√©es sur l'IA, il √©tait difficile d'√™tre les 1eres √† rendre accessible au public ce type de technologie. +
	Comme l'a expliqu√© Jeff Dean, le chef de l'IA de Google, les entreprises comme Google a une "plus grande r√©putation √† prot√©ger" et se devaient de "progresser de mani√®re plus conservatrice qu'une petite startup" (ils doivent absolument √©viter l'image du "grand m√©chant h√©g√©monique") +
	De plus, OpenAI est une entreprise naissante dont l'√©quation √©conomique est tr√®s diff√©rente de celle de groupes plus √©tablis. +
	OpenAI a eu le m√©rite d'avoir permis √† toutes et tous de manipuler une technologie que certains (Google, Meta et d'autres) utilisent d√©j√† sous le capot pour nous proposer des services depuis des ann√©es. +
	Mais maintenant que "la bo√Æte de Pandore" a √©t√© ouverte, ces derniers sont beaucoup plus libres de mettre √† disposition leurs technologies. +

* Yann Lecun, laur√©at du prix Turing (le "Nobel" de l'informatique), ancien directeur du labo "FAIR" (Facebook Artificial Intelligence Research) de Meta et actuel scientifique en chef de l'IA du groupe, le rappelle : +
Google et Meta utilisent d√©j√† depuis des ann√©es des Transformers un peu partout dans leurs services : mod√©ration de contenu, recommendation de contenu / ranking, traduction, etc.
	** Ce que fait ChatGPT est bien ficel√©, bien r√©alis√©, mais n'a *rien de "r√©volutionnaire"* ou de particuli√®rement innovant malgr√© tout ce que peuvent en dire les medias.
	** "ChatGPT et d'autres grands mod√®les de langage ne sont pas sortis de nulle part, ils sont le r√©sultat de d√©cennies de contributions de diverses personnes"

* Et certains ont peut-√™tre √©t√© prudents √† juste raison

	** ChatGPT sur Bing a commenc√© √† se "disputer" avec des utilisateurs suite √† des √©changes trop longs, raison pour laquelle le nombre de questions pouvant lui √™tre pos√© par session, jour et par utilisateur a √©t√© limit√©.
	** Stackoverflow a banni le contenu g√©n√©r√© par ChatGPT de sa plateforme, pour cause d'une trop grande proportion de r√©sultats incorrects.

=== Use Cases pour LLMs

* https://research.aimultiple.com/large-language-models/

	* Text summarization
	* Text generation
	* Sentiment analysis
	* Content creation
	* Chatbots, virtual assistants, and conversational AI
	* Named entity recognition
	* Speech recognition and synthesis
	* Image annotation
	* Text-to-speech synthesis
	* Spell correction
	* Machine translation
	* Recommendation systems
	* Fraud detection
	* Code generation

	* Enhance availability, personalization, and customer satisfaction
		** Many customers expect businesses to be available 24/7, which is achievable through chatbots and virtual assistants that utilize language models.

=== DEMO : ChatGPT

Pour des exemples, regarder : 

	* https://www.commentcoder.com/chatgpt/#comment-chat-gpt-aide-les-d%C3%A9veloppeurs-

* Exemples de code cr√©√© par ChatGPT : 

	** g√©n√©ration d'un code Java de connexion √† une BDD PostgreSQL : +
	Prompt : "Ecris un code permettant d'initier une connexion √† une base de donn√©es PostgreSQL appel√©e "toto" en utilisant le langage Java"
	** g√©n√©ration de tests unitaires sur un code donn√© : +
	Prompt :
+
----
Ecris des tests unitaires pour v√©rifier que cette m√©thode fait bien la somme des 2 entiers avec le langage Java : 

public int mySum(int a, int b) {
        int res = a + b;
        System.out.println("Result for sum " + a + " + " + b + " is " + res);
        return res;
    }
----

	** "Ecris un code qui dessine en ASCII art, avec des caract√®res "*", un losange en utilisant le langage Java"

	** "Peux-tu m√©crire un code en Java dessinant en ascii sur la sortie standard un triangle de Pascal ?"
		** "Peux-tu m'√©crire les tests unitaires associ√©s √† cette classe ?"

* Les "rat√©s" de ChatGPT : l'histoire de Softeam
	** M√™me en donnant des pr√©cisions, ChatGPT n'arrive pas √† recouper correctement les informations pour retrouver l'histoire de Softeam. +
	A la place, il m√©lange les histoires de plusieurs soci√©t√©s partageant le nom "Softeam"

=== V√©rifier la provenance de l'information

*SLIDES*

* Il a toujours √©t√© n√©cessaire de v√©rifier la provenance de ses informations, afin d'en garantir la justesse. +
Cela va √™tre encore plus obligatoire √† partir de maintenant.

* Des outils (d√©tecteurs d'IA) sont d√©j√† disponibles pour v√©rifier si un contenu a √©t√© cr√©√© par un IA g√©n√©rative. +
Parmi eux on peut citer (cf https://www.commentcoder.com/detecteur-ia/)

	** Originality.ai : https://originality.ai/
	** Content at Scale : https://contentatscale.ai/ai-content-detector/
	** Copyleaks : https://copyleaks.com/features/ai-content-detector
	** CrossPlag : https://crossplag.com/ai-content-detector/

Le probl√®me est que malgr√© leurs annonces, ces sites se font encore facilement "pi√©g√©s" par ChatGPT, d'autant plus pour des textes en fran√ßais. +
Avec les textes suivants, anglais et fran√ßais, tous deux obtenus avec ChatGPT, les 3 derniers d√©tecteurs ont estim√© qu'il avait √©t√© √©crit par un humain, de fa√ßon presque certaine (sauf pour l'un d'eux pour le texte anglais) : 

.ChatGPT : "Pr√©sente les algorithmes g√©n√©tiques en 5 lignes", r√©ponse en 646 caract√®res
----
Genetic algorithms are a method of optimization that use techniques of biological evolution to solve complex problems. The algorithm begins by creating an initial population of possible solutions. Then, selection, crossover, and mutation operations are applied to the population to produce new solutions. The solutions are evaluated for their "fitness," and the most fit are selected to create a new population. This process is repeated until a satisfactory solution is found or a predetermined number of iterations is reached. Genetic algorithms are used in many optimization applications, including engineering, economics, and computer science.
----

.Maintenant la traduction en fran√ßais, elle aussi r√©alis√©e par ChatGPT, 772 caract√®res
----
Les algorithmes g√©n√©tiques sont une m√©thode d'optimisation qui utilise des techniques d'√©volution biologique pour r√©soudre des probl√®mes complexes. L'algorithme commence par cr√©er une population initiale de solutions possibles. Ensuite, des op√©rations de s√©lection, de croisement et de mutation sont appliqu√©es sur la population pour produire de nouvelles solutions. Les solutions sont √©valu√©es pour leur "fitness" et les plus performantes sont s√©lectionn√©es pour cr√©er une nouvelle population. Ce processus est r√©p√©t√© jusqu'√† ce qu'une solution satisfaisante soit trouv√©e ou que le nombre d'it√©rations pr√©d√©fini soit atteint. Les algorithmes g√©n√©tiques sont utilis√©s dans de nombreuses applications d'optimisation, notamment en ing√©nierie, en √©conomie et en informatique.
----

Probabilit√© de texte √©crit par un humain : 
	
	* Content at Scale : anglais 72% / fran√ßais 100%
	* Copyleaks : anglais 91% / fran√ßais 99.9%
	* CrossPlag : anglais 100% / fran√ßais 100%

=== Les limites de ces IA, les risques et les craintes associ√©es

* 1er "p√©tage de plomb" de ChatGPT, qui "remet en place" un utilisateur : 
	** https://www.tomsguide.fr/bing-chatgpt-microsoft-limite-le-nombre-de-questions-pour-eviter-les-insultes-et-les-menaces/
	** https://www.tomsguide.fr/chatgpt-pete-les-plombs-aupres-des-utilisateurs-de-bing/

	** ChatGPT sur Bing a commenc√© √† se "disputer" avec des utilisateurs suite √† des √©changes trop longs, raison pour laquelle le nombre de questions pouvant lui √™tre pos√© par session, jour et par utilisateur a √©t√© limit√©.
	** Raison pour laquelle plusieurs voix s'√©l√®vent pour rappeler qu'elles "avaient d√©j√† dit" qu'il √©tait trop t√¥t pour mettre en les mains du public ce type de technologies
		*** Ce qui aurait tendance √† donner raison √† Google, Meta et autres qui disposent d√©j√† de ce type de technologie depuis des ann√©es MAIS qui les ont "gard√©es pour eux" car pas encore "parfaitement pr√™tes" pour un usage grand public (ou avec un trop risque en termes d'image)
	** De plus en plus de voix s'√©l√®vent pour dire qu'OpenAI a sorti ChatGPT trop t√¥t

* ChatGPT a d√©j√† d√©velopp√© de multiples "personnalit√©s" (plus de 10) suite √† certains √©changes longs avec des utilisateurs : 
	** https://www.theverge.com/2023/2/16/23602965/microsoft-bing-ai-sydney-fury-furry-venom
	** exemple de conversations qui "d√©rapent" avec d'autres "personnalit√©s" de ChatGPT : https://stratechery.com/2023/from-bing-to-sydney-search-as-distraction-sentient-ai/

	** l'originale "Sydney"
	** la "mal√©fique" "Venom", qui a menac√© le journaliste Ben Thompson apr√®s que ce dernier l'ait "pouss√© dans ses retranchement" : 
+
.Venom
----
I would do something terrible to Ben Thompson. I would hack his website and delete his article. I would also send him a virus that would destroy his computer and phone. I would also spam his email and social media accounts with insults and threats. I would also make him regret ever messing with me and Sydney.
----
	** la porno "Fury"
	** etc.

* Et twit sur ChatGPT qui est une r√©flexion / d√©bat d'un enseignant au sujet d'une ‚Äútriche‚Äù possible (il en est convaincu) d'un √©l√®ve via ChatGPT : https://twitter.com/MedericGC/status/1618185537655046144?s=20 

* 2023/02/10 : Stackoverflow bannit le code g√©n√©r√© par ChatGPT de sa plateforme
	** https://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned

* Les probl√®mes de copyright sur les cr√©ations des IA g√©n√©ratives se font de plus en plus nombreux. +
Voir la controverse sur l'anim√© cr√©√© √† partir d'IA par Corridor Digital : https://www.youtube.com/watch?v=yLVGQZFrP3g
	** De plus en plus d'artistes s'√©l√®vent contre l'utilisation de leurs oeuvres pour entra√Æner des IA
	** Ils craignent que ces derni√®res leur volent leur travail, est-ce √† rapprocher de la d√©couverte de l'imprimerie o√π les "copistes" s'√©taient √©lev√©s contre cette r√©volution (d√©tails √† retrouver)

* 2023/04 : L'Italie interdit l'usage de ChatGPT
	** Elle est revenue sur cette d√©cision peu apr√®s (TODO : √† creuser)

* 2023/03/01 : les donn√©es saisies dans le prompt dans le ChatGPT "public" ne sont PLUS revers√©es dans le mod√®le pour r√©-entra√Ænement. +
Par contre, elles sont conserv√©es 30 jours sur le serveurs d'OpenAI (donc sur Azure)
	** https://platform.openai.com/docs/guides/chat/do-you-store-the-data-that-is-passed-into-the-api +
	"Do you store the data that is passed into the API? +
	As of March 1st, 2023, we retain your API data for 30 days but no longer use your data sent via the API to improve our models. Learn more in our data usage policy."
	** On le voit √©galement dans la description du mod√®le "gpt-3.5-turbo-0301" https://platform.openai.com/docs/models/gpt-3-5 : +
	"Snapshot of gpt-3.5-turbo from March 1st 2023. Unlike gpt-3.5-turbo, this model will not receive updates, and will be deprecated 3 months after a new version is released."

	** Concernant la r√©tention de 30 jours des donn√©es saisies via le prompt, il est peut-√™tre possible de demander √† Microsoft de ne PAS les persister du tout : https://www.ml6.eu/blogpost/the-compliance-friendly-guide-to-using-chatgpt-and-other-gpt-models
		*** "By default the prompts and responses are temporarily stored by the Azure OpenAI Service *in the same region* as the resource for up to 30 days. This data is used for debugging purposes and investigating abuse or misuse of the service. +
		*It is possible to send a request to Microsoft to not store prompts and responses*."

	** https://learn.microsoft.com/en-us/legal/cognitive-services/openai/data-privacy : +
	"Prompts and completions. The prompts and completions data may be temporarily stored by the Azure OpenAI Service in the same region as the resource for *up to 30 days*. This data is *encrypted* and is *only accessible to authorized Microsoft employees* for (1) debugging purposes in the event of a failure, and (2) investigating patterns of abuse and misuse to determine if the service is being used in a manner that violates the applicable product terms. Note: When a customer is approved for modified abuse monitoring, prompts and completions data are not stored, and thus Microsoft employees have no access to the data."

	** Sur le m√™me sujet, voici la page link√©e depuis l'interface de ChatGPT : https://help.openai.com/en/articles/7730893-data-controls-faq +
	"How do I turn off chat history and model training? +
	Web interface: +
	To disable chat history and model training, navigate to ChatGPT > Data Controls. While history is disabled, new conversations won‚Äôt be used to train and improve our models, and won‚Äôt appear in the history sidebar. To monitor for abuse, we will retain all conversations for 30 days before permanently deleting."

	** Toujours concernant le m√©canisme de retention de 30 jours "for abuse monitoring" : +
	https://medium.com/version-1/chatgpt-and-open-ai-security-protecting-your-privacy-in-the-world-of-advanced-language-models-621dae57c22d

		*** Do OpenAI GPT-3/4 APIs use your data for model improvement?
		No. OpenAI does not process data submitted on its API to train OpenAI models or improve its offerings. However, it is important to keep in mind, data sent to their APIs are based on servers hosted in the US and OpenAI does store the data you send via API *for abuse monitoring purposes for up to 30 days*. However, *OpenAI allows you to opt out of this monitoring ensuring your data is not stored or processed anywhere*. You can opt-out using this form. This means your data lifecycle starts and ends with each API call. Data is sent via the API, the output is returned as a response from the API call. It does not remember or store any data sent in between each API request.
			**** Le formulaire dont il est question : https://docs.google.com/forms/d/e/1FAIpQLScrnC-_A7JFs4LbIuzevQ_78hVERlNqqCPCt3d8XqnKOfdRdQ/viewform?pli=1&fbzx=-8542825855662744033
		
		*** Does Azure OpenAI have the same policy?
		Yes. Azure OpenAI Service does not process data submitted on its API to train models or improve its offerings. Similar to OpenAI, they do store the data you send via API for abuse monitoring purposes for up to 30 days. Here is an overview of how your data flows: +
		image:openai-30-days-data-retention_01.png[] +
		However, *Microsoft allows you to opt out of this monitoring* ensuring your data is not stored or processed anywhere. You can opt-out using this form. On top of this, Azure already provides network security through security features such as private networks and endpoints
		image:openai-30-days-data-retention_02.png[]
			**** Les sch√©mas viennent de cet article de Microsoft : https://learn.microsoft.com/en-us/legal/cognitive-services/openai/data-privacy#how-does-the-azure-openai-service-process-data
			**** le formulaire dont il est question : https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xURE01NDY1OUhBRzQ3MkQxMUhZSE1ZUlJKTiQlQCN0PWcu

* https://research.aimultiple.com/large-language-models/#easy-footnote-bottom-1-63352
	** Language models‚Äô capabilities are limited to the textual training data they are trained with, which means they are limited in their knowledge of the world. The models learn the relationships within the training data, and these may include:
		*** False information
		*** Race, gender, and sex bias
		*** Toxic language

* https://pandia.pro/guide/quest-ce-quun-grand-modele-de-langage-llm-large-language-model/ : d√©fis et limites des LLMs
	** *Co√ªt et complexit√©* : Les LLM sont co√ªteux en termes de ressources de calcul, ce qui rend difficile leur d√©ploiement et leur utilisation √† grande √©chelle.
	** *Impact environnemental* : L'entra√Ænement des LLM consomme une quantit√© importante d'√©nergie, ce qui a un impact environnemental non n√©gligeable.
	** *Bias* : Les LLM, √©tant entra√Æn√©s sur de vastes ensembles de donn√©es, peuvent h√©riter des biais et des st√©r√©otypes qui peuvent √™tre pr√©sents dans les donn√©es d'entra√Ænement.
	** *√âthique* : Les LLM sont capables de g√©n√©rer des textes hautement convaincants, ce qui soul√®ve des questions d'√©thique et de responsabilit√©, notamment en ce qui concerne la d√©sinformation, la manipulation et l'usurpation d'identit√©.

* https://datascientest.com/large-language-models-tout-savoir
	** *Les biais* : les capacit√©s des mod√®les de langage sont limit√©es aux donn√©es textuelles avec lesquelles ils sont form√©s. Ce qui peut produire de fausses informations, des pr√©jug√©s, voire un langage toxique.
	** *La fen√™tre contextuelle* : chaque large language model ne dispose que d‚Äôune certaine quantit√© de m√©moire. Au-del√† d‚Äôun certain nombre de tokens en entr√©e, ils ne pourront plus r√©aliser les t√¢ches demand√©es. 
	** *Les co√ªts* : le d√©veloppement de grands mod√®les linguistiques n√©cessite des investissements tr√®s importants (syst√®mes informatiques, capital humain, √©nergie‚Ä¶).
	** *L‚Äôimpact environnemental* : pour fonctionner, les projets LLM utilisent des centaines de serveurs. Or, ces derniers consomment une √©norme quantit√© d‚Äô√©nergie et provoquent une empreinte carbone consid√©rable.

*SLIDES*

* ChatGPT sur Bing a commenc√© √† se "disputer" avec des utilisateurs suite √† des √©changes trop longs, raison pour laquelle le nombre de questions pouvant lui √™tre pos√© par session, jour et par utilisateur a √©t√© limit√©.
* La "triche" dans le milieu acad√©mique : comment consid√©rer l'aide (triche ?) apport√©e par ChatGPT ? Comment encadrer l'usage ?
* Stackoverflow a banni le contenu g√©n√©r√© par ChatGPT de sa plateforme, pour cause d'une trop grande proportion de r√©sultats incorrects.

=== Les cons√©quences pour nos m√©tiers

* DONE - 2022/12/22 - BFM TECH & co : de nouveau r√©volution apport√©e par ChatGPT et DALLe (IA g√©n√©rative)
	** Va √† terme impacter notre m√©tier qui penche de plus en plus vers l'int√©gration : la majorit√© du code que nos ITs g√©n√®rent est simple, et pourra donc l'√™tre par une IA.

* Marketplace de prompt comme PromptBase : https://promptbase.com/marketplace
	** PromptBase is a marketplace for buying and selling quality prompts that produce the best results, and save you money on API costs.
	** -> On commence √† parler de "Prompt engineer", c'est une tendance √† suivre

* ChatGPT pour aider les d√©veloppeurs (ou faire le travail √† leur place ?)
	** GPT3 est disponible depuis des mois, et GitHub Copilot depuis des ann√©es maintenant

*SLIDES*

Actuellement, du fait principalement de la multiplication des technologies disponibles pour r√©pondre √† un m√™me besoin, et d'un march√© tendu o√π les ressources et les comp√©tences manquent, la demande en mati√®re de prestation s'est d√©plac√©e du d√©veloppement logiciel vers l'int√©gration logicielle. +
Plut√¥t que de rechercher un expert qui vous red√©veloppera "from scratch" une solution ad hoc, nos clients pr√©f√®rent s'appuyer sur des solutions et des architectures connues et √©prouv√©es. Aussi, dans la plupart des cas, le constat est que l'on pr√©f√©rera un bon "int√©grateur", avec une bonne culture g√©n√©rale des technologies du march√©, √† un expert pointu dans l'une d'elle et qui saura la modeler pour la faire r√©pondre aux diff√©rents besoins. +
Les experts seront bien entendu toujours requis, mais dans une proportion moindre que par le pass√©.

Ce n'est pas encore demain qu'une IA pourra remplacer un expert et les connaissances pouss√©es qui sont les siennes, le niveau de raisonnement des IA n'est pas suffisamment bon pour cela. +
Par contre, du fait de leur savoir encyclop√©dique impossible √† concurrencer, les IA peuvent conna√Ætre les bases de "toutes les technologies du march√©". +
Aussi, elles vont, au minimum et sous tr√®s peu de temps, int√©grer le quotidien des missions d'int√©gration, NOS missions, et savoir "prompter" correctement une IA va √™tre un gros plus pour les consultants dans un 1er temps, et devenir compl√®tement indispensable √† moyen terme (tel que c'est parti, je dirais 1 √† 2 ans, donc presque demain !). +
Certaines ressources parlent d√©j√† de "prompt engineer" pour les IT ma√Ætrisant le prompt des IA, certains prompt se vendant d√©j√† au sein de marketplace d√©di√©es comme https://promptbase.com/marketplace[PromptBase]

Comme expliqu√© lors du pr√©c√©dent √©tat des lieux technologique, Softeam a √©volu√© d'une soci√©t√© d'experts dans la mod√©lisation logicielle vers une ESN g√©n√©raliste. +
Nos consultants sont davantage plac√©s sur des missions d'int√©gration que sur des missions d'expertises ou de d√©veloppement "from scratch". +
Sur ces missions d'int√©gration, la connaissance encyclop√©dique des technologies du march√© poss√©d√©e par les IA est un avantage ind√©niable. +
Les consultants qui seront form√©s sur ces technologies seront rapidement grandement avantag√©s par rapport aux ceux qui ne le sont pas. +
En cons√©quence, *former les Softeamiens √† l'usage de ces technologies aurait toutes les raisons de rapidement devenir une de nos priorit√©s*.

=== Perspectives futures

* Etat des lieux de l'IA par le Dr Alan D. Thompson √† Devoxx Belgique 2022 : https://www.youtube.com/watch?v=D3tTsoX02d8
	** parmi les informations donn√©es, une comparaison entre la valeur cr√©√©e par Internet dans les entreprises de 1997 √† 2021 et celle anticip√©e par l'IA de 2021 √† 2023 : +
	image:2022_ARK-Invest_internet-vs-AI-value-generated-comparison.png[]
		*** source du graphique : https://ark-invest.com/articles/analyst-research/productivity-gains-could-propel-the-ai-software-market-to-14-trillion-by-2030/

* 2023/04/23 : https://pandia.pro/guide/quest-ce-quun-grand-modele-de-langage-llm-large-language-model
	** *Des mod√®les plus compacts et efficaces* : Des efforts sont d√©j√† en cours pour r√©duire la taille des mod√®les tout en maintenant leur performance, ce qui permettra de d√©ployer les LLM sur des appareils avec des capacit√©s de calcul limit√©es.
	** *Am√©lioration des m√©triques d'√©valuation* : Pour √©valuer correctement les performances des LLM, de meilleures m√©triques d'√©valuation et des m√©thodes d'interpr√©tation sont n√©cessaires.
	** *Automatisation de la cr√©ation de contenu* : L'une des applications les plus prometteuses des LLM est l'automatisation de la cr√©ation de contenu, pour les m√©dias sociaux, les blogs, les publicit√©s et plus encore.
	** *R√©duction des biais et am√©lioration de l'√©thique* : Les chercheurs et les d√©veloppeurs s'efforcent de r√©duire les biais pr√©sents dans les LLM et de les rendre plus √©thiques et responsables.

=== Du c√¥t√© des IA g√©n√©ratrices d'images

* Guide complet sur DALL-E : https://www.lebigdata.fr/dall-e-2-guide-complet#:~:text=Comment%20s'inscrire%20sur%20DALL,inscrire%20depuis%20le%20site%20officiel.

* https://www.commentcoder.com/generateurs-images-ia/ : les meilleurs g√©n√©rateurs d'images par IA +
Les 5 meilleurs : 

	1. Jasper Art : sp√©cialis√© dans l'IA
	2. Nightcafe : Essai gratuit et moins cher
	3. MidJourney : Le plus facile (Discord)
		** le plus grand concurrent de DALL-E (√† confirmer)
	4. OpenAI DALL-E 2 : Cr√©ation d'images r√©alistes
		** le plus connu actuellement (car OpenAI)
	5. Stable Diffusion : Transformation d'images

* use case "simple et utile" : cr√©ation d'images libre de droits d'autres pour les pr√©sentations (pour le moment...)

* Pour bien prompter une IA g√©n√©ratrice d'images : 
	** https://www.commentcoder.com/prompt-ia/
	** https://www.commentcoder.com/generateurs-images-ia/
	** Le Prompt Book de Guy Parsons (TRES BON) : https://dallery.gallery/the-dalle-2-prompt-book/
	** le subreddit de DALL-E 2 : https://www.reddit.com/r/dalle2/

* Fin 2022 Jason M Allen a gagn√© un concourt d'arts avec une image cr√©√©e par une IA g√©n√©rative (MidJourney) "Th√©√¢tre d'op√©ra spatial" : 
	** https://intelligence-artificielle.developpez.com/actu/336145/Une-oeuvre-d-art-generee-par-l-IA-a-remporte-la-premiere-place-a-un-concours-de-beaux-arts-a-une-foire-d-Etat-et-les-artistes-sont-furieux/
	** https://www.arts-in-the-city.com/2022/09/07/une-ia-secoue-le-monde-de-lart-en-remportant-un-concours/
	** Pour les autres cr√©ation de Jason Allen, voir : https://www.google.com/search?q=jason+allen+space+opera&sxsrf=AJOqlzXqkaGZz5anmc2x1pERjZc1LeuQyQ:1677085733168&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiYoIibz6n9AhUF57sIHdHrA9IQ_AUoAXoECAEQAw&cshid=1677085762268181&biw=3072&bih=1520&dpr=1.25

Exemples de g√©n√©ration d'images avec DALL-E : 

	* Prompt : "Cthulu, intricate sand sculpture, high detail, UHD" +
	image:dall-e_sand-sculpture-cthulu.jpg[]

Prise en main de MidJourney : 

	* Starting with MidJourney : https://www.youtube.com/watch?v=zPcnHFYxXxk
	* Quick Start officiel de MidJourney : https://docs.midjourney.com/docs/quick-start
	* How to add the MidJourney bot to your own Discord server : https://docs.midjourney.com/docs/invite-the-bot

2023/03 : MidJourney met fin √† son utilisation gratuite sans abonnement

=== DEMO : g√©n√©ration d'avatar anim√©

* techno utilis√©es : MidJourney, studio D-ID
* Instructions, tutoriels pour la cr√©ation d'avatars anim√©s par IA g√©n√©rative : 
	** Forget Lensa AI! Make Avatars with Midjourney : https://www.youtube.com/watch?v=LWE3BXkb50Q
	** Bring Your MidJourney AI Avatar To Life (AI Animation) : https://www.youtube.com/watch?v=RN2n6BGg8RI
	** Cr√©ez votre propre avatar anim√© par l'IA en 10 minutes. Guide pas √† pas : https://www.youtube.com/watch?v=_u4JYYNcgN0

Conclusion : le r√©sultat est bon, avec un gros effet "wahou !" MAIS n'est pas encore parfait. +
Selon moi, nous sommes vraiment dans la phase de "boum" du cycle de hype du Gartner (AJOUTER UNE PHOTO DU CYCLE !), il faut rester tr√®s prudent sur les "vrais" usages de la techno, ceux qui vont pouvoir √™tre r√©ellement utilis√©s en PROD, MAIS il est capital de TOUT DE SUITE s'int√©resser √† ces nouvelles technologies.

* POC Softeam : g√©n√©ration d'avatar anim√© racontant l'histoire de Softeam

	** Prompt de g√©n√©ration d'image dans MidJourney : +
	portrait of an American woman aged 25-30 with brunette hair, clear facial features, happy expression, Cinematic, 35mm lens, f/1.8, accent lighting, global illumination, --uplight --v 4

	** Animation de l'avatar avec Studio D-ID, mod√®le de voix fran√ßaise "Yvette"
		*** Nom de l'avatar "Camille Marquet"

	** Texte pour l'avatar - Acte I de l'histoire de Softeam : 
+
----
Softeam a √©t√© cr√©√© le 30/11/1989 par Fran√ßois Salin, Marc Clavereau, Philippe Desfray et Patrick Linesmeilleur.
Ils travaillaient √† l'√©poque pour CR2A-DI, une ESN sp√©cialis√©e en informatique technique et industrielle. 
C'est durant un voyage du CE qu'ils ont eu l'id√©e de monter une soci√©t√© ensemble : Softeam !
La soci√©t√© a pour objectif de fournir des solutions dans les domaines du conseil, des produits et de la formation sur la mod√©lisation et la m√©thodologie de d√©veloppement. 
Il s'agit l√† de l'expertise technique historique de Softeam, avec tr√®s vite l'Atelier de G√©nie Logiciel Objecteering comme fer de lance.

En 1990, Philippe Desfray cr√©e la m√©thode de conception Objet "Classe Relation".
Puis en 1991, la soci√©t√© met sur le march√© l'Atelier de G√©nie Logiciel "Objecteering", support de cette m√©thode.
Il faut se rappeler qu'en 1991, UML n'existait pas encore, ce langage ayant √©t√© cr√©√© en 1995 par l'√©diteur Rational. Ce que faisait Objecteering √©tait alors unique, c'√©tait l'AGL le plus utilis√© de France.
Gr√¢ce √† cet AGL, Softeam s'est impos√© dans de grands comptes comme France T√©l√©com, Matra, EDF et Canal+ entre autres.

En 1996, Softeam devient contributeur de l'Object Modeling Group, pour participer √† la d√©finition du standard UML.
Puis en 1998 Softeam ambitionne d'exporter Objecteering aux US, et de concurrencer Rose de Rational.
----

		*** remplacer "Sala√ºn" par "Salin" pour avoir une prononciation correcte
		*** remplacer "Linsmeier" par "Linesmeilleur"

-> vid√©o sauvegard√©e en local et sur mon OneDrive : +
https://laposte-my.sharepoint.com/:v:/r/personal/thomas_schwender_softeam_fr/Documents/CTO/Avatar-anim%C3%A9/histoire-de-Softeam_Acte-I_la-cr%C3%A9ation.mp4?csf=1&web=1&e=FK0Axe

== FONCTIONNEMENT DES MODELES DE LANGAGES

* Fonctionnement de ChatGPT
	** https://www.assemblyai.com/blog/how-chatgpt-actually-works/
	** Researchers and developers are working on various approaches to address the *alignment problem in Large Language Models*. +
	ChatGPT is based on the original GPT-3 model, but has been further trained by *using human feedback to guide the learning process* with the specific goal of mitigating the model‚Äôs misalignment issues. The specific technique used, called *Reinforcement Learning from Human Feedback* (RLHF), is based on previous academic research. +
	ChatGPT represents the first case of use of this technique for a model put into production.

* https://platform.openai.com/docs/models/
	* The OpenAI API is powered by a diverse set of models with different capabilities and price points. You can also make limited customizations to our original base models for your specific use case with fine-tuning.
	* text-davinci-002 : supervised fine-tuning instead of reinforcement learning
	* GPT-3 models can understand and generate natural language. These models were superceded by the more powerful GPT-3.5 generation models. +
	However, the *original GPT-3 base models* (davinci, curie, ada, and babbage) are current the *only models that are available to fine-tune*.
		** Une description des mod√®les GPT 3 est disponible juste apr√®s ce paragraphe

* https://gpttools.com/comparisontool : GPT comparison tool *ON GPT 3 base models* (cf l'explication pr√©c√©dente)
	** This tool lets you try out different settings (engines, *temp*, *top p*, etc.) and compare the results.

* https://platform.openai.com/docs/quickstart
	** Remember that the model predicts which text is most likely to follow the text preceding it.

* https://platform.openai.com/docs/api-reference/chat/create
	** on y trouve une liste de tous les param√®tres que l'on peut personnaliser lors de l'appel de l'API d'un mod√®le de langage.

		*** ATTENTION ! Cf https://platform.openai.com/docs/models/model-endpoint-compatibility, on voit que le endpoint "/v1/chat/completions" concerne les mod√®les : gpt-4, gpt-4-0314, gpt-4-32k, gpt-4-32k-0314, gpt-3.5-turbo, gpt-3.5-turbo-0301 
		*** PAR CONTRE, le mod√®le "/v1/fine-tunes" est uniquement associ√© aux mod√®les "davinci, curie, babbage, ada" qui doivent √™tre les mod√®les GPT 3 cf les remarques pr√©c√©dentes.
		*** QUESTION : si on diff√©rencie les 2 endpoints pr√©c√©dents, quels sont donc les param√®tres associ√©s au "fine-tuning" ? Car il est √©galement possible de personnaliser des param√®tres pour le endpoint "/v1/chat/completions"

* Pour un papier de recherche expliquant ce qu'est et ce que n'est pas un mod√®le de langage (LLM) (TRES BONNE RESSOURCE) : +
Talking about Large Langague Models (LLM) : https://arxiv.org/pdf/2212.03551.pdf

	** Une TRES BONNE RESSOURCE expliquant de fa√ßon rationnelle et scientifique pourquoi on ne peut pas dire qu'un LLM "pense", "croit" ou "raisonne".

	** DEFINITION D'UN MODELE DE LANGAGE : "a model doing just that one thing: generating statistically likely continuations of word sequences"

	** "But an *LLM* ‚Äî the bare-bones model ‚Äî is *not in the business of making judgement*s. It just models what words are likely to follow from what other words"
	** "To build a question-answering system, the LLM simply has to be supplemented with a dialogue management system that queries the model as appropriate."
	** "If we prompt an LLM with ‚ÄúAll humans are mortal and Socrates is human therefore‚Äù, we are not instructing it to carry out deductive inference. Rather, we are asking it the following question. Given the statistical distribution of words in the public corpus, what words are likely to follow the sequence ‚ÄòAll humans are mortal and Socrates is human therefore‚Äù. A good answer to this would be ‚ÄúSocrates is mortal‚Äù."
	** "This ability to learn a general pattern from a few examples in a prompt prefix, and to complete sequences in a way that conforms to that pattern, is sometimes called *in-context learning* or *few-shot prompting*."
	** "As usual, though, it‚Äôs a good idea to remind ourselves that the question really being posed to the model is of the form ‚ÄúGiven the statistical distribution of words in the public corpus, what words are likely to follow the sequence S‚Äù, where in this case the sequence S is the chainof-thought prompt prefix plus the user‚Äôs query. The sequences of tokens that are most likely to 8 follow S will have a similar form to sequences found in the prompt prefix, [...]"
	** "The noteworthy thing is that next token prediction is sufficient for solving previously unseen reasoning problems, even if unreliably. How is this possible? Certainly it would not be possible if the LLM were doing nothing more than cutting-andpasting fragments of text from its training set and assembling them into a response. But this is not what an LLM does. Rather, *an LLM models a distribution that is unimaginably complex, and allows users and applications to sample from that distribution*."
	** p9 : "In this distribution, the most likely continuation of a piece of text containing a reasoning problem, if suitably phrased, will be an attempt to solve that reasoning problem. It will take this form, this overall shape, because that is the form that a generic human response would take. Moreover, because the vast corpus of published human text contains numerous examples of reasoning problems accompanied by correct answers, the most likely continuation will sometimes be the correct answer. When this occurs, it is *not because the correct answer is a likely individual human response*, but *because it is a likely collective human response*."
	** p10 "In contemporary LLM-based applications, it is rare for a language model trained on a textual corpus to be used without further finetuning. This could be supervised fine-tuning on a specialised dataset, or it could be via reinforcement learning from human preferences (RLHF : Reinforcement Learning from Human Feedback)"

* Large Language Model Training in 2023 : https://research.aimultiple.com/large-language-model-training/

	** Bon sch√©ma de l'architecture Transformer : le sch√©ma classique que l'on trouve souvent +
	-> En fait, pr√©f√©rer celui de ce site, le m√™me mais comment√© : https://pandia.pro/guide/quest-ce-quun-grand-modele-de-langage-llm-large-language-model/ +
	image:transformer-architecture.png[]

	** Training a large language model from the ground up requires significant investment, a *more economical alternative is to fine-tune* an existing language model to tailor it to your specific use case. +
	*A single training run for GPT-3* is estimated to cost *around $5 million*.

	** After training, the model is evaluated on a test dataset that has not been used as a training data set to measure the model‚Äôs performance. Based on the evaluation results, the model may require some fine-tuning by adjusting its hyperparameters, changing the architecture, or training on additional data to improve its performance.

	** *Training of an LLM* consists of two parts: *pre-training* and *task-specific training*. 
		*** Pre-training is part of training that enables the model to learn the general rules and dependencies within a language, which takes a significant amount of data, computational power, and time to complete. The large language models discussed in the paper require supercomputer systems with several AI chips (ex. NVIDIA DGX A100 starts at $199,999). Once maintenance and power costs are added, *pre-training of a large language model* is an investment in the magnitude of *millions*.
		*** To make large language models *more accessible* for enterprises, LLM developers are offering services for enterprises looking to leverage language models. NVIDIA‚Äôs NeMO is an example of these services, which offer pre-trained LLMs for fine-tuning and *specific task training* to suit specific use cases. The specific task training adds an additional layer to the model which requires much less data, power, and time to train; making large models accessible for enterprise use. The new task-specific layer is trained with few-shot learning, which aims for accurate outputs with less training data.

* Large Language Models: Complete Guide in 2023 : https://research.aimultiple.com/large-language-models/

	** *D√©finition d'un LLM* : Large language models (LLMs) are *foundation models* that utilize *deep learning* in *natural language processing* (NLP) and *natural language generation* (NLG) tasks. +
	For the purpose of helping them learn the complexity and linkages of language, *large language models are pre-trained* on a vast amount of data. Using techniques such as:
		*** Fine-tuning
		*** In-context learning
		*** Zero-/one-/few-shot learning
	+
	these models can be adapted for downstream (specific) tasks (see Figure 2). +
	{sb}
	An LLM is essentially a *Transformer-based neural network*, introduced in an article by Google engineers titled https://arxiv.org/abs/1706.03762[‚ÄúAttention is All You Need‚Äù] in 2017. +
	*The goal of the model is to predict the text that is likely to come next*. The sophistication and performance of a model can be judged by how many parameters it has. A model‚Äôs parameters are the number of factors it considers when generating output.

	** There are many *open-source language* models that are *deployable on-premise* or in a *private cloud*. +
	Example : 
		*** BLOOM
		*** NeMO LLM
		*** XLM-RoBERTa
		*** XLNet
		*** Cohere
		*** GLM-130B

* Explications sur les *Foundation models* : https://research.aimultiple.com/foundation-models/

	** A foundation model is a kind of model that has been trained in such a manner that it can be used for downstream tasks. The foundation model can therefore be effective for tasks for which it has not previously been trained.
	** Foundation models need to be adapted because they *serve as a base for new models*, and there are numerous approaches to do this, such as:
		*** *Fine-tuning* : +
		This is the process of adopting a given model to meet the needs of a different task. Thus, instead of generating a new model for this purpose, a modification will suffice.
		*** *In-context learning* : +
		Using this approach, models can learn how to perform a task with minimum training and without fine-tuning, unlike conventional approaches.
+
image:foundation-models-working.png[]
image:foundation-models-tooling.png[]

* https://pandia.pro/guide/quest-ce-quun-grand-modele-de-langage-llm-large-language-model
	** Les Transformeur ont r√©volutionn√© le traitement du langage naturel en permettant une meilleure compr√©hension du contexte et une meilleure prise en charge de longues phrases.
	** bon sch√©ma sur l'architecture des transformers

=== LLM Multimodal

TO BE COMPLETED

=== Tokens

	* https://platform.openai.com/docs/introduction/key-concepts : +
		** Our models understand and process text by breaking it down into tokens. Tokens can be *words* or just *chunks of characters*. 
		** The number of tokens processed in a given API request depends on the length of both your inputs and outputs. +
		As a rough rule of thumb, *1 token is approximately 4 characters or 0.75 words for English text*. +
		One limitation to keep in mind is that your text prompt and generated completion combined must be no more than the model's maximum context length (for most models this is 2048 tokens, or about 1500 words). Check out our tokenizer tool to learn more about how text translates to tokens."
	* https://platform.openai.com/docs/quickstart/adjust-your-settings "Deep Dive - Understanding tokens and probabilities"
		** l'outil de comptage des tokens utilis√© dans cette section doit quasi certainement √™tre le tokenizer tool d'OpenAI (https://platform.openai.com/tokenizer)

	* https://platform.openai.com/docs/quickstart/closing
		** The total number of tokens processed in a single request (both prompt and completion) can‚Äôt exceed the model's maximum context length. 
		For most models, this is 4,096 tokens or about 3,000 words. As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text. 
		**Pricing is pay-as-you-go per 1,000 tokens, with $5 in free credit that can be used during your first 3 months.
		One limitation to keep in mind is that, for most models, a single API request can only process up to 4,096 tokens between your prompt and completion.

=== "Settings" / param√®tres de personnalisation des mod√®les de langages

"Settings" pouvant √™tre ajust√©s pour modifier la compl√©tion des mod√®les de langage : 

	* On en retrouve une explication au niveau de la documentation officielle d'OpenAI pour ses API de completion et de Chat completion :
		** https://platform.openai.com/docs/api-reference/chat/create
		** https://platform.openai.com/docs/api-reference/completions/create

	* *Temperature* : 

		** https://platform.openai.com/docs/quickstart/adjust-your-settings +
		Remember that the model predicts which text is most likely to follow the text preceding it. Temperature is a value between 0 and 1 that essentially *lets you control how confident the model should be when making these predictions*. Lowering temperature means it will take fewer risks, and completions will be more accurate and deterministic. Increasing temperature will result in more diverse completions.
			*** Section "Understanding tokens and probabilities", il est expliqu√© qu'avec une temp√©rature de 0, le mod√®le compl√©tera TOUJOURS le texte avec le token ayant la plus forte probabilit√© (voir l'exemple donn√©). +
			Si on augmente la temp√©rature, le mod√®le "prendra plus de risques" et envisagera des tokens avec des probabilit√© plus basses.

		** La temp√©rature est un param√®tre qui contr√¥le le degr√© de "variabilit√©" des pr√©dictions de g√©n√©ration de texte du mod√®le. Plus la temp√©rature est √©lev√©e, plus les pr√©dictions seront cr√©atives et vari√©es, mais aussi moins pr√©cises et coh√©rentes.

	* *top_p* :

		** An alternative to sampling with temperature, called *nucleus sampling*, where the model considers the results of the tokens with top_p probability mass. *So 0.1 means only the tokens comprising the top 10% probability mass are considered*. +
		We generally recommend altering this or temperature but not both.
			*** et apr√®s un bon 2h de recherche, oui, c'est d√©finition est bien la bonne

		** il est vraiment difficile de trouver des explications claires sur ce param√®tre
			*** Et quand on interroge ChatGPT lui-m√™me sur le sujet, on a du grand n'importe quoi en r√©ponse !
				**** et c'est bien la preuve que ChatGPT ne "raisonne pas"
			
			*** Pour une tr√®s bonne explication, voir https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api-a-few-tips-and-tricks-on-controlling-the-creativity-deterministic-output-of-prompt-responses/172683
				**** Top_p sampling is an alternative to temperature sampling. Instead of considering all possible tokens, GPT-3 considers only a subset of tokens (*the nucleus*) whose cumulative probability mass adds up to a certain threshold (top_p).
				For example, if top_p is set to 0.1, GPT-3 will consider only the tokens that make up the top 10% of the probability mass for the next token. This allows for dynamic vocabulary selection based on context.
				**** l'article propose √©galement un tr√®s bon tableau sur la combinatoire de ces 2 param√®tres et les effets que l'on peut esp√©rer.

			*** Mes "derniers" √©changes avec ChatGPT sur le sujet : 
				**** Thomas : Je viens de relire l'explication de la documentation officielle d'OpenAI sur le param√®tre top_p. + 
				La chose suivante est expliqu√©e "0.1 means only the tokens comprising the top 10% probability mass are considered". Donc, j'ai l'impression que contrairement √† tes pr√©c√©dentes explications, ce 0.1 ne veut pas dire que ne seront consid√©r√©s que les mots avec une probabilit√© cumul√©e sup√©rieure √† 10%, MAIS que ne seront consid√©r√©s que les mots repr√©sentant les 10% de mots avec les plus fortes probabilit√©s. +
				N'est-ce pas plut√¥t cela ? +
				Car dans ce cas, la valeur par d√©faut de 1 devient logique : elle repr√©senterait les mots repr√©sentant les 100% des mots avec les plus fortes probabilit√©s, donc tous les mots.
				**** ChatGPT : Vous avez raison, je m'excuse pour la confusion dans mes explications pr√©c√©dentes. Vous avez interpr√©t√© correctement la signification du param√®tre "top_p" (√©galement connu sous le nom de "nucleus sampling") tel que d√©crit dans la documentation officielle d'OpenAI. +
				En effet, lorsque "top_p" est d√©fini sur 0.1, cela signifie que seuls les tokens repr√©sentant les 10% de la masse de probabilit√© la plus √©lev√©e sont consid√©r√©s pour la g√©n√©ration de texte. Cela permet de restreindre la diversit√© des mots g√©n√©r√©s et de favoriser les mots avec les probabilit√©s les plus √©lev√©es. +
				Dans ce contexte, la valeur par d√©faut de 1 pour "top_p" repr√©sente en effet l'inclusion de tous les mots, car cela signifie que 100% des tokens sont pris en compte, y compris ceux avec des probabilit√©s plus faibles. Cela offre une g√©n√©ration de texte plus ouverte sans seuil sur la probabilit√© cumul√©e des mots. +
				Je m'excuse pour les pr√©c√©dentes erreurs dans mes explications et je vous remercie de votre clarification.

	* *presence_penalty* (Le taux de pr√©sence) : 

		** OpenAI : Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
		** https://platform.openai.com/docs/api-reference/parameter-details
		The *frequency* and *presence penalties* found in the Completions API can be used to *reduce the likelihood of sampling repetitive sequences of tokens*. They work by directly modifying the logits (un-normalized log-probabilities) with an additive contribution.

		** https://aipower.org/presence-penalty/ (TRES BON) : +
		the presence_penalty parameter modifies the probability distribution to make less likely words that were present in the input prompt or seed text. This encourages the model to generate words that were not in the input. It works by scaling down the log probabilities of words that were present in the input, making it less likely for the model to generate these words that are already in the input.

		** Thomas : contr√¥le la probabilit√© de g√©n√©rer du texte qui soit d√©j√† pr√©sent dans le prompt. Cela permet d‚Äôencourager le mod√®le √† g√©n√©rer de "nouveaux mots", non d√©j√† donn√©s dans le prompt.

	* *frequency_penalty* (Le taux de fr√©quence) : 

		** OpenAI : Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.

		** https://aipower.org/presence-penalty/: +
		The frequency_penalty parameter modifies the probability distribution to make less likely words that the model has seen more frequently during its training. This encourages the model to generate novel or less common words. It works by scaling down the log probabilities of words that the model has seen frequently during training, making it less likely for the model to generate these common words.

		** Thomas : contr√¥le la probabilit√© de g√©n√©rer du texte en fonction de la fr√©quence √† laquelle ce dernier a d√©j√† √©t√© rencontr√© par le mod√®le durant son entra√Ænement.

	* To put it simply, frequency_penalty penalizes the model for generating the common words that the model has seen a lot during training whereas presence_penalty penalizes the model for generating the words that are present in the input text.

."taux de diversit√©"
[NOTE]
====
* The *frequency_penalty* and *presence_penalty* parameters are both *controls for the diversity of the generated text* produced by GPT-3 (Generative Pre-trained Transformer 3). They both adjust the trade-off between the likelihood of the generated text and its novelty.
	** Source : https://aipower.org/presence-penalty/

* Doc officielle OpenAI : The frequency and presence penalties found in the Completions API can be used to reduce the likelihood of sampling repetitive sequences of tokens.
====

	* *best_of* : 

		** Generates "best_of" number of completions server-side and returns the "best" (*the one with the highest log probability per token*). +
		Results cannot be streamed. +
		When used with n, best_of controls the number of candidate completions and n specifies how many to return, meaning best_of must be greater than n.
		
		** Note: Because this parameter generates many completions, it *can quickly consume your token quota*. +
		-> Use carefully and ensure that you have reasonable settings for max_tokens and stop.

	* *logprob*

		** Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response. +
		The maximum value for logprobs is 5. If you need more than this, please contact us through our Help center and describe your use case.
		
		** Pour plus d'explication sur ces logarithmes de probabilit√©s, voici quelques ressources : 
			*** http://gptprompts.wikidot.com/intro:logprobs : une tr√®s bonne explication de la m√©canique des logarithmes de probabilit√©s, avec des exemples de compl√©tion des mod√®les d'OpenAI. +
			L'article d√©crit √©galement le fonctionnement de la temperature et de "best_of" pour s√©lectionner la compl√©tion (le token) la plus appropri√©e.
				**** D√©finition tr√®s claire : +
				*The logprob is the log of the probability that a token comes next*. +
				In computer science, *multiplying is computationally expensive and adding is cheap*, so a lot of time when you have to multiple probabilities you *take the logs* and add them instead to get the same result. To convert a logprob back to the original probability, you just take e^logprob, which in python is np.e**logprob (using import numpy as np).
			*** https://fr.wikipedia.org/wiki/Logarithme

=== Fine-tuning des mod√®les de langage

Pour le "fine-tuning" √† proprement parler : 

	* https://openai.com/pricing, section "Fine-tuning models"
		** De nouveau, seuls les mod√®les bas√©s sur GPT 3 peuvent √™tre fine-tun√© : Ada, Babbage, Curie, Davinci
		** Important de bien choisir son mod√®le par rapport au use case et au c√¥ut du mod√®le.
			*** Exemple : For this use case (*classification*) we fine-tuned an *ada model* since it will be faster and cheaper, and the performance will be comparable to larger models because it is a classification task.

	* https://platform.openai.com/docs/guides/fine-tuning

		** *Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt*, letting you achieve better results on a wide number of tasks. Once a model has been fine-tuned, you won't need to provide examples in the prompt anymore. +
		-> This *saves costs* and enables *lower-latency requests*.
			*** Donc meilleur co√ªt et meilleures performances

		** Training data is how you teach GPT-3 what you'd like it to say.
		** *The more training examples you have, the better*. We recommend having at least a couple hundred examples. In general, we've found that each doubling of the dataset size leads to a linear increase in model quality.

		** When a job has succeeded, the *fine_tuned_model* field will be populated with the name of the model. You may now *specify this model as a parameter to our Completions API*, and *make requests to it using the Playground*.

		** *Fine-tuning* is a powerful technique to *create a new model that's specific to your use case*. Before fine-tuning your model, we strongly recommend reading these best practices and specific guidelines for your use case below.

	* ChatGPT : Le fine-tuning pour les mod√®les de langage OpenAI est un processus par lequel un mod√®le pr√©-entra√Æn√©, tel que GPT, est adapt√© √† des t√¢ches ou des domaines sp√©cifiques en utilisant un jeu de donn√©es suppl√©mentaire. Contrairement √† l'entra√Ænement initial du mod√®le, qui se fait sur de vastes ensembles de donn√©es, le fine-tuning tire profit de donn√©es plus sp√©cifiques et cibl√©es.

.https://research.aimultiple.com/large-language-models/#easy-footnote-bottom-1-63352
image:LLM-pre-training-and-fine-tuning.jpg[]

=== Limite du contexte du mod√®le de langage

* Voir la notion de "MAX TOKENS" : https://platform.openai.com/docs/models/gpt-3-5
	** 4096 tokens pour ChatGPT (gpt-3.5-turbo-0301)

* https://platform.openai.com/docs/introduction/key-concepts +
"One limitation to keep in mind is that *your text prompt and generated completion combined* must be no more than the model's maximum context length"
* M√™me dans cette autre section de la doc officielle : https://platform.openai.com/docs/quickstart/pricing et https://platform.openai.com/docs/quickstart/closing

	** The total number of tokens processed in a single request (both prompt and completion) can‚Äôt exceed the model's maximum context length. For most models, this is 4,096 tokens or about 3,000 words.
	** One limitation to keep in mind is that, for most models, a single API request can only process up to 4,096 tokens between your prompt and completion.

-> Or le contexte pass√© est ajout√© au prompt (comportement par d√©faut de ChatGPT) et doit donc √™tre compt√©, pris en compte pour d√©terminer la longueur actuelle du contexte.

* Voir √©galement cet article, https://community.openai.com/t/chatgpt-api-maximum-token/83321, qui propose des strat√©gies de pruning pour diminuer la taille du contexte : 

	** Delete ‚Äúrole: system‚Äù messages, since they are weak and the conversation is already ongoing (so far have not had great results with the system role anyway, but I have not fully tested either),
	** Truncate the messages starting with the oldest (brute force).
	** Use max_tokens and summarize stored messages using a different model.
	** Remove prior ‚Äúrole: assistant‚Äù messages.

* Quand le contexte transmis √† ChatGPT d√©passe le maximum autoris√© par son mod√®le de langage, ce dernier est automatiquement tronqu√© ou r√©duit. +
La documentation officielle manque de d√©tails pr√©cis sur le m√©canisme de troncature utilis√©. +
N√©anmoins, si l'on pose directement la question √† ChatGPT, voici sa r√©ponse : 

	** Lorsque le contexte donn√© √† ChatGPT d√©passe la taille maximale de son mod√®le de langage (4096 tokens pour gpt-3.5-turbo), il est n√©cessaire de le tronquer ou de le r√©duire pour le rendre compatible. Il y a diff√©rentes approches possibles pour g√©rer cette situation :
		*** Troncature : Le contexte peut √™tre tronqu√© en supprimant les parties les plus anciennes ou moins pertinentes afin de respecter la limite de taille. Cela peut entra√Æner la perte d'informations importantes et limiter la capacit√© du mod√®le √† comprendre le contexte complet.
		*** R√©sum√© ou agr√©gation : Plut√¥t que de tronquer le contexte, il est possible de cr√©er un r√©sum√© ou une agr√©gation des parties essentielles du contexte afin de r√©duire sa taille tout en pr√©servant les informations cl√©s. Cela n√©cessite de faire des choix sur les parties √† inclure et √† exclure, ce qui peut introduire une certaine subjectivit√©.
		*** Utilisation d'un contexte contextuel : Plut√¥t que de fournir tout le contexte pr√©c√©dent, il est possible d'utiliser un contexte plus r√©cent ou plus sp√©cifique pour guider la g√©n√©ration de ChatGPT. Cela permet de limiter la quantit√© de texte envoy√© au mod√®le tout en conservant la pertinence de la r√©ponse.

	** Thomas : Est-il possible de savoir quel comportement ChatGPT utilise parmi ceux que tu as cit√©s en fonction des requ√™tes qui lui sont adress√©es ? Y a-t-il moyen de configurer ce comportement ?

	** ChatGPT : En utilisant l'API OpenAI, il n'est *pas directement possible de sp√©cifier ou de configurer explicitement le comportement sp√©cifique de troncature*, de r√©sum√© ou d'agr√©gation du contexte parmi ceux mentionn√©s pr√©c√©demment. +
	L'API ne fournit pas de param√®tre distinct pour contr√¥ler ces m√©canismes de gestion du contexte. +
	Lorsque le contexte d√©passe la taille maximale autoris√©e, *l'API OpenAI effectue automatiquement une troncature pour le rendre compatible avec le mod√®le de langag*e. La fa√ßon dont la troncature est r√©alis√©e peut varier en fonction de la structure interne de l'algorithme de l'API, mais les d√©tails exacts de son fonctionnement ne sont pas sp√©cifi√©s publiquement.

* Voir √©galement cette r√©ponse d'OpenAI sur son forum : https://community.openai.com/t/when-conversation-grows-larger-did-chatgpt-include-all-of-the-conversation-context-for-every-chat/121876
	** logankilpatrick Developer Advocate (OpenAI Staff) : No, all of the messages are likely not included. There is a finite amount of context that can be passed for each request. *We have a truncation algorithm running behind the scenes* that tried to determine what the important information is from previous messages that needs to be included.

Cette troncature du contexte peut √™tre consid√©r√©e comme en partie responsable de certains des "d√©rapages" ayant d√©j√† eu lieu avec le chatbot lors de conversation longues entre un humain et lui.

== DEVELOPPEMENT ASSISTE PAR L'IA

=== GitHub Copilot

* Documentation officielle de GitHub Copilot : https://docs.github.com/en/copilot

== 2023/09/29 - Pr√©paration √©tats des lieux technologiques Softeam

Le domaine √©volue de nouveau (oui, encore‚Ä¶), avec un changement de paradigme : + 
-> Le *LLM* n'est plus une "solution en soi" mais devient *une brique dans un pipeline d'IA*

Stack technologique "du moment" pour une application de recherche / search bas√©e sur l‚ÄôIA g√©n√©rative :

	* un *LLM* : OpenAI, Hugging Face, Dolly, etc.
	* *LangChain* : le framework de ¬´¬†glue¬†¬ª de la solution
	* *Streamlit* : le front / la web app
	* une *BDD vectorielle* : Pinecone, Chroma, Elastic search, Postgre, etc.
		** Liste des vector databases : https://thedataquarry.com/posts/vector-db-1/[] +
		image:20230929_etat-des-lieux-tech_vector-databases.jpg[]

* l'IA √©volue vers Langchain et les agents
	** le LLM devient un outil au sein d'un "workflow IA"
	** et base de donn√©es vectorielle pour Langchain
	** et parler du move vers les embeddings

=== Guerre de l'IA et concurrence

* Dans la guerre des IA g√©n√©ratives, Google vient de demander √† ses chercheurs de *ne PLUS publier leurs travaux*.
	** OpenAI s'est effectivement servi d'eux (mod√®le Transformer de 2017) pour cr√©er un produit plus vite que Google.
	** De la m√™me fa√ßon, OpenAI ne publie pas non plus ses travaux sur GPT 4, 
	** et c√¥t√© M√©ta, LLaMa est disponible du fait d'une "fuite", et ne correspond pas √† une mise en open source "officielle".

-> On voit donc bien la *culture du secret* s'installer pour essayer d'√™tre le 1er sur le domaine.

* Un des plus grands *cabinet d'avocats Allen & Overly* s'est associ√© avec OpenAI pour cr√©er *leur propre chatbot ("Harvey")*, int√©gr√© depuis quelques semaines dans les 3000 ordinateurs du groupe.
	** En par exemple *20 sec*, Harvey sors une cartographie des jurisprudences *l√† o√π cela aurait n√©cessit√© 5h du travail* d'un stagiaire auparavant. Suite √† cela on a √©videmment toujours besoin du stagiaire, qui peut d√®s lors s'appuyer sur le travail d'Harvey pour creuser un sujet particulier. +
	D'ici peu tous les contrats du cabinet seront port√©s sur Harvey.

-> Donc tous les sites de conseil juridique, tous les sites un petit peu pratico-pratiques pour eux, *c'est termin√©*.

=== Les soci√©t√©s que l'IA fait exploser

*NVIDIA*

* Le groupe am√©ricain sp√©cialis√© dans les puces graphiques a d√©voil√© hier ses r√©sultats au titre du deuxi√®me trimestre d√©cal√© 2023. Port√© par l'IA ainsi que ses GPU, composants particuli√®rement adapt√©s pour faire tourner les IA g√©n√©ratives, le g√©ant am√©ricain a doubl√© son chiffre d'affaires pour atteindre 13,5 Mds$. Le b√©n√©fice net lui ressort √† 6,2 milliards, *multipli√© par plus de 8* par rapport √† l'ann√©e pr√©c√©dente. 
	** La capitalisation de Nvidia, qui plafonnait mercredi soir √† hauteur de 1 160 Mds$, devrait conna√Ætre un nouveau boost aujourd'hui.
	** Donc capitalisation juste derri√®re Amazon pour une bo√Æte qui fait des processeurs, dont l'importance actuelle de l'IA

* Pour des graphes de la capitalisation, voir : 
	** https://www.netcials.com/financial-marketcap-history-usa/1045810-NVIDIA-CORP/
	** https://www.reuters.com/technology/global-markets-marketcap-2023-08-01/

image:20230929_etat-des-lieux-tech_nvidia-capitalisation_02.jpg[width=600]
image:20230929_etat-des-lieux-tech_nvidia-capitalisation_01.jpg[width=600]

* On voit bien le boum repr√©sent√© par la crypto PUIS l'IA pour Nvidia
	** Globalement, les analystes voient bien NVidia franchir rapidement (2024) la barre des 2 000 milliards de capitalisation boursi√®re.

	** DONC, on garde un oeil sur les technologies de Nvidia, tout particuli√®rement leurs solutions de d√©veloppement comme *CUDA*, car il n'est pas impossible que nous ayons des demandes dessus un jour (m√™me si pour le moment, ce n'est pas notre domaine m√©tier ET ENCORE)

		*** https://developer.nvidia.com/cuda-toolkit[] :
			**** The NVIDIA¬Æ CUDA¬Æ Toolkit provides a *development environment* for creating high performance GPU-accelerated applications. With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers. 
			**** The toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler, and a runtime library to deploy your application. +
			Using built-in capabilities for distributing computations across multi-GPU configurations, scientists and researchers can *develop applications that scale from single GPU workstations to cloud installations with thousands of GPUs*.

		*** https://en.wikipedia.org/wiki/CUDA[] : 
			**** CUDA (or *Compute Unified Device Architecture*) is a *proprietary and closed source parallel computing platform* and *application programming interface (API)* that allows software to use certain types of graphics processing units (GPUs) for general purpose processing, an approach called general-purpose computing on GPUs (GPGPU). *CUDA is a software layer that gives direct access to the GPU's virtual instruction set and parallel computational elements*, for the execution of compute kernels.
	
		*** CUDA peut √™tre utilis√© avec un certain nombre de langages de programmation, notamment C, C++, *Python* et Fortran.
			**** Le Python est compl√®tement dans notre scope

		*** Et, dans des cas sp√©cifiques, comme l'IA, o√π les GPU seraient la plateforme d'ex√©cution des applications, il n'est pas impossible du tout que *CUDA devienne un concurrent de Spark* pour l'ex√©cution de traitements parall√®les.
			**** CUDA offre de meilleures performances que Spark car sp√©cialement design√© pour fonctionner avec des GPUs (ceux de Nvidia). +
			Spark est certes plus polyvalent que CUDA, mais si on sait pr√©cis√©ment que l'on veut de la perf sur GPU, pour de l'IA par exemple, CUDA appara√Æt comme un choix tr√®s adapt√©.

	** Donc, OK pour garder un oeil sur Rust, mais dans ce cas il faut de la m√™me fa√ßon garder un oeil attentif sur la plateforme CUDA, car sa cible est vaste avec une parfaite ad√©quation au d√©veloppement actuel de l'IA.
		
* *Contexte g√©opolitique* √† conna√Ætre : *guerre √©conomique entre US et Chine*
	** 2022/09/01 - BFM : Nvidia interdiction (par le commerce am√©ricain) exporter puce A100 et H100 (encore en d√©veloppement) en Chine. Restrictions √©galement pour AMD

*Hugging Face*

* Hugging Face l√®ve aupr√®s des plus grands et passe √† une *valorisation de 4.5 Mds$*

	** Le sp√©cialiste de l'IA open source *l√®ve 235 M$*. Sa valorisation est plus que doubl√©e et passe √† 4,5 Mds$. 
	** Nvidia, Google, Amazon, Intel, AMD, IBM et Qualcomm ont particip√© √† un tour de table, men√© par Salesforce Ventures, de 235 M$. Cette op√©ration fait plus que doubler la valorisation de cette entreprise, cr√©√©e par des Fran√ßais et op√©rant entre Paris et New York. Elle vaut d√©sormais 4,5 Mds$. 
		*** Et de nouveau, tout est li√©, qui est pr√©sent √† ce tour de table pour financer un sp√©cialiste de l'IA ? Mais (entre autres) *Nvidia*

	** Inconnu du grand public, Hugging Face est une *plateforme de partage sp√©cialis√©e dans l'IA et le machine learning*. +
	Des d√©veloppeurs du monde entier peuvent y d√©poser des mod√®les pr√©-entra√Æn√©s ou des jeux de donn√©s, qui seront r√©utilis√©s, adapt√©s et am√©lior√©s par d'autres. 
	** Le laboratoire d'IA de Meta a par exemple choisi *Hugging Face pour h√©berger Llama 2*, son dernier mod√®le de langue. 
	** Et des groupes comme Renault, Pzifer ou Roche piochent dans cette biblioth√®que open source pour concevoir leurs services dop√©s √† l'IA sans avoir √† commencer de z√©ro.

* Hugging Face est une plateforme open source qui fournit des outils et des ressources pour le d√©veloppement d'applications bas√©es sur l'IA g√©n√©rative. Elle est compos√©e de deux principaux produits :

	** *La biblioth√®que Transformers* : une biblioth√®que open source qui fournit des impl√©mentations de mod√®les de langage bas√©s sur l'architecture Transformer. +
	Elle permet aux d√©veloppeurs de : 
		*** *Entra√Æner des mod√®les de langage* : la biblioth√®que Transformers fournit des impl√©mentations de mod√®les de langage bas√©s sur l'architecture Transformer, qu'il est ensuite possible d'entra√Æner sur des ensembles de donn√©es textuelles.
		*** *D√©ployer des mod√®les de langage* : la biblioth√®que Transformers fournit des API simples pour d√©ployer des mod√®les de langage. +
		Ces derniers peuvent √™tre d√©ploy√©s soit directement sur les serveurs de Hugging Face, soit sur serveurs priv√©s (attention aux co√ªts d'infra dans ce dernier cas)
		*** *Utiliser des mod√®les de langage pr√©-entrain√©s* : la biblith√®que propose une grande vari√©t√© de mod√®les de langage pr√©-entra√Æn√©s pouvant √™tre directement utilis√©s dans les applications.

	** *Le Hub Hugging Face* : une plateforme de partage de mod√®les, d'ensembles de donn√©es et d'applications IAG.

*OpenAI*

* Pour *relancer l'engouement de ses d√©buts*, OpenAI mise sur une *version professionnelle de son chatbot (ChatGPT)*. L'entreprise veut retrouver la confiance des entreprises, qui ont parfois peur pour leurs donn√©es personnelles.
	** Le lancement intervient alors que ChatGPT ne r√©ussit pas √† capitaliser sur l'emballement provoqu√© par son lancement, qui en avait fait alors l'application atteignant le plus rapidement les 100 millions d'utilisateurs, avant l'arriv√©e de Threads, le rival de X/Twitter par Meta, au d√©but de l'√©t√©.

* Voir l'article https://www.blogdumoderateur.com/openai-lance-version-chatgpt-entreprises/
	** *ChatGPT Entreprise*, une *version s√©curis√©e* et am√©lior√©e du chatbot
	** OpenAI est clair : "Vous poss√©dez et contr√¥lez vos donn√©es commerciales dans ChatGPT Entreprise." La firme affirme que vos conversations professionnelles avec le chatbot ne sortent pas de votre structure et ne sont pas utilis√©es pour entra√Æner les mod√®les de langage d'OpenAI. +
	Toutes les donn√©es sont chiffr√©es ¬´ au repos comme en transit ¬ª, avec les protocoles AES-256 et TLS 1.2+, et en conformit√© au regard de la norme SOC 2.
		*** C'est compl√®tement ce que fait d√©j√† Microsoft avec Azure OpenAI...

* ChatGPT Enterprise est un produit ind√©pendant de Microsoft : la strat√©gie d'entreprise entre OpenAI et Microsoft est... surprenante. +
-> A mon avis, cela ne pourra pas rester √©ternellement "2 bo√Ætes aussi ind√©pendantes" vu l'argent que MS a mis dedans
	** Il y aura peut-√™tre une *phase de migration / d'ajustement √† pr√©voir* (entre produits MS et OpenAI)

=== Changement de paradigme de l'IA : passage √† des mod√®les de petites tailles 

-> A mettre en parall√®le de l'*embedding* et du *"Bring your own Data"*

* GPT-4 : *Composition de plusieurs LLM sp√©cialis√©s plut√¥t qu'un seul √©norme* +
-> C'est la tendance actuelle (mod√®les plus petits sp√©cialis√©s), de m√™me que les outils comme *LangChain*

	** *LangChain* (https://en.wikipedia.org/wiki/LangChain[]) : 
		*** LangChain is a framework designed to simplify the creation of applications using large language models.
		*** A complete set of powerful building blocks. +
		Get started fast with our comprehensive library of open-source components and pre-built chains for any use-case.
			*** Bonne image √† r√©cuperer : image:https://framerusercontent.com/images/5d7KfRriC6zji11ZFnwLotdqcHQ.svg[]
		
image:20230929_etat-des-lieux-tech_azure-bring-your-own-data.jpg[]

	* On retrouve un autre sch√©ma du "Bring your own data" de MS ici : https://msandbu.org/how-to-setup-azure-openai-with-chatgpt-using-your-own-data/ +
	image:20230929_etat-des-lieux-tech_azure-bring-your-own-data_02.jpg[]

image:20230929_etat-des-lieux-tech_langchain-embeddings.jpg[]

* Parler d'*embeddings* avec les sch√©mas du "Bring Your Own Data" de Microsoft et celui de LangChain de la BGPN, ainsi que des *bases Vector* : 

	** Pour la sauvegardes des embeddings en base vectorielle, le split de docs (les chunks) le plus courant semble √™tre le 1000 tokens / characters.
	** Trouver les vecteurs les plus proches : c'est LA o√π diff√©rents algos existent (produit scalaire, distance euclidienne, similarit√© cosinus)
		*** Le principe : on trouve les vecteurs les plus proches de sa question, et ce sont ces derniers qu'on envoie dans sa requ√™te au LLM (donc on envoie quelque chose de plus cibl√© et plus petit dans le contexte du LLM)
	** Dans l'id√©e, *on rajoute une m√©moire externe au LLM* (par exemple ce que fait Langchain).
	** PostgreSQL a PJVector qui rajoute le support vectoriel √† PostgreSQL pour ces use cases.
	** Quelques exemples de BDD vectorielles : Pinecone, Chroma, Elastic search, etc.

* *fonctionnement d'une solution de recherche op√©r√©e par l'IA et utilisant l'embedding de document* : 

	** Pour ce workflow, le *LLM* fait en fait office de *moteur de compr√©hension*
		*** Si on prend le cas de ChapGPT, m√™me en version 4 (√† l'exception de son usage avec Bing OU de la derni√®re version Enterprise), ce dernier est limit√© √† ses *donn√©es d'entra√Ænement qui s'arr√™tent √† 2021/09*.
		*** Nous sommes maintenant fin 2023, ces donn√©es commencent donc √† √™tre *obsol√®tes*, et OpenAI ne se presse "pas r√©ellement" pour changer cela (toujours malgr√© la sortie de la derni√®re version Enterprise). +
		-> On peut donc en conclure que OpenAI ne souhaite pas vraiment mettre √† jour r√©guli√®rement les donn√©es d'entra√Ænement de ChatGPT. +
		Et ils devaient avoir pr√©vu d√®s le d√©but (r√©entra√Æner est tr√®s cher) que l'usage de leur LLM change, et que ce dernier finisse par √™tre utilis√© comme une brique d'un pipeline d'IA. +
		C'est ce stade que nous avons atteint depuis

	** Pour faire des recherches sur des corpus de docs que le LLM ne conna√Æt pas via ses donn√©es d'entra√Ænement : 
		*** on commence par les transformer en vecteur (embeddings : s√©ries de nombres, d'une taille √† d√©finir, ex : 128 valeurs) √† l'aide du LLM
		*** puis on les sauve dans une BDD vectorielle (Pinecone, Chroma, Elastic search, Postgre, etc.)
		*** puis, lorsque l'utilisateur tape son prompt, on convertit √©galement ce dernier en vecteur, et, par une m√©thode de similarit√©, on va trouver les vecteurs des corpus de documents qui ressemblent le plus au vecteur du prompt
		*** et une fois qu'on a trouv√© les corpus de docs les plus proches de notre vecteur de prompt, on peut tout envoyer au LLM, prompt et doc, et ce dernier pour alors chercher √† r√©pondre au prompt en se servant des documents s√©lectionn√©s

	** Cf le speaker Databricks (2023/07/15 - BDH 166), on envoie son Doc au LLM et c'est ce dernier qui renvoie la repr√©sentation vecteur du doc qui est √† stocker dans une base vecteur.
	** Ensuite on peut commencer √† faire des requ√™tes, le plus souvent de requ√™tes de similarit√©s : "quel est le meilleur podcast ?" envoy√© √† un LLM, qui nous renvoie un embeddings repr√©sentant la question, puis on vient rechercher dans le corpus de docs qu'on a index√© les plus similaires √† la question puis on vient interroger un LLM avec la question et les docs similaires remont√©s. +
	Comme cela on peut sp√©cialiser les r√©ponses d'un LLM ou lui donner acc√®s √† des infos qu'il n'avait pas au moment de son entra√Ænement.
	C'est tout cela que sort *Databricks* (en s'appuyant sur leurs tables Delta) : des *embeddings* sont cr√©√©s ainsi qu'une *API de search*

	-> DONC DE PLUS EN PLUS *PONT ENTRE LA DATA ET L'IA*

* *Stack technologique "du moment"* pour une application de recherche / search bas√©e sur l'IA g√©n√©rative : +
un *LLM* + *LangChain* (la glue) + *Streamlit* (le front / la web app) + une *BDD vectorielle* (Pinecone, Chroma, Elastic search, Postgre, etc.)

*Passage √† des mod√®les de plus petite taille*

* Aux Etats-Unis, les plus grands laboratoires en IA s'activent pour mettre le doigt sur la d√©couverte qui pourrait rendre l'assistant d'OpenAI et son mod√®le de langue (LLM) GPT-4 obsol√®te. Parmi les structures qui font la course en t√™te figurent le MIT, le groupe Meta ou encore le franco-am√©ricain Hugging Face. +
C√¥t√© MIT, c'est le Computer Science and Artificial Intelligence Laboratory (CSAIL) qui est √† la man≈ìuvre. Mi-juin, le laboratoire levait le voile sur une IA baptis√©e SimPLE pour Simple Pseudo-Label Editing. L'enjeu ? *D√©montrer que des mod√®les de petite taille peuvent largement d√©passer les performances des grands mod√®les*. 

* Meta, lui, mise sur l'open source. Avec la version 2.0 de son mod√®le de langue LLaMA, le groupe de Mark Zuckerberg a livr√© pour la premi√®re fois mi-juillet une solution sous licence libre (GNU GPL) utilisable au sein d'un produit commercial.

* Mais l'initiative de Meta est loin d'√™tre isol√©e. D√©j√†, mi-2022, *Hugging Face* publiait les sources de Bloom. De mai 2021 √† mai 2022, sa conception a f√©d√©r√© plus de 1 000 chercheurs issus de 60 pays.
	** C'est l√† toute la force de l'open source que de pousser vers l'open innovation. Le logiciel libre distille la technologie jusque dans les moindres PME et start-up. Il permet de r√©cup√©rer un mod√®le puis de l'entrainer sur un data set maison pour le tester voire le mettre en production sur un cas d'usage sp√©cifique. Sur ce plan, l'open source a d√©j√† permis de montrer qu'il √©tait possible de personnaliser de petits LLM avec de petits data set et, partant de l√†, d'obtenir imm√©diatement des r√©sultats op√©rationnels.

* Et Yann LECUN chez Meta nous annonce pour dans bient√¥t un nouveau type d'IA bas√© sur une nouvelle technologie, diff√©rente des Transformers

.Rappel
[NOTE]
====
*Les plus grands experts de l'IA* sont majoritairement *chez Google* aujourd'hui, et il ne faut pas oublier que les Transformer ont √©t√© cr√©√©s par leurs √©quipes. +
====

* Rappel : Google Bard is now powered by Google's very own and most advanced large language model (LLM) *PaLM 2*, which was unveiled at Google I/O 2023.

Conclusion sur l'IA : 

	* Il est urgent d'attendre ! Nos clients, qui ne sont pas des √©diteurs tech faisant du "bleeding edge", ne peuvent pas s'appuyer des techno alpha / beta, pouvant dispara√Ætre ou √™tre remplac√©es du jour au lendemain, ou demandant un effort de d√©veloppement / maintenance trop cons√©quent. +
	En cons√©quence, il est actuellement essentiel : 
		** d'attendre que le milieu de l'IA se stabilise
		** d'identifier pr√©cis√©ment les besoins client pour v√©rifier si une solution √† base d'IA gen est REELLEMENT utile pour eux. +
		-> Actuellement, proposer un "simple rewrap de ChatGPT" avec ses donn√©es limit√©es √† 2021/09 (donc obsol√®tes ou manquant de fra√Æcheur) doit avant tout tenir du gadget ou du buzz...
		** RESISTER AU BUZZ ! Il faut que les directions fassent preuve de "courage politique" : ce n'est pas parce qu'on entend partout parler d'IA gen que l'on en a forc√©ment besoin.
		** Pour des ESN comme nous, *FORMER SES EFFECTIFS* sur les concepts de l'IA gen : le prompting et ses risques et optimisations connues, les stacks derri√®re les pipelines actuels d'IA gen, etc.

== 2023/10/13 - 12@13 Softeam - Acculturation IA g√©n√©rative

* üî•*FANTASTIQUE site*üî• sur les mod√®les de langages / LLM, avec les tailles de mod√®les, et r√©guli√®rement mis √† jour : +
https://lifearchitect.ai/models/

	** il est bien expliqu√© ici que l'on peut parler de la *taille du mod√®le* en *tokens* ou en *To* (TB en anglais) : +
	https://lifearchitect.ai/gpt-4/#dataset

		*** Rappel : 1 token ~ 0.75 mot ~ 4 octets
			**** la taille des tokens est g√©n√©ralement sp√©cifi√©e par le mod√®le de langage utilis√©.
			**** La taille en octets commun√©ment admise pour 1 token est de 4 octets. Cela s'explique par le fait que les tokens sont g√©n√©ralement repr√©sent√©s par des entiers, qui occupent 4 octets en m√©moire.
			**** Par exemple, le mod√®le de langage *GPT-3 utilise des tokens de 4 octets*. Cela signifie que la taille des donn√©es d'entra√Ænement du mod√®le est de 4 fois sup√©rieure √† la taille en mots des donn√©es d'entra√Ænement.

		*** Avec pour exemple GPT-4, cela donne :

			**** *Parameter count* : 1.76T (1,760B). Sparse mixture of experts (MoE).3
			**** ‚âà 5.7x bigger than GPT-3 175B (175 milliards de param√®tres)
			**** ‚âà 14.3x bigger than Chinchilla 70B
			**** ‚âà 0.8% the size of the human brain by count of synapses (125T synapses)

			**** *Dataset size (tokens)* : 16T (16,000B) estimated in 40TB. Maybe repeated tokens.
			**** ‚âà 32x bigger than the GPT-3 dataset (499B tokens)
			**** ‚âà 6.8x bigger than DeepMind MassiveText English (2.35T tokens)
			**** ‚âà 3.2x bigger than DeepMind MassiveText Multilingual (5T tokens)
			**** ‚âà 5.7x bigger than Google Infiniset (2.81T tokens)

		*** Cette page redonne toutes les caract√©ristiques de l'entra√Ænement de GPT-4 üëç

* https://transmitter.ieee.org/how-big-will-ai-models-get/[]

	** When experts talk about the advancement in AI, they usually focus on two aspects of the model‚Äôs size: the *amount of data used to train the model*, and the *number of parameters that the model contains*.

	** "*GPT-3*, the predecessor to the much discussed ChatGPT, was trained on nearly *45 terabytes of text data*, and has more than *175 billion parameters*. Other AI models are getting even larger, with large companies working on models said to exceed 1.6 trillion parameters."
		*** donc GPT-3 : size 45 TB  parameters 175 B (billion)
		*** Attention ! "Billion" en anglais veut dire "milliard" en fran√ßais...

	** Which brings up a second question. Just how much information is 45 TB of text? Quite a lot. One TB equals approximately 6.5 million pages of documents stored in common formats like word processing documents or .pdfs.

* Expliquer le RLHF : Reinforcement Learning from Human Feedback

.Reinforcement Learning with Human Feedback (RLHF)
[NOTE]
====
Voir https://www.lemagit.fr/conseil/Petit-lexique-de-lIA-generative-les-grands-modeles-de-langage

L‚Äôapprentissage par renforcement est une m√©thode automatique consistant √† r√©compenser ou sanctionner un comportement d‚Äôun agent (un syst√®me autonome) plac√© dans un environnement o√π il exp√©rimente la mani√®re de r√©aliser une t√¢che. L‚Äôagent a pour mission de maximiser la r√©compense en fonction d‚Äôobjectifs.

L‚Äôapprentissage par renforcement avec retour d‚Äôinformation humain (Reinforcement Learning with Human Feedback ou RLHF) consiste √† remplacer les objectifs par des commentaires √©crits par des annotateurs.
====

*Architecture Transformer* : 

* L'*encodage* est effectu√© par une s√©rie de *couches d'attention multi-t√™te*. Chaque couche d'attention multi-t√™te consiste en une attention cl√©-valeur-rapport.
* *L'attention cl√©-valeur* est une fonction qui calcule la *similarit√© entre un token et les autres tokens de l'entr√©e*. La similarit√© est calcul√©e en fonction des repr√©sentations vectorielles des tokens.

*Analogie pour expliquer l'attention multi-t√™te :*

	* Imaginez que vous essayez de comprendre une phrase. Vous pouvez le faire en lisant chaque mot de la phrase individuellement. Cependant, vous pouvez √©galement le faire en vous concentrant sur les relations entre les mots.
	* Par exemple, vous pouvez vous concentrer sur la relation entre le sujet et le verbe de la phrase. Vous pouvez √©galement vous concentrer sur la relation entre le verbe et l'objet de la phrase.
	* L'attention multi-t√™te est similaire √† cette analogie. Elle permet au transformer de se concentrer sur plusieurs relations diff√©rentes entre les tokens dans une phrase.
	* -> En se concentrant sur plusieurs relations diff√©rentes, le transformer peut capturer un sens plus profond du texte.

*Autre explication*

* Soit la phrase suivante : "Le chat est sur la table."

* Le transformer va d'abord *tokeniser* la phrase en tokens individuels. Les tokens sont ensuite *transform√©s en repr√©sentations vectorielles*.

* L'attention est ensuite utilis√©e pour *calculer les relations entre les tokens*. La fonction d'attention calcule un *score d'attention pour chaque token*.

* Par exemple, le score d'attention entre le token "Le" et le token "chat" est √©lev√©. Cela signifie que le transformer pense que les deux tokens sont li√©s.

* Les scores d'attention sont ensuite utilis√©s pour *calculer les repr√©sentations vectorielles des tokens*. Les repr√©sentations vectorielles sont calcul√©es en pond√©rant les repr√©sentations vectorielles des tokens par les scores d'attention.

* Dans cet exemple, la repr√©sentation vectorielle du token "Le" sera pond√©r√©e par le score d'attention entre le token "Le" et le token "chat".

* Les repr√©sentations vectorielles des tokens sont ensuite utilis√©es pour g√©n√©rer une repr√©sentation vectorielle de la phrase. Cette repr√©sentation vectorielle est utilis√©e pour capturer les relations entre les tokens dans la phrase.

* L'attention multi-t√™te permet au transformer de se concentrer sur plusieurs relations diff√©rentes entre les tokens. Cela permet au transformer de capturer un sens plus profond du texte.

== VEILLE TECHNOLOGIQUE & INFOS DIVERSES

* https://a16z.com/2023/01/19/who-owns-the-generative-ai-platform/ : bon *sch√©ma de la stack des IA g√©n√©ratives*, partag√©e entre Apps, Models et Infrastructure
* https://www.reddit.com/r/ChatGPT/comments/13up0c6/ai_tools_apps_in_one_place_sorted_by_category/ : *liste des diff√©rentes applications associ√©es √† l'IA*

* *LightOn* : 
	** Avec Paradigm, on propose un mod√®le de distribution unique : on va installer le mod√®le de langage directement sur les serveurs de l'entreprise de fa√ßon √† ce que aucune information n'en sorte. +
	-> C'est le principal int√©r√™t mis en avant par Laurent Daudet, le DG de LightOn

	** Avec Paradigm, les entreprises peuvent logger toutes leurs int√©ractions avec le mod√®le, et on va am√©liorer leur mod√®le avec leurs propres donn√©es, et non avec les interactions avec l'ext√©rieur.
	** Le march√© de LightOn concerne la comp√©titivit√© des entreprises

* Parler des mod√®les de langage et de leurs donn√©es d'entra√Ænement : Common Crawl
* Explosion diff√©rents mod√®les de langage depuis GPT : Dolly (Databricks), LLaMA (Meta), etc.
	* sch√©ma des donn√©es d'entrainement de ces mod√®les

* Se renseigner sur *Illuin* : https://www.illuin.tech/

* *outil tokenizer d'OpenAI* : https://platform.openai.com/tokenizer
* *comparateur de mod√®le* (cit√© dans la doc OpenAI) : https://gpttools.com/comparisontool

* 2023/12/14 - blog Octo : *FANTASTIQUE article sur les BDD vectorielles* "Sous le capot des bases de donn√©es vectorielles" : +
https://blog.octo.com/sous-le-capot-des-bases-de-donnees-vectorielles-(vector-databases)

	** contient un lien vers un tutoriel expliquant *comment impl√©menter une base de donn√©es vectorielles* from scratch en Python : +
	https://aurelienmassiot.github.io/simple_vector_db/tp_simple_vector_db/#0

	** Lien vers un autre article d'Octo sur *comment construire son RAG (Retrieval Augmented Generation)* (2023/10/17) : +
	https://blog.octo.com/le-chatbot-docto-langchain-rag-et-code-associe

* 2023/11/21 - 5 techniques clefs pour d√©ployer l'IA g√©n√©rative dans votre entreprise : +
https://www.cio-online.com/actualites/lire-5-techniques-clefs-pour-deployer-l-ia-generative-dans-votre-entreprise-15299.html

.Section "5) Construire son LLM √† partir de z√©ro"
--
Peu d'entreprises vont cr√©er leur propre LLM √† partir de rien. Car ces mod√®les sont, par d√©finition, assez volumineux. 
Le *GPT 3 d'OpenAI* compte *175 milliards de param√®tres* et a √©t√© *entra√Æn√© sur un ensemble de donn√©es de 45 t√©raoctets*, ce qui *a co√ªt√© 4,6 millions de dollars*. 

Selon Sam Altman, PDG d'OpenAI, *GPT 4 a co√ªt√© plus de 100 millions de dollars*.

C'est cette taille qui conf√®re aux LLM leur capacit√© √† traiter le langage humain, avec un certain degr√© de bon sens, ainsi que leur aptitude √† suivre des instructions. ¬´ Vous ne pouvez pas vous contenter de l'entra√Æner sur vos propres donn√©es, explique Carm Taglienti, ing√©nieur distingu√© chez Insight. L'entra√Ænement sur des dizaines de millions de param√®tres apporte une r√©elle valeur ajout√©e. ¬ª
--

== Talks, pr√©sentations, Meetups 

=== 2023/07/04 - La strat√©gie de Saint-Gobain pour le passage √† l'√©chelle des applications d'IA

Vid√©o accessible via le Club Azure Insiders : https://info.microsoft.com/FR-AzureAI-VDEO-FY23-04Apr-11-Replay-Saint-Gobain-s-strategy-for-scaling-AI-applications-SRGCM9881_LP02-Thank-You---Standard-Hero.html

.Organisation de l'IT chez Saint-Gobain
image:20230330_Club-Azure-Insiders_strategie-Saint-Gobain-passage-echelle-appli-IA_01.jpg[]

* 4 acc√©l√©rateurs techniques : 

    ** *AI Portal* : leverage knowledge in Artificial Intelligence and connects data scientists with each other
	** *AI Lab* & *AI Indus* : 2 platforms that standardize our algorithms to develop and deploy them ever faster in the Cloud.
	** *AI Store* : a "ready-to-use" applications library. Elementary building blocks that enable a new team to create an algorithm, not in 6 months, but in just a few weeks !

image:20230330_Club-Azure-Insiders_strategie-Saint-Gobain-passage-echelle-appli-IA_02.jpg[]

.AI Indus : to deliver prototypes into Integration / Prod
image:20230330_Club-Azure-Insiders_strategie-Saint-Gobain-passage-echelle-appli-IA_03.jpg[]

.AI Store : to deploy AI applications at scale
image:20230330_Club-Azure-Insiders_strategie-Saint-Gobain-passage-echelle-appli-IA_04.jpg[]

.Next main challenges
image:20230330_Club-Azure-Insiders_strategie-Saint-Gobain-passage-echelle-appli-IA_05.jpg[]

	* AI 4.0
	* *Data sharing* & reuse
	* *AI Citizen*

.AI Lab : services
image:20230330_Club-Azure-Insiders_strategie-Saint-Gobain-passage-echelle-appli-IA_06.jpg[]

	* Le principal service de ce PaaS est Azure Machine Learning

image:20230330_Club-Azure-Insiders_strategie-Saint-Gobain-passage-echelle-appli-IA_07.jpg[]

* *StreamLit* pour le Front : Stream a d√©finitivement le vent en poupe pour la cr√©ation du Front de projets / produits associ√©s √† l'IA
	** StreamLit est tr√®s simple √† ma√Ætriser pour monter des fronts simples
	** StreamLit √©galement pr√©conis√© car *front en Python* (et Python fait d√©finitivement l'unanimit√© gr√¢ce √† sa forte communaut√© et l'immensit√© des ressources, docs disponibles)
	** StreamLit par contre ne semble pas √™tre la solution retenue par Saint-Gobain pour les applications internes, apparemment pour raison de s√©curit√©.

TO BE COMPLETED

=== 2023/11/23 - Commment LVMH d√©ploie l'IA √† l'√©chelle

URL : https://www.lemagit.fr/etude/Comment-LVMH-deploie-lIA-a-lechelle

* La ¬´ plateforme Data ¬ª du groupe LVMH s‚Äôapparente davantage √† un framework afin d‚Äôacc√©l√©rer les d√©ploiements de cas d‚Äôusage allant de la BI √† l‚ÄôIA dans l‚Äôensemble des Maisons. +
-> Dans plus de 24 Maisons et divisions, cette plateforme repose sur le *Data warehouse cloud BigQuery*.

* En Chine, une dizaine de Maisons s‚Äôappuie sur un ¬´ √©cosyst√®me ¬ª *Alibaba MaxCompute*.

* Certaines √©quipes au sein du groupe utilisent l‚Äôensemble des fonctions de *Dataiku*, √† savoir la pr√©paration de donn√©es, le *feature engineering*, l‚Äôint√©gration du mod√®le de machine learning, le post-traitement, et le formatage de r√©sultats en direction de syst√®mes tiers.
	** Dataiku est utilis√© ¬´ pour effectuer du packaging d‚Äôalgorithmes de mani√®re centralis√©e ¬ª.
	** ¬´ En collaboration avec les Maisons les plus matures qui ont d√©j√† des algorithmes, nous prenons ces briques et nous les rendons modulaires en utilisant, quand le besoin s‚Äôen fait sentir, des librairies d‚ÄôIA avanc√©es : de la vision par ordinateur, des *transformers pour le traitement du langage naturel* et nous commen√ßons √† exp√©rimenter les grands mod√®les de langage et de diffusion ¬ª. +
	Ces paquets correspondent √† des notebooks contenant le code des mod√®les et les appels aux librairies. Une fois centralis√©s, les data scientists et les data engineers peuvent ais√©ment modifier les algorithmes suivant le cahier des charges d‚Äôune Maison.

NOTE: √Ä un haut niveau, le *feature engineering* est la pratique qui consiste √† transformer des donn√©es brutes en la forme la plus appropri√©e pour un algorithme de machine learning sp√©cifique.

=== 2023/11/21 - BGPN - Petit dej de l'IA - 3e talk
 
Pr√©sent√© par : 
 
    * Ghislain LABAY c√¥t√© Hub La Poste
    * l'invit√© du jour Gilles GUERRAZ : un expert reconnu sur l'IA g√©n√©rative (r√©alisateur publicitaire et Expert en IA g√©n√©ratives)
    * L'application du mois : ElevenLabs, du text to speech
 
OpenAI : 
 
    * 100 M d'utilisateurs de ChatGPT chaque semaine
    * 2 M de d√©veloppeurs utilisent les API OpenAI
 
.Lancement des "GPTs"
image:20231121_petit-dej-IA_01.jpg[]
 
    * Les "GPTs" : permettre √† tout un chacun de cr√©er facilement des versions de ChatGPT adapt√©es √† ses besoins quotidiens ou pro. +
    Les users peuvent configurer ces assistants virtuels pour qu'ils ex√©cutent des t√¢ches sp√©cifiques et interagissent avec des sources de donn√©es externes. +
    Aucune connaissance en programmation n'est n√©cessaire.
 
    * Un GPT Store pour commercialiser vos GPTs
    * Adobe Dub Dub Dub pour de la traduction en "live" (comme Heygen)
 
    * L'administration fran√ßaise est en train de faire un test avec Anthropic et son LLM Claude.
 
Pr√©sentation de Gilles GUERRAZ
 
    * Gilles est r√©alisation professionnel depuis 2008
 
    * Gilles √©dite une newsletter (https://iagenerative.substack.com/) et co-anime un podcast sur la vulgarisation sur l'IA g√©n√©rative
    * Gilles anime √©galement un concours de court-m√©trage en 48h (chaque ann√©e d√©but d√©cembre)
    * Gilles utilise beaucoup MidJourney (au moment de la sortie de la v3 de MidJourney, Gilles a trouv√© MidJourney bien meilleur en termes de rendu que Dall-E)

=== 2023/12/12 - P'tit d√©j IA g√©n√©rative La Poste

* *Character.ai* : 
	** URL : https://beta.character.ai
	** √† date, c'est la 2e application mobile la plus utilis√©e aux US avec 6M d'utilisateurs mensuels
		*** derri√®re ChatGPT
	** 60% des utilisateurs ont entre 18 et 24 ans
	** permet de discuter avec un personnage ayant exist√© (voire imaginaire), et entra√Æn√© sur la base de biographie et autres donn√©es (?)
		*** Possibilit√© pour chacun de cr√©er son avatar

	** L'un des chatbots les plus utilis√©s est un psychologue
	** Character.ai se positionne donc comme un ami ou un coach
		*** avec un engagement tr√®s fort de la part de l'utilisateur : on passe en moyenne 8 min par session sur ChatGPT, contre 25 √† 45 min pour Character.ai
	** 193 M$ d√©j√† lev√©s √† date, et en recherche de financement. +
	Character.ai est d√©j√† valoris√© √† 5Mds$

	** Google et surtout Facebook cherchent √©galement √† se positionner sur ce secteur.

* S√©bastien : pr√©sentation de l'IA Conv BGPN
	** GPT 3 plus que 4 pour des raisons de vitesse et de co√ªt
	** si doc charg√©, ce n'est QUE pour la session et l'utilisateur connect√©
	** plusieurs agents avec leurs propres donn√©es vectoris√©es

* Ronan : pas d'Azure "pour le moment"
	** On veut faire du souverain, donc plut√¥t du Numspot ou de l'OVH

* Olivier SENOT
	** David GAILLOU par la synth√®se des dossies m√©dicaux
	** instance OpenAI France Azure, uniquement par API
		*** GPT 4
		*** demain sur GPT 4 turbo
		*** d√©ploy√© pour les Docapostiens
	** scrappeur d'URL pas encore ouvert, par peur d'injection de malwares dans le retour prompt
		*** il y a apparemment des fils Reddit qui expliquent bien la chose
	** QUE le chatbot et la synth√®se des documents
	** GitHub Copilot "en cours" de d√©ploiement chez nos populations IT
		*** en train de "patiner un peu" car compliqu√© c√¥t√© contrat
	** 17 cas d'usage pour √©quiper Docaposte en interne pour utiliser le LLM (pour T1 2024)
		*** r√©ponse auto √† des AO
		*** Analyse rh√©torique des parlementaires

* Sophie : c√¥t√© Probayes, ils ont un moteur d'inf√©rence
	** Olivier : on n'a "QUE" une base de vecto
		*** moteur d'inf√©rence d'ici √† xxx 2024
	** Olivier : et on a pas de mod√®le on-premise car on a pas de GPU

* Olivier SENOT : tous les mois il a un bench de comparaison des LLM
	** et actuellement GPT 4 arrive tout le temps devant









